{"Title": "Modeling Advection on Directed Graphs using Mat\u00e9rn Gaussian Processes for Traffic Flow", "Authors": "Danielle C Maddix, Nadim Saad, Yuyang Wang", "Abstract": "  The transport of traffic flow can be modeled by the advection equation. Finite difference and finite volumes methods have been used to numerically solve this hyperbolic equation on a mesh. Advection has also been modeled discretely on directed graphs using the graph advection operator [4, 18]. In this paper, we first show that we can reformulate this graph advection operator as a finite difference scheme. We then propose the Directed Graph Advection Mat\u00e9rn Gaussian Process (DGAMGP) model that incorporates the dynamics of this graph advection operator into the kernel of a trainable Mat\u00e9rn Gaussian Process to effectively model traffic flow and its uncertainty as an advective process on a directed graph.      ", "Subject": "Numerical Analysis (math.NA)", "ID": "arXiv:2201.00001", "Text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Advection on Directed Graphs using\nMat\u00e9rn Gaussian Processes for Traffic Flow\n\nDanielle C. Maddix\nAmazon Research\n\n2795 Augustine Dr.\nSanta Clara, CA 95054\ndmmaddix@amazon.com\n\nNadim Saad\nStanford University\n\n450 Serra Mall\nStanford, CA 94305\n\nnsaad31@stanford.edu\n\nYuyang Wang\nAmazon Research\n\n2795 Augustine Dr.\nSanta Clara, CA 95054\nyuyawang@amazon.com\n\nAbstract\n\nThe transport of traffic flow can be modeled by the advection equation. Finite\ndifference and finite volumes methods have been used to numerically solve this\nhyperbolic equation on a mesh. Advection has also been modeled discretely on\ndirected graphs using the graph advection operator [4, 18]. In this paper, we first\nshow that we can reformulate this graph advection operator as a finite difference\nscheme. We then propose the Directed Graph Advection Mat\u00e9rn Gaussian Process\n(DGAMGP) model that incorporates the dynamics of this graph advection operator\ninto the kernel of a trainable Mat\u00e9rn Gaussian Process to effectively model traffic\nflow and its uncertainty as an advective process on a directed graph.\n\n1 Introduction\n\nThe continuous linear advection equation models the flow of a scalar concentration along a vector\nfield. The solutions to this hyperbolic partial differential equation may develop discontinuities or\nshocks over time depending on the initial condition. These shocks can model the formation of traffic\njams, and their propagation along a road [20]. Figure 1 illustrates an example, where initially the\nfirst half of the road is 70% occupied with cars, and the second half of the road is empty. The traffic\npropagates to the right until the whole road is 70% occupied. Classical methods, such as finite\ndifferences and finite volumes, have been used to predict the flow of traffic along a road [14, 20].\nThese classical numerical methods do not incorporate any randomness into the model, and can be\nlimited in incorporating the uncertainty among different driver\u2019s behaviors [6].\n\nFigure 1: Propagation of cars on a road using an\nadvection process.\n\nGaussian processes (GPs) [19] can learn unknown\nfunctions that allow use of prior information\nabout their properties and for uncertainty model-\ning. K\u00fcper and Waldherr [9] propose the Gaussian\nProcess Kalman Filter (GPKF) method to simu-\nlate spatiotemporal models, and test on the ad-\nvection equation. Raissi et al. [17] train GPs on\ndata to learn the underlying physics of non-linear\nadvection-diffusion equations. Additional physics-\nbased machine learning models [2] use the Mat\u00e9rn\ncovariance function given below:\n\nu \u223c N\n(\n0,\n(2\u03bd\n\u03ba2\n\n+ \u2206\n)\u2212\u03bd)\n\n, (1)\n\nwhere u denotes an unknown function, \u03bd <\u221e, \u03ba <\u221e and \u2206 denotes the laplacian [1]. The Mat\u00e9rn\n\nFourth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2021).\n\nar\nX\n\niv\n:2\n\n20\n1.\n\n00\n00\n\n1v\n1 \n\n [\nm\n\nat\nh.\n\nN\nA\n\n] \n 1\n\n4 \nD\n\nec\n 2\n\n02\n1\n\n\n\nkernel captures physical processes due to its finite differentiability, and is also commonly used to\ndefine distances between two points that are d units distant from each other [2]. Perdikaris and\nKarniadakis [16] propose training joint Mat\u00e9rn GPs to model space-fractional differential equations,\nin which the advection-diffusion equation is a special case.\n\nRecent works including [22] have studied solving partial differential equation (PDEs) on graphs.\nChapman and Mesbahi [4], Rak [18] propose discrete advection and consensus operators to model\nadvection and diffusion flows, respectively on directed graphs. Ho\u0161ek and Volek [8] study the\nadvection-diffusion equation on graphs using this discrete advection operator, and show that finite vol-\nume numerical discretizations can be reformulated as equations on graphs resulting in a corresponding\nmaximum principle for this operator. Additional works have also looked at combining scientific\ncomputing and machine learning on graphs for spatiotemporal traffic modeling [11]. Chamberlain\net al. [3] propose the Graph Neural Diffusion (GRAND) method, which combines traditional ODE\nsolvers with graph neural networks (GNNs) to model diffusion on a undirected graph. Borovitskiy\net al. [2] propose to replace the continuous laplacian \u2206 in (1) with the discrete graph laplacian\noperator L to model diffusion on undirected graphs, which can be limited for traffic modeling.\n\nThe goal of this paper is two-fold: to develop a model that effectively models traffic flow as an\nadvective process on a directed graph and its uncertainty. We propose a novel method, Directed Graph\nAdvection Mat\u00e9rn Gaussian Process (DGAMGP) that uses a symmetric positive definite variant of\nthe graph advection operator Ladv as a covariance matrix in the Mat\u00e9rn Gaussian Process. We use the\nsquare of the singular values of Ladv to model the advection dynamics, and train a Mat\u00e9rn Gaussian\nProcess to model the uncertainty. We also show the connection between consistent finite difference\nstencils for solving the linear advection equation and the graph advection operator. Our novel linkage\nhelps improve the understanding and interpretability of this graph advection operator.\n\n2 Understanding the directed graph advection operator\n\nWe aim to model the continuous advection equation for unknown scalar u under vector field v:\n\n\u2202u\n\n\u2202t\n= \u2212\u2207 \u00b7 (vu),\n\nstochastically on a directed graph. We define a directed, weighted graph G = (V,E,W ) with |V | = n\nnodes and |E| = |W | = m edges, where V denotes the vertex, E the edge, and W the edge weight\nsets, respectively. We discretize the flow vu along edge (i, j) \u2208 E with weight wji \u2208W as wjiui(t),\nwhere ui(t) denotes the concentration u at node i and time t.\n\nThe graph advection operator Ladv is defined so that the flow into a node equals the flow out of it [4]:\n\ndui(t)\n\ndt\n=\n\n\u2211\nj:(j,i)\u2208E\n\nwijuj(t)\u2212\n\u2211\n\nj:(i,j)\u2208E\n\nwjiui(t) = \u2212[Ladvu(t)]i, (2)\n\nwhere Ladv = Dout \u2212 Ain for diagonal out-degree matrix Dout and in-degree adjacency matrix\nAin. For general directed graphs, Ladv belongs to the square, non-symmetric with non-negative real\npart eigenvalues [18] class of matrices in [13]. By design, Ladv is conservative, unlike the related\ndiffusion or consensus operator Lcons = Din \u2212 Ain, where Din denotes the diagonal in-degree\nmatrix [4, 18]. A main motivating reason for using Ladv to model traffic flow is that it results in a\nconservative scheme.\n\nReformulation of Ladv as finite difference on balanced graphs. We notice that Ladv at node i\nis a weighted linear combination of the other nodes adjacent to it, which resembles finite difference\nstencils of the unknown and its neighbors. We make this connection precise, and then construct\nexample graphs where Ladv corresponds to common finite difference schemes for linear advection.\n\nTheorem 2.1. Ladv corresponds to a semi-discrete finite difference advection scheme, where the sum\nof the coefficients is zero if and only if the graph G is balanced, i.e. Ladv = Lcons.\nProof. A finite difference approximation to the gradient can be written as the following weighted\nlinear combination of its neighbors uj for arbitrary coefficients cij \u2208 R:\n\n\u2212(ux)i \u2248\n\u2211\nj 6=i\n\ncijuj + ciiui. (3)\n\n2\n\n\n\nA consistent finite difference scheme is at least zero-th order accurate [10]. Since the derivative of a\nconstant is 0, the coefficients must sum to 0, i.e cii = \u2212\n\n\u2211\nj 6=i cij . Combining (2) with (3) gives:\n\n(Dout)ii =\n\u2211\n\nj:(i,j)\u2208E\n\nwji = \u2212cii =\n\u2211\nj 6=i\n\ncij =\n\u2211\n\nj:(j,i)\u2208E\n\nwij = (Din)ii.\n\nThe graph G is balanced by definition, and it follows that Ladv = Lcons. The other direction follows\nsimilarly.\n\nApplying Ladv on the directed line graph in Figure 2(a) results in the first order upwind scheme\nwith spatial step size \u2206x for v > 0 in (5) (See Appendix A and Figure 6 for the convergence study).\nSimilarly, Figure 2(b) illustrates the directed graph in which Ladv gives the second order central\ndifference scheme, where (ux)i \u2248 (ui+1 \u2212 ui\u22121)/(2\u2206x) (See Appendix B for additional examples).\n\nui\u22121 ui ui+1\nv/\u2206x v/\u2206x\n\n(a) first order upwind scheme\n\nui\u22121 ui ui+1\nv/2\u2206x \u2212v/2\u2206x\n\n\u2212v/2\u2206x v/2\u2206x\n\n(b) second order central scheme\n\nFigure 2: Balanced graphs on which Ladv corresponds to finite difference stencils of linear advection.\n\n3 Directed Graph Advection Mat\u00e9rn Gaussian Process (DGAMGP)\n\nWe propose the novel Directed Graph Advection Mat\u00e9rn Gaussian Process (DGAMGP) model, which\nuses the dynamics of Ladv to model advection stochastically on a directed graph through a discrete\napproximation to the continuous Laplacian \u2206 of the Mat\u00e9rn Gaussian Process in (1). The covariance\nmatrix or kernel K of a Gaussian process needs to be symmetric and positive semi-definite. This\nleads to some challenges with the Ladv operator as it is not guaranteed in general to be symmetric or\npositive semi-definite (See Section 2). Note that using the graph Laplacian L in the covariance matrix\nin the undirected graph case is more straightforward since L is symmetric positive semi-definite.\n\nIn our directed graph case, we propose using LTadvLadv as the covariance matrix since it is symmetric\npositive definite, and hence orthogonally diagonalizable. Analogous to [2], we define a function \u03c6 of\na diagonalizable matrix through Taylor series expansion. Then we can define its eigendecomposition\nas LTadvLadv = Xadv\u039badvX\n\nT\nadv, so that \u03c6(L\n\nT\nadvLadv) = Xadv\u03c6(\u039badv)X\n\nT\nadv, where \u03c6(\u039badv) is\n\ncomputed by applying \u03c6 to the diagonal elements of \u039badv .\n\nWe compute the eigendecomposition of LTadvLadv = Vadv\u03a3\n2\nadvV\n\nT\nadv , using the singular value decom-\n\nposition (SVD) of Ladv = Uadv\u03a3advV Tadv, where the eigenvalues and eigenvectors are the singular\nvalues squared and right singular vectors of Ladv, respectively. Hence, we model the advection\ndynamics using the square of the singular values of Ladv . Our approach can also be viewed as adding\nthe square of the singular values of Ladv to the diagonal for regularization. Computing the thin-SVD\nis more computationally efficient and numerically stable, since we avoid explicitly forming the\nmatrix-matrix product LTadvLadv , which has double the condition number of Ladv , and the numerical\nissues with then computing its eigendecomposition.\n\nWe chose \u03c6 to be the Mat\u00e9rn covariance function in (1), and our DGAMGP model is given by:\n\nu \u223c N\n(\n0,\n(\nVadv(\n\n2\u03bd\n\n\u03ba2\nI + \u03a32adv)\n\n\u2212\u03bdV Tadv\n))\n. (4)\n\nThis advective Gaussian Process is then trained on data by minimizing the negative log-likelihood of\nthe Gaussian Process to learn the kernel hyperparameters \u03bd and \u03ba, and predict u [7]. For inference,\nwe draw samples from the GP predictive posterior distribution with the learned hyperparameters [19].\nSee Algorithm 1 for details.\n\nChoice of LTadvLadv . There are alternate approaches to symmetrize Ladv . The first simple approach\nexplored is to utilize Lsym = (LTadv + Ladv)/2 . This operator is not positive semi definite except\n\n3\n\n\n\nin the balanced graph case. The second approach is to use the symmetrizer method in [21], which\ngenerates a symmetric matrix L\u2032sym with the same eigenvalues as Ladv but is not always positive\nsemi definite.\n\nAlgorithm 1 The Directed Graph Advection Mat\u00e9rn Gaussian Process (DGAMGP)\nGiven a directed graph G = (V,E,W ) and training data D = {(xi, yi)}ni=1.\n\n1. Compute Ladv(G) = Dout \u2212Ain.\n2. Compute the SVD of Ladv = Uadv\u03a3advV Tadv .\n3. Generate a DGAMGP model in (4).\n4. Minimize the GP negative log marginal likelihood using D to learn \u03bd, \u03ba and \u03c3 [7].\n5. Given test data {x\u2217i }, draw samples from the GP predictive posterior distribution [19].\n\n4 Numerical Results\nIn this section, we utilize our DGAMGP model for traffic modeling on synthetic and real-\nworld directed traffic graphs. The data D = {(xi, yi)}ni=1 denotes the traffic flow speed in\nmiles per hour yi at location xi. We test our model\u2019s predictive ability to predict the veloci-\nties of cars on a road at different positions. We use hold-out cross validation to split the data\npoints generated into training (70% of the data) and testing data (30% of the data). We extend\nthe code in [2] to compute the singular value decomposition of Ladv to train our DGAMGP\nmodel on a directed graph. The code is available at https://github.com/advectionmatern/\nModeling-Advection-on-Directed-Graphs-using-Mat-e-rn-Gaussian-Processes, and\nthe experiments are run on Amazon Sagemaker [12].\n\nRegression results on synthetic graphs. We generate synthetic data that models traffic along a\nroad, which has a relatively high density of cars in the first half and a low density of cars in the second\nhalf. We train and test our model on the upwind scheme in Figure 2(a), central scheme in Figure\n2(b), an intersecting lane graph, where two lanes merge into one lane in Figure 3(a) and a loop graph\nrepresenting the upwind scheme with periodic boundary conditions in Figure 3(b). Table 1 compares\nthe results to the consensus baseline model of using the singular value decomposition of Lcons in\nEqn. (4).\n\nModel Graph type n = 280 n = 325 n = 400 \u03bd \u03ba \u03c3\nAdvection\n\nUpwind\n0.52 0.45 0.0005 0.65 8.09 7.75\n\nConsensus 0.51 0.44 0.0005 0.65 8.29 7.77\nAdvection\n\nCentral\n1.31 0.85 8.41e-05 0.67 9.00 8.03\n\nConsensus 0.97 0.8 8.02e-05 0.67 9.45 8.11\nAdvection\n\nIntersection\n0.96 0.45 0.0005 0.65 8.19 7.75\n\nConsensus 0.52 0.46 0.0005 0.64 8.28 7.77\nAdvection\n\nLoop\n0.47 0.41 0.00045 0.65 8.49 7.76\n\nConsensus 0.47 0.41 0.00045 0.65 8.49 7.76\n\nTable 1: Comparison of l2 test error on synthetic directed graphs with n nodes and the learned\nhyperparameters.\n\nui\u22122 ui\u22121 ui\n\nui\u22124 ui\u22123\n\nui+1\nv/\u2206x v/\u2206x\n\nv/\u2206x v/\u2206x\n\n2v/\u2206x\n\n(a) intersection graph\n\nu1 ui\u22121 ui un\nv/\u2206x v/\u2206x v/\u2206x\n\nv/\u2206x\n\n(b) loop graph\n\nFigure 3: Graphs representing two lanes merging into one (left) and a loop (right).\n\n4\n\nhttps://github.com/advectionmatern/Modeling-Advection-on-Directed-Graphs-using-Mat-e-rn-Gaussian-Processes\nhttps://github.com/advectionmatern/Modeling-Advection-on-Directed-Graphs-using-Mat-e-rn-Gaussian-Processes\n\n\nRegression results on a real-world traffic graph. We test on the real-world traffic data from the\nCalifornia Performance Measurement System [5] with the road network graph from the San Jose\nhighways from Open Street Map [15] at a fixed time. Since our method supports directed graphs,\nwe do not need to convert the raw directed traffic data to an undirected graph as in [2]. We use the\nsame experimental setup from [2] to generate the train and test data. Figure 4 shows the resulting\npredictive mean and standard deviation of the speed on the San Jose highways using the visualization\ntools from [2]. We notice that the predictive standard deviation along the nodes is relatively small,\nand is larger on the points that are farther from the sensors.\n\nFigure 4: Traffic speed interpolation over a graph of San Jose highways using our DGAMGP method\nwith \u03bd = 0.35, \u03ba = 1002.8, \u03c3 = 1.14 and plotting tools from [2].\n\n5 Conclusions\n\nIn this paper, we propose a novel method DGAMGP to model an advective process on a directed\ngraph and its uncertainties. We show connections between finite differences schemes used to solve\nthe linear advection equation and the graph advection operator Ladv employed in our model. We\nexplore a regression problem on various graphs, and show that our proposed DGAMGP model\nperforms similarly to other state-of-the-art models. Future work includes adding a time-varying\ncomponent to our model, comparing our method to classical numerical methods for solving PDEs,\nand incorporating the behavior of the non-linear advection equation for traffic modeling.\n\nReferences\n\n[1] Bakka, H., Krainski, E., Bolin, D., Rue, H., and Lindgren, F. (2020). The diffusion-based\nextension of the mat\u00e9rn field to space-time. arXiv:2006.04917.\n\n[2] Borovitskiy, V., Azangulov, I., Terenin, A., Mostowsky, P., Deisenroth, M., and Durrande,\nN. (2021). Mat\u00e9rn gaussian processes on graphs. In Banerjee, A. and Fukumizu, K., editors,\nProceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume\n130 of Proceedings of Machine Learning Research, pages 2593\u20132601. PMLR.\n\n5\n\n\n\n[3] Chamberlain, B., Rowbottom, J., Gorinova, M. I., Bronstein, M., Webb, S., and Rossi, E. (2021).\nGrand: Graph neural diffusion. In Meila, M. and Zhang, T., editors, Proceedings of the 38th\nInternational Conference on Machine Learning, volume 139 of Proceedings of Machine Learning\nResearch, pages 1407\u20131418. PMLR.\n\n[4] Chapman, A. and Mesbahi, M. (2011). Advection on graphs. IEEE Conference on Decision and\nControl and European Control Confereence (CDC-ECC), 50:1461\u20131466.\n\n[5] Chen, C., Petty, K., Skabardonis, A., Varaiya, P., and Jia, Z. (2001). Freeway performance\nmeasurement system: mining loop detector data. Transportation Research Record, 1748(1):96\u2013\n102.\n\n[6] Chen, Y., Sohani, N., and Peng, H. (2018). Modelling of uncertain reactive human driving\nbehavior: a classification approach. In 2018 IEEE Conference on Decision and Control (CDC),\npages 3615\u20133621.\n\n[7] Gardner, J., Pleiss, G., Bindel, D., Weinberger, K., and Wilson, A. (2018). GPytorch: Blackbox\nmatrix-matrix gaussian process inference with gpu acceleration. 32nd Conference on Neural\nInfromation Processing Systems (NIPS 2018) arXiv:1809.11165v2.\n\n[8] Ho\u0161ek, R. and Volek, J. (2019). Discrete advection\u2013diffusion equations on graphs: Maximum\nprinciple and finite volumes. Applied Mathematics and Computation, 361(C):630\u2013644.\n\n[9] K\u00fcper, A. and Waldherr, S. (2020). Numerical gaussian process kalman filtering. 21st IFAC\nWorld Congress.\n\n[10] LeVeque, R. J. (2007). Finite Difference Methods for Ordinary and Partial Differential Equa-\ntions: Steady-State and Time-Dependent Problems. SIAM.\n\n[11] Li, Y., Yu, R., Shahabi, C., and Liu, Y. (2018). Diffusion convolutional recurrent neural network:\nData-driven traffic forecasting. International Conference on LEarning Representations (ICLR).\n\n[12] Liberty, E., Karnin, Z., Xiang, B., Rouesnel, L., Coskun, B., Nallapati, R., Delgado, J.,\nSadoughi, A., Astashonok, Y., Das, P., Balioglu, C., Chakravarty, S., Jha, M., Gautier, P., Arpin,\nD., Januschowski, T., Flunkert, V., Wang, Y., Gasthaus, J., Stella, L., Rangapuram, S., Salinas, D.,\nSchelter, S., and Smola, A. (2020). Elastic machine learning algorithms in amazon sagemaker. In\n2020 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201920, New York,\nNY, USA. Association for Computing Machinery., pages 731\u2013737.\n\n[13] Liesen, J. and Parlett, B. N. (2008). On nonsymmetric saddle point matrices that allow conjugate\ngradient iterations. Numer. Math., 108:605\u2013624.\n\n[14] Lighthill, M. and Whitham, G. (1955). On kinematic waves ii. a theory of traffic flow on long\ncrowded roads. Proceedings of the Royal Society of London. Series A. Mathematical and Physical\nSciences, 229:317 \u2013 345.\n\n[15] OpenStreetMap (2017). https://www.openstreetmap.org.\n\n[16] Perdikaris, P. and Karniadakis, G. (2019). Machine learning of space-fractional differential\nequations, SIAM Journal on Scientific Computing, Vol. 41, No. 4, Society for Industrial and\nApplied Mathematics.\n\n[17] Raissi, M., Perdikaris, P., and Karniadakis, G. (2019). Physics-informed neural networks: A\ndeep learning framework for solving forward and inverse problems involving nonlinear partial\ndifferential equations. Journal of Computational Physics, 378:686\u2013707.\n\n[18] Rak, A. (2017). Advection on graphs. http://nrs.harvard.edu/urn-3:HUL.InstRepos:\n38779537.\n\n[19] Rasmussen, C. and Williams, C. (2006). Gaussian Processes for Machine Learning. MIT Press.\n\n[20] Richards, P. (1956). Shock waves on the highway. Operation Res., pages 42 \u2013 51.\n\n[21] Sen, S. and Venkaiah, V. C. (1988). On symmetrizing a matrix. Indian J. pure appl. Math.,\n19(6):554\u2013561.\n\n[22] Solomon, J. (2015). PDE approaches to graph analysis. ArXiv, abs/1505.00185.\n\n6\n\nhttps://www.openstreetmap.org\nhttp://nrs.harvard.edu/urn-3:HUL.InstRepos:38779537\nhttp://nrs.harvard.edu/urn-3:HUL.InstRepos:38779537\n\n\nA Upwinding discretizations of linear advection\n\nWe discretize the 1D linear advection equation with velocity v:\nut + vux = 0,\n\nusing the standard first order upwinding scheme on a simple uniform Cartesian mesh with spatial step\nsize \u2206x. Then the classical finite difference first-order upwind scheme depends on the sign of v. For\nflow moving from left to right, v > 0, and we have the following semi-discrete discretization [10]:\n\ndui\ndt\n\n+ v\nui \u2212 ui\u22121\n\n\u2206x\n= 0, if v > 0,\n\ndui\ndt\n\n+ v\nui+1 \u2212 ui\n\n\u2206x\n= 0, if v < 0.\n\n(5)\n\nUpwinding schemes are useful in the advection case since information is moving from left to right.\nThe Courant-Friedrichs-Lewy (CFL) condition for stability of the first order upwinding scheme with\nForward Euler time-stepping discretization with time step \u2206t is given by:\u2223\u2223\u2223v\u2206t\n\n\u2206x\n\n\u2223\u2223\u2223 \u2264 1 \u21d0\u21d2 \u2206t \u2264 \u2223\u2223\u2223 v\n\u2206x\n\n\u2223\u2223\u2223.\nA less diffusive second order upwind scheme is also known as linear upwind differencing (LUD),\nand is given by:\n\ndui\ndt\n\n= v\n\u2212ui\u22122 + 4ui\u22121 \u2212 3ui\n\n2\u2206x\n. (6)\n\nWe can show that the scheme is second-order accurate using Taylor expansions. It is designed to be\nless diffusive because the uxx term from the first-order upwinding scheme cancels. We have\n\nui\u22122 \u2212 4ui\u22121 + 3ui\n2\u2206x\n\n=\n1\n\n2\u2206x\n\n[(\nu\u2212 2\u2206xux +\n\n4\u2206x2\n\n2\nuxx \u2212\n\n8\u2206x3\n\n6\nuxxx +O(\u2206x4)\n\n)\n+\n(\n\u2212 4(u\u2212\u2206xux +\n\n\u2206x2\n\n2\nuxx \u2212\n\n\u2206x3\n\n6\nuxxx +O(\u2206x4))\n\n)\n+ 3u\n\n]\n\n= ux \u2212\n\u2206x2\n\n3\nuxxx +O(\u2206x4).\n\nHence, the scheme is second order accurate with a dispersive uxxx leading error term.\n\nB Examples of Ladv on balanced graphs resulting in finite difference\ndiscretizations of linear advection\n\nIn addition to the finite difference schemes provided in Section 2, we also provide an example of a\nnon-uniform mesh discretization:\n\ndui\ndx\n\u2248\n\n4\n3\nui+1/2 \u2212 ui \u2212 13ui\u22121\n\n\u2206x\n,\n\nwhich results in the following graph, where the in-going and out-going edges from ui:\n\nui\u22121 ui\u22121/2 ui ui+1/2 ui+1\n\nv/3\u2206x\n\n\u22124v/3\u2206x\u22124v/3\u2206x\n\nv/3\u2206x\n\nWe can obtain the less diffusive second order upwind scheme (LUD) in (6) using the following graph:\n\nui\u22122 ui\u22121 ui ui+1 ui+2\n\n\u2212v/2\u2206x\n\n2v/\u2206x 2v/\u2206x\n\n\u2212v/2\u2206x\n\n2v/\u2206x\n\n\u2212v/2\u2206x\n\n2v/\u2206x\n\n7\n\n\n\nC Additional Experiments\n\nC.1 Gaussian Process prior results with DGAMGP\n\nA main property of the Mat\u00e9rn Gaussian Process kernel is that it varies along Riemannian manifolds.\nThe variance of the kernel is a function of degree, and depends on a complex manner on the graph.\nWe show the results generated with a star graph directed towards the center node and a directed\ncomplete graph. Figure 5(a) shows that as expected for the complete graph, the nodes have the\nsame variability, since for a random walk starting from any node, there is equal probability to get to\nanother node. For the star graph in Figure 5(b), we observe that the center node has a variability of\napproximately 0 as starting from any node on the graph, the random walk always ends at the center.\n\n(a) complete graph prior. (b) star graph prior.\n\nFigure 5: Prior results using DGAMGP obtained using various graphs, and plotting tools from [2].\n\nC.2 Convergence Studies\n\nWe conduct a convergence study of applying Ladv on the upwind graph in Figure 2(a), and show that\nit has first order convergence matching the performance of the equivalent first order upwind scheme.\nWe use the same initial condition as in Figure 1. We then solve the resulting system of ODEs using\nthe RK5 ODE solver. Figure 6(a) shows the solution at different time steps, and we see how the\nsolution is propagating to the right. Figure 6(b) shows a loglog plot, where the error is decreasing\nlinearly with a slope of 1 as the number of nodes n is increasing, as expected.\n\n(a) Solution of the linear advection equation using (2) (b) Convergence study in a log-log plot\n\nFigure 6: Upwinding solution with RK5 to the linear advection equation over time and corresponding\nconvergence study.\n\n8\n\n\n\t1 Introduction\n\t2 Understanding the directed graph advection operator\n\t3 Directed Graph Advection Mat\u00e9rn Gaussian Process (DGAMGP)\n\t4 Numerical Results\n\t5 Conclusions\n\tReferences\n\tA Upwinding discretizations of linear advection\n\tB Examples of Ladv on balanced graphs resulting in finite difference discretizations of linear advection\n\tC Additional Experiments\n\tC.1 Gaussian Process prior results with DGAMGP\n\tC.2 Convergence Studies\n\n\n"}
{"Title": "Time-Dependent Duhamel Renormalization method with Multiple Conservation and Dissipation Laws", "Authors": "Sathyanarayanan Chandramouli, Aseel Farhat, Ziad Musslimani", "Abstract": "  The time dependent spectral renormalization (TDSR) method was introduced by Cole and Musslimani as a novel way to numerically solve initial boundary value problems. An important and novel aspect of the TDSR scheme is its ability to incorporate physics in the form of conservation laws or dissipation rate equations. However, the method was limited to include a single conserved or dissipative quantity. The present work significantly extends the computational features of the method with the (i) incorporation of multiple conservation laws and/or dissipation rate equations, (ii) ability to enforce versatile boundary conditions, and (iii) higher order time integration strategy. The TDSR method is applied on several prototypical evolution equations of physical significance. Examples include the Korteweg-de Vries (KdV), multi-dimensional nonlinear Schr\u00f6dinger (NLS) and the Allen-Cahn equations.      ", "Subject": "Numerical Analysis (math.NA)", "ID": "arXiv:2201.00002", "Text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE\nCONSERVATION AND DISSIPATION LAWS\n\nSATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\nABSTRACT. The time dependent spectral renormalization (TDSR) method was introduced by Cole and Musslimani as\na novel way to numerically solve initial boundary value problems. An important and novel aspect of the TDSR scheme\nis its ability to incorporate physics in the form of conservation laws or dissipation rate equations. However, the method\nwas limited to include a single conserved or dissipative quantity.\n\nThe present work significantly extends the computational features of the method with the (i) incorporation of multiple\nconservation laws and/or dissipation rate equations, (ii) ability to enforce versatile boundary conditions, and (iii) higher\norder time integration strategy. The TDSR method is applied on several prototypical evolution equations of physical\nsignificance. Examples include the Korteweg-de Vries (KdV), multi-dimensional nonlinear Schro\u0308dinger (NLS) and the\nAllen-Cahn equations.\n\nKeywords: Renormalization method, initial boundary value problems, partial differential equations, Duhamel\u2019s\nprinciple, nonlinear waves, soliton equations, Hamiltonian and dissipative systems.\n\n1. INTRODUCTION\n\nNumerical simulation of initial boundary value problems is of utmost importance in several engineering and\nscientific disciplines. Over the last few decades, several time-stepping methods have been developed and proposed\nto achieve this goal. Among them are the class of implicit/explicit Runge-Kutta methods [35], exponential time-\ndifferencing [12, 25, 37, 43] and the split-step operator splitting [40] to name a few. For evolution equations that\narise in physical applications, it is highly desirable to devise time-stepping schemes that reflect the underlying\nphysics at hand. Such structure preserving numerical schemes are of paramount importance for long-time integra-\ntion, where either it is necessary to ensure numerical stability (e.g., if the numerics could conserve the L2 norm of\nthe solution for the KdV/NLS), or preserve other features (such as capturing the correct shock speed in the con-\ntext of systems of hyperbolic partial differential equations).To date, there are various ways to input some physics\ninto the numerical time-integration. For example, the geometric/symplectic integrators that preserve the Hamil-\ntonian and symplectic structure [18], the operator splitting method that was used for the NLS equation to preserve\nthe power and the non-linear dispersion relation [40], the multi-symplectic schemes designed for the generalized\nSchro\u0308dinger equations [21\u201323], the Taha-Ablowitz [38], the Ablowitz-Ladik [6], and the Ablowitz-Musslimani\n[2, 4] schemes that preserve the integrable structure of the KdV, NLS and class of nonlocal NLS equations, re-\nspectively. Other relevant works correspond to the conservative finite volume Godunov schemes [16, 19, 28],\nfinite difference schemes that preserve the energy or dissipation property of the underlying model equation (see,\ne.g.,[15, 36]), as well as a finite volume scheme that conserve mass and momentum of the KdV equation (see, e.g.,\n[13]). Recently, Cole and Musslimani [11] proposed an alternative method to simulate dynamical systems that\nenables the inclusion of physics \u201con-demand\u201d. The core idea is to make use of the Duhamel\u2019s principle to recast\nthe underlying evolution equation as a space-time integral equation. The resulting system is then solved iteratively\nusing a novel time-dependent renormalization process that controls both the numerical convergence properties of\nthe scheme while at the same time preserving a single physical law.\n\nIn this paper, we extend the time-dependent spectral renormalization method to allow for multiple conserva-\ntion laws or dissipation rates to be simultaneously incorporated in the simulation. This is achieved by introducing\nas many time-dependent renormalization factors as the number of conservation/dissipation laws being enforced.\nThe solution sought is then written as a linear superposition of finite number of space-time dependent auxiliary\nwave functions with the time-dependent renormalization factors envisioned to play the role of \u201cexpansion coeffi-\ncients\u201d. When inserted into the corresponding Duhamel\u2019s formula, a finite set of \u201csub-Duhamel\u201d integral equations\n\n1\n\nar\nX\n\niv\n:2\n\n20\n1.\n\n00\n00\n\n2v\n1 \n\n [\nm\n\nat\nh.\n\nN\nA\n\n] \n 1\n\n9 \nD\n\nec\n 2\n\n02\n1\n\n\n\n2 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\nare obtained governing the space-time dynamics of each individual auxiliary function. These integral equations\nare then numerically solved using a novel space-time fixed point iteration. The importance of such a dynamic\nrenormalization process are twofold: (i) it provides convergence when needed and (ii) enables the inclusion of\nconservation/dissipation laws. The Duhamel integrals are numerically computed using a Cauchy-Filon-Simpson\nquadrature formula. The TDSR method is implemented on several prototypical evolution equations of physical\nsignificance. This includes the KdV, Allen-Cahn, multi-dimensional and the PT symmetric integrable nonlocal\nNLS equations.\n\nThe paper is organized as follows. In Sec. 2 we put forward a general framework for the TDSR scheme in\narbitrary space dimensions and show how to incorporate multiple conservation laws or dissipation rate equations\ninto the algorithm. The Cauchy-Filon Simpson time integration is derived in Sec. 3.1 for evolution equations\nsubject to either periodic or rapidly decaying boundary conditions. The TDSR scheme is applied on the KdV\nand NLS equations, with single and multiple conservation laws. In Sec. 3.2 the Cauchy-Filon trapezoidal time\nintegration is developed for evolution equations subject to non-periodic and non decaying boundary conditions. In\nthis regard, the Allen-Cahn equation is used as a test bed to assess the performance of the algorithm. We conclude\nin Sec. 7 with comments on future directions.\n\n2. TDSR AND DUHAMEL PRINCIPLE\n\nIn this section, we formulate the TDSR method using Duhamel\u2019s principle in conjunction with multiple conser-\nvation laws. Consider the evolution equation for the real (or complex) valued function u(x, t):\n\nut = L (u)+N (u), u(x,0) = u0(x), (2.1)\nwhere L is a linear, constant coefficient differential operator and N (u) is a nonlinear operator. The initial-\nboundary value problem (2.1) is posed on a spatial domain \u2126 that is either bounded or unbounded. Furthermore,\nEq. (2.1) is supplemented with either periodic, rapidly decaying, or other types of boundary conditions. As men-\ntioned above, we are interested in evolution equations that are either (i) conservative, in which case, there exists N\nconserved quantities given by\n\nQm(u)\u2261\n\u222b\n\n\u2126\n\nQm[u(x, t)]dx =\n\u222b\n\n\u2126\n\nQm[u0(x)]dx\u2261Cm , m = 1,2,3, \u00b7 \u00b7 \u00b7N, (2.2)\n\nor (ii) dissipative, so that there are N densities \u03c1m and fluxes Fm that obey the rate equations\nd\ndt\n\n\u222b\n\u2126\n\n\u03c1m[u(x, t)]dx =\u2212\n\u222b\n\n\u2126\n\nFm[u(x, t)]dx , m = 1,2,3, \u00b7 \u00b7 \u00b7 ,N. (2.3)\n\nEquation (2.1) is rewritten in an integral form using the Duhamel\u2019s principle:\n\nu(x, t) = etL [u0(x)]+\n\u222b t\n\n0\ne(t\u2212\u03c4)L N [u(x,\u03c4)]d\u03c4. (2.4)\n\nRecall that for any periodic or L2 function w(x), the semi-group etL admits the spectral representation:\n\netL [w(x)] = F\u22121[exp(tL\u0302 )F [w(x)]], (2.5)\n\nwhere L\u0302 is the Fourier symbol associated with L and F , F\u22121 denote the d-dimensional forward and inverse\nFourier transforms, defined by\n\nw\u0302(k)\u2261F [w(x)] = (2\u03c0)\u2212d/2\n\u222b\nRd\n\nw(x)e\u2212ik\u00b7xdx, (2.6)\n\nF\u22121[w\u0302(k)] = (2\u03c0)\u2212d/2\n\u222b\nRd\n\nw\u0302(k)eik\u00b7xdk. (2.7)\n\nFor periodic functions defined on a bounded spatial domain the forward Fourier integral (2.6) represents the coef-\nficients of its Fourier series.\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 3\n\nWe now outline in details how to incorporate multiple conservation laws or dissipation rate equations in the\nTDSR scheme. To this end, we seek a solution to Eq. (2.4) in the form\n\nu(x, t) =\nN\n\n\u2211\nj=1\n\nR j(t)v j(x, t), (2.8)\n\nwhere R j(t) are time-dependent renormalization factors to be determined from knowledge of the conservation or\ndissipation laws and v j(x, t) are space-time dependent auxiliary functions that satisfy the same boundary conditions\nas the solution u(x, t). Our extensive numerical experiments seem to indicate that in order for the TDSR iteration\nto converge, the initial condition u0(x) needs to be re-written as a sum of N (identically non-zero) functions f j(x)\n(here referred to as pseudo initial conditions). Namely, we write\n\nu0(x) =\nN\n\n\u2211\nj=1\n\nf j(x), (2.9)\n\nwhere each f j is chosen to be compatible with the underlying boundary conditions. The specific choice of the\nfunctions { f j(x)}, j = 1,2, \u00b7 \u00b7 \u00b7 ,N, is discussed in Sec. (5) when solving the KdV equation. Substituting Eqns. (2.8)\nand (2.9) into (2.4), we obtain an equation for the auxiliary functions v j(x, t), j = 1,2, \u00b7 \u00b7 \u00b7N:\n\nN\n\n\u2211\nj=1\n\nR j(t)v j(x, t) =\nN\n\n\u2211\nj=1\n\netL [ f j(x)]+\n\u222b t\n\n0\ne(t\u2212\u03c4)L N\n\n[\nN\n\n\u2211\nj=1\n\nR j(\u03c4)v j(x,\u03c4)\n\n]\nd\u03c4. (2.10)\n\nScrutinizing Eq. (2.10) reveals the existence of N\u22121 degrees of freedom for the variables v1,v2, \u00b7 \u00b7 \u00b7 ,vN\u22121. Next,\nwe outline how to eliminate each degree of freedom and derive a self-consistent set of equations that forms the\nbasis for the TDSR scheme. To begin with, we choose v1(x, t) such that\n\nv1(x, t) = M1[R1,v1]\u2261\n1\n\nR1(t)\n\n{\netL [ f1(x)]+\n\n\u222b t\n0\n\ne(t\u2212\u03c4)L N [R1(\u03c4)v1(x,\u03c4)]d\u03c4\n}\n. (2.11)\n\nThe rationale behind this choice is rooted in the fact that Eq. (2.11) must reduce back to the case when only\none conservation or dissipation law is under consideration with f1(x) \u2261 u0(x). With this at hand, we next require\nv2(x, t) to satisfy\n\nv2(x, t) = M2[R1,R2,v1,v2]\n\n\u2261\n1\n\nR2(t)\n\n(\netL [ f2(x)]+\n\n\u222b t\n0\n\ne(t\u2212\u03c4)L N [R1(\u03c4)v1(x,\u03c4)+R2(\u03c4)v2(x,\u03c4)]d\u03c4\n)\n\n\u2212\n1\n\nR2(t)\n\n\u222b t\n0\n\ne(t\u2212\u03c4)L N [R1(\u03c4)v1(x,\u03c4)]d\u03c4, (2.12)\n\nand for general j = 3,4, \u00b7 \u00b7 \u00b7 ,N,\nv j(x, t) = M j[R1,R2, \u00b7 \u00b7 \u00b7 ,R j;v1,v2, \u00b7 \u00b7 \u00b7 ,v j] (2.13)\n\n\u2261\n1\n\nR j(t)\netL [ f j(x)]+\n\n1\nR j(t)\n\n\u222b t\n0\n\ne(t\u2212\u03c4)L N\n\n[\nj\n\n\u2211\n`=1\n\nR`(\u03c4)v`(x,\u03c4)\n\n]\nd\u03c4\n\n\u2212\n1\n\nR j(t)\n\n\u222b t\n0\n\ne(t\u2212\u03c4)L N\n\n[\nj\u22121\n\n\u2211\n`=1\n\nR`(\u03c4)v`(x,\u03c4)\n\n]\nd\u03c4,\n\n\n\n4 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\nNote that Eqns. (2.11)-(2.13) are self-consistent with the Duhamel\u2019s formula (2.10). Indeed, multiplying (2.12)\nby R2 and (2.13) by R j and summing over all j = 2,3, \u00b7 \u00b7 \u00b7 ,N, we obtain\n\nN\n\n\u2211\nj=2\n\nR j(t)v j(x, t) = etL\n[\n\nN\n\n\u2211\nj=2\n\nf j(x)\n\n]\n\n+\n\u222b t\n\n0\ne(t\u2212\u03c4)L\n\nN\n\n\u2211\nj=2\n\n{\nN\n\n[\nj\n\n\u2211\n`=1\n\nR`(\u03c4)v`(x,\u03c4)\n\n]}\nd\u03c4\n\n\u2212\n\u222b t\n\n0\ne(t\u2212\u03c4)L\n\nN\n\n\u2211\nj=2\n\n{\nN\n\n[\nj\u22121\n\n\u2211\n`=1\n\nR`(\u03c4)v`(x,\u03c4)\n\n]}\nd\u03c4. (2.14)\n\nThe last two terms on the right hand side of Eq. (2.14) satisfy\u222b t\n0 d\u03c4e\n\n(t\u2212\u03c4)L\n\u2211\n\nN\nj=2\n\n{\nN\n[\n\u2211\n\nj\n`=1 R`(\u03c4)v`(x,\u03c4)\n\n]\n\u2212N\n\n[\n\u2211\n\nj\u22121\n`=1 R`(\u03c4)v`(x,\u03c4)\n\n]}\n=\n\u222b t\n\n0 d\u03c4e\n(t\u2212\u03c4)L N\n\n[\n\u2211\n\nN\n`=1 R`(\u03c4)v`(x,\u03c4)\n\n]\n\u2212N [R1(\u03c4)v1(x,\u03c4)]. (2.15)\n\nThe conclusion is complete once we multiply Eq. (2.11) by R1(t); add the result to Eq. (2.15) and use the condition\n(2.9). In summary, Eqns. (2.11)-(2.13) give an implicit integral representation for the auxiliary functions v j. To\nclose the system, all we need is to compute the renormalization factors R j(t). Substituting Eq. (2.8) into Eqns. (2.2)\nand (2.3) gives:\n\nConservative case:\n\nQm\n\n(\nN\n\n\u2211\n`=1\n\nR`(t)v`(x, t)\n\n)\n\u2261\n\u222b\n\n\u2126\n\nQm\n\n(\nN\n\n\u2211\n`=1\n\nR`(t)v`(x, t)\n\n)\ndx =Cm, (2.16)\n\nDissipative case:\nd\ndt\n\n\u222b\n\u2126\n\n\u03c1m\n\n[\nN\n\n\u2211\n`=1\n\nR`(t)v`(x, t)\n\n]\ndx =\u2212\n\n\u222b\n\u2126\n\nFm\n\n[\nN\n\n\u2211\n`=1\n\nR`(t)v`(x, t)\n\n]\ndx, (2.17)\n\nwhere m = 1,2, \u00b7 \u00b7 \u00b7 ,N. System (2.16) defines N algebraic equations for the time-dependent renormalization factors\nwhereas (2.17) a set of coupled nonlinear ordinary differential equations governing the evolution of R j(t). With\nthis at hand, the TDSR iterative process is summarized below:\n\nv(n+1)1 = M1[R\n(n)\n1 (t),v\n\n(n)\n1 ], (2.18)\n\nv(n+1)2 = M2[R\n(n)\n1 ,R\n\n(n)\n2 ,v\n\n(n)\n1 ,v\n\n(n)\n2 ], (2.19)\n\nv(n+1)j = M j[R\n(n)\n1 ,R\n\n(n)\n2 , \u00b7 \u00b7 \u00b7 ,R\n\n(n)\nj ,v\n\n(n)\n1 ,v\n\n(n)\n2 , \u00b7 \u00b7 \u00b7 ,v\n\n(n)\nj ] , j = 3, \u00b7 \u00b7 \u00b7 ,N, (2.20)\n\nwith R(n)j , j = 1,2, \u00b7 \u00b7 \u00b7 ,N given by (for conservative cases)\n\nQm\n\n(\nN\n\n\u2211\n`=1\n\nR(n)` (t)v\n(n)\n` (x, t)\n\n)\n\u2261Cm , (2.21)\n\nand\n\nd\ndt\n\n\u222b\n\u2126\n\n\u03c1m\n\n[\nN\n\n\u2211\n`=1\n\nR(n)` (t)v\n(n)\n` (x, t)\n\n]\ndx =\u2212\n\n\u222b\n\u2126\n\nFm\n\n[\nN\n\n\u2211\n`=1\n\nR(n)` (t)v\n(n)\n` (x, t)\n\n]\ndx, (2.22)\n\nfor the dissipative cases where m = 1,2, \u00b7 \u00b7 \u00b7 ,N. As a reminder, the functionals M1 and M j, j = 2,3, \u00b7 \u00b7 \u00b7 ,N are\nrespectively defined by Eqns. (2.11) - (2.13). A workflow for the TDSR algorithm is given below, clarifying the\nstructure of the iterative process:\n\n(1) Choose the pseudo initial conditions: The set of pseudo initial conditions f j(x), j = 1,2, \u00b7 \u00b7 \u00b7 N, are\nchosen in such a way that Eq. (2.9) is satisfied (see Sec.5 for further details). Note that they are used in\nthe Picard iterations defined by Eqs. (2.11)-(2.13).\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 5\n\n(2) Select initial guesses v(1)j (x, t) for j = 1,2, \u00b7 \u00b7 \u00b7 ,N: We seed Eqs. (2.11)-(2.13) with these initial guesses\nfor the space-time dependent auxiliary functions.\n\n(3) Compute the initial iterate of the set of renormalization factors: The set of auxiliary functions are used\nto evaluate the time-dependent renormalization factors R(1)j (t) , j = 1,2, \u00b7 \u00b7 \u00b7 ,N via the system of equations\n(2.21) or (2.22) depending on whether the underlying evolution equation is conservative or dissipative.\n\n(4) Compute the Duhamel integrals defined in Eqs. (2.18)-(2.20): The Duhamel integrals are computed\nusing v(1)j (x, t) and R\n\n(1)\nj (t), for j = 1,2, \u00b7 \u00b7 \u00b7 ,N.\n\n(5) Update the Duhamel iteration: The Duhamel integrals computed in the previous step are now used to\ncompute the second iterate of v(2)j (x, t) using Eqs. (2.18)-(2.20).\n\n(6) Update the renormalization factors: The updated {v(2)j (x, t)}\u2019s are now used to correct the the set of\nrenormalization factors {R(2)j (t)} , j = 1,2, \u00b7 \u00b7 \u00b7 ,N; from the system of equations given by (2.21) (for the\nconservative case), or (2.22) (for the dissipative case).\n\n(7) Iterative update: Repeat steps (5) and (6) till convergence is achieved.\n\n3. TIME INTEGRATION WITH VARIOUS BOUNDARY CONDITIONS\n\n3.1. Periodic and decaying boundary conditions. In this section, we detail the numerical approach used to\napproximate the Duhamel integral\n\nI(x, t)\u2261\n\u222b t\n\n0\ne(t\u2212\u03c4)L G(x,\u03c4)d\u03c4, (3.1)\n\nwith G(x,\u03c4) \u2261N [u(x,\u03c4)]. When subject to periodic or rapidly decaying boundary conditions, the action of the\nsemi-group exp(tL ) on G follows from its spectral representation F [exp(tL )G] = exp[tL\u0302 ]F [G] where L\u0302 is\nthe Fourier symbol associated with the constant coefficients linear operator L . Our approach in approximating the\nintegral Eq. (3.1) is based on the Filon-Simpson quadrature method [9, 20, 34]. To this end, we consider an NT\nequally spaced mesh points residing inside the time interval [0,T ] with ti = i\u2206t, i = 0,1,2, \u00b7 \u00b7 \u00b7 ,NT , labeling all grid\npoints. It can be shown that I\u0302(k, ti) satisfies the exact recurrence relation\n\nI\u0302(k, ti+1) = e2\u2206tL\u0302 (k)\n[\n\nI\u0302(k, ti\u22121)+\n\u222b ti+1\n\nti\u22121\ne(ti\u22121\u2212\u03c4)L\u0302 (k)G\u0302(k,\u03c4)d\u03c4\n\n]\n. (3.2)\n\nAs a reminder, a hat over a quantity represents its Fourier transform (see definition (2.6)) or its Fourier series\ncoefficients. Next, we approximate the function G\u0302(k,\u03c4) by a quadratic polynomial defined in the interval [ti\u22121, ti+1]\n\nG\u0302(k,\u03c4) \u2248 G\u0302(k, ti\u22121)\n(\u03c4\u2212 ti)(\u03c4\u2212 ti+1)\n\n2(\u2206t)2\n\u2212 G\u0302(k, ti)\n\n(\u03c4\u2212 ti\u22121)(\u03c4\u2212 ti+1)\n(\u2206t)2\n\n+ G\u0302(k, ti+1)\n(\u03c4\u2212 ti\u22121)(\u03c4\u2212 ti)\n\n2(\u2206t)2\n. (3.3)\n\nSubstituting Eq. (3.3) back into (3.2) and integrating by parts gives a recursive formula for the Duhamel integral\n(3.1):\n\nI\u0302(k, ti+1) = e2\u2206tL\u0302 (k)[I\u0302(k, ti\u22121)+q1G\u0302(k, ti\u22121)+q2G\u0302(k, ti)+q3G\u0302(k, ti+1)]. (3.4)\nThe quadrature coefficients q j \u2261 q j(k,\u2206t), j = 1,2,3, depend on the Fourier wavenumber and the time step \u2206t but\nnot on the iteration index i. Thus, they are computed only once. The exact expressions for the q j\u2019s, j = 1,2,3, are\ngiven by (z\u2261 \u2206tL\u0302 (k))\n\nq1 = \u2206t(\u2212ze\u22122z\u22122e\u22122z +2z2\u22123z+2)/(2z3), (3.5a)\nq2 = \u2206t(2ze\n\n\u22122z +2e\u22122z +2z\u22122)/z3, (3.5b)\nq3 = \u2206t(\u22122z2e\u22122z\u22123ze\u22122z\u22122e\u22122z\u2212 z+2)/(2z3). (3.5c)\n\nFor linear operators satisfying L\u0302 (0) = 0 we find (in the limiting case k\u2192 0), q1(0,\u2206t) = q2(0,\u2206t)/4= q3(0,\u2206t)\u2261\n\u2206t/3. Equation (3.4) needs to be initialized with I\u0302(k, t = 0)= 0 and the quantity I\u0302(k,\u2206t)=\n\n\u222b\n\u2206t\n0 e\n\n(\u2206t\u2212\u03c4)L\u0302 (k)G\u0302(k,\u03c4)d\u03c4\n\n\n\n6 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\nwhich we next explain how to find. Note that in the interval [0,\u2206t], the values (in time) of the function G\u0302(k,\u03c4)\nare available only at two grid points: 0 and \u2206t. To maintain the same order of accuracy as was done at the other\ntemporal grid points, we apply a combination of two different quadrature rules to approximate I\u0302(k,\u2206t). First,\nconsider the following identity:\u222b 3\u2206t\n\n0\ne(\u2206t\u2212\u03c4)L\u0302 (k)G\u0302(k,\u03c4)d\u03c4 =\n\n\u222b\n\u2206t\n\n0\ne(\u2206t\u2212\u03c4)L\u0302 (k)G\u0302(k,\u03c4)d\u03c4\ufe38 \ufe37\ufe37 \ufe38\n\nI\u0302(k,\u2206t)\n\n+\n\u222b 3\u2206t\n\n\u2206t\ne(\u2206t\u2212\u03c4)L\u0302 (k)G\u0302(k,\u03c4)d\u03c4. (3.6)\n\nThe second integral on the right-hand side of Eq. (3.6) is computed using a quadratic interpolation (in time) for\nG\u0302(k,\u03c4). Indeed, after some algebra, we find\u222b 3\u2206t\n\n\u2206t\ne(\u2206t\u2212\u03c4)L\u0302 (k)G\u0302(k,\u03c4)d\u03c4 \u2248 q1G\u0302(k,\u2206t)+q2G\u0302(k,2\u2206t)+q3G\u0302(k,3\u2206t). (3.7)\n\nTo obtain a similar order of accuracy for the integral on the left hand side of Eq. (3.6), we first represent G\u0302(k,\u03c4) as\na cubic polynomial (in time)\n\nG\u0302(k,\u03c4) \u2248 \u2212G\u0302(k,0)\n(\u03c4\u2212\u2206t)(\u03c4\u22122\u2206t)(\u03c4\u22123\u2206t)\n\n6(\u2206t)3\n+ G\u0302(k,\u2206t)\n\n\u03c4(\u03c4\u22122\u2206t)(\u03c4\u22123\u2206t)\n2(\u2206t)3\n\n\u2212 G\u0302(k,2\u2206t)\n\u03c4(\u03c4\u2212\u2206t)(\u03c4\u22123\u2206t)\n\n2(\u2206t)3\n+ G\u0302(k,3\u2206t)\n\n\u03c4(\u03c4\u2212\u2206t)(\u03c4\u22122\u2206t)\n6(\u2206t)3\n\n. (3.8)\n\nSubstituting expressions (3.8) and (3.7) into Eq. (3.6) gives (after integration by parts)\n\nI\u0302(k,\u2206t) = q4e\u2206tL\u0302 (k)G\u0302(k,0)+\n(\n\nq5e\n\u2206tL\u0302 (k)\u2212q1\n\n)\nG\u0302(k,\u2206t) (3.9)\n\n+\n(\n\nq6e\n\u2206tL\u0302 (k)\u2212q2\n\n)\nG\u0302(k,2\u2206t)+\n\n(\nq7e\n\n\u2206tL\u0302 (k)\u2212q3\n)\n\nG\u0302(k,3\u2206t).\n\nHere, q j \u2261 q j(k,\u2206t), j = 4,5,6,7, denote the quadrature coefficients whose expressions are given by\n\nq4 = \u2206t(2z\n2e\u22123z +6ze\u22123z +6e\u22123z +6z3 +12z\u221211z2\u22126)/(6z4), (3.10a)\n\nq5 = \u2206t(\u22123z2e\u22123z\u22128ze\u22123z\u22126e\u22123z +6z2\u221210z+6)/(2z4), (3.10b)\nq6 = \u2206t(6z\n\n2e\u22123z +10ze\u22123z +6e\u22123z\u22123z2 +8z\u22126)/(2z4), (3.10c)\nq7 = \u2206t(\u22126z3e\u22123z\u221211z2e\u22123z\u221212ze\u22123z\u22126e\u22123z +2z2\u22126z+6)/(6z4). (3.10d)\n\nFor linear operators satisfying L\u0302 (0)= 0, and for wavenumber k\u2192 0, q4(0,\u2206t)= q7(0,\u2206t)\u2261 3\u2206t/8, and q5(0,\u2206t)=\nq6(0,\u2206t)\u2261 9\u2206t/8. To summarize, the Duhamel integral I(x, t) is determined from iterating Eq. (3.4) subject to the\ninitial conditions: I(x,0) = 0 and I(x,\u2206t) given in Fourier space by Eq. (3.9). Note that in some cases, the Filon\ncoefficients q j may exhibit a removable singularity in the variable z \u2261 \u2206tL\u0302 (k) at zero wave number that could\ntrigger numerical instability. To remedy this, we represent each quadrature term as a Cauchy integral that allows a\nstable and uniform approximation valid for all wavenumbers. This idea has been first implemented in the context\nof exponential time differencing fourth order Runge-Kutta (ETDRK4) [25]. For the sake of completeness, we\nshow how to implement this approach on the coefficient q1. The computation of the other quadrature coefficients\nfollow similar derivation. Since the function q1(\u03b6 ;\u2206t) is analytic in the \u03b6 complex plane, by the Cauchy integral\nformula we have\n\nq1(z;\u2206t) =\n1\n\n2\u03c0i\n\n\u222b\nC\n\nq1(\u03b6 ;\u2206t)\n\u03b6 \u2212 z\n\nd\u03b6 , (3.11)\n\nwhere C is a circle of constant radius centered at z. The above integral can be evaluated to spectral accuracy with\nthe use of the trapezoidal quadrature [14, 25].\n\n3.2. Time integration: non-periodic boundary conditions. So far, we have discussed the development and\napplication of the TDSR method to evolution equations subject to periodic or localized boundary conditions. Here,\nwe intend to extend the TDSR scheme to allow for non-periodic and non-decaying boundary conditions where the\nuse of Fourier analysis is not applicable. In such circumstances the matrix approximating the linear operator could\nbe banded (as is the case with finite differences) or dense, for example, in case of Chebyshev spectral method.\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 7\n\nThe derivation of the Duhamel formula follows similar steps as outlined in Sec.2 with the exception of the use\nof trapezoidal scheme instead of Simpson. Using a Chebyshev basis function or other discretization methods we\nrepresent the differential operator L in Eq. (3.1) by a finite dimensional matrix L. The boundary conditions are\nincorporated within the matrix L. By creating a mesh in time domain, Eq.(3.1) can be put in the recursive form\n\nI(ti+1) = e\u2206tLI(ti)+ e\u2206tL\n\u222b ti+1\n\nti\ne(ti\u2212\u03c4)LG(\u03c4)d\u03c4, (3.12)\n\nwhere now I(ti) is the matrix representing the Duhamel integral at space meshgrid x and time level ti. Additionally,\nG(ti) is the matrix representing the nonlinear terms at ti = i\u2206t with i = 0,1, \u00b7 \u00b7 \u00b7NT . Using a linear interpolant to\napproximation G(\u03c4) in the interval [ti, ti+1] we find\n\nG(\u03c4)\u2248G(ti)+\nG(ti+1)\u2212G(ti)\n\n\u2206t\n(\u03c4\u2212 ti). (3.13)\n\nSubstituting Eq.(3.13) into (3.12) we obtain after some algebra\n\nI(ti+1) = e\u2206tL[I(ti)+AG(ti)+BG(ti+1)], (3.14)\nwhere the matrix valued quadrature coefficients A\u2261 A(L,\u2206t) and B\u2261 B(L,\u2206t) are defined by\n\nA\u2261 \u2206tA\u0303, A\u0303 = L\u0303\u22122\n(\n\ne\u2212L\u0303 + L\u0303\u2212I\n)\n, (3.15)\n\nB\u2261 \u2206tB\u0303, B\u0303 = L\u0303\u22122\n(\nI \u2212 L\u0303e\u2212L\u0303\u2212 e\u2212L\u0303\n\n)\n, (3.16)\n\nwith I being the identity matrix and L\u0303\u2261 \u2206tL. As was done in Sec. 3.1 for the periodic case [25], we again adopt\nthe Cauchy integral formula to represent each quadrature coefficient as a contour integral in the complex plane.\nThus we write :\n\nA\u0303(L\u0303) =\n1\n\n2\u03c0i\n\n\u222b\n\u0393\n\nA\u0303(\u03b6 )(\u03b6I \u2212 L\u0303)\u22121d\u03b6 , B\u0303(L\u0303) =\n1\n\n2\u03c0i\n\n\u222b\n\u0393\n\nB\u0303(\u03b6 )(\u03b6I \u2212 L\u0303)\u22121d\u03b6 (3.17)\n\nwith \u0393 being a circular contour that encloses all the eigenvalues of L\u0303. The integral in Eq.(3.17) is computed to a\nspectral accuracy with the use of the trapezoidal rule.\n\n4. NUMERICAL IMPLEMENTATION OF TDSR: ONE CONSERVATION OR DISSIPATION LAW WITH VARIOUS\nBOUNDARY CONDITIONS\n\n4.1. The KdV equation. In this section, we use the KdV equation as a testbed PDE model to examine various\nnumerical aspects related to the TDSR method such as convergence, accuracy and dependence on initial guesses.\nThe KdV equation is given by:\n\nut +\u03b1uux + \u03b5\n2uxxx = 0, (4.1)\n\nwhere \u03b1,\u03b5 are real and positive numbers. When considered on the whole real line with rapidly decaying boundary\nconditions, Eq.(4.1) admits a one parameter family of soliton solution given by (e.g. \u03b1 = 6,\u03b5 = 1)\n\nuex(x, t) = 2\u03b2\n2sech2(\u03b2 (x\u22124\u03b2 2t)), \u03b2 > 0. (4.2)\n\nIt is noteworthy that the KdV equation is an integrable dynamical system admitting infinitely many conservation\nlaws. Among them are the physically relevant mass, momentum and Hamiltonian given for \u03b1 = 6,\u03b5 = 1 by\nEq. (2.2) with Q1 = u, Q2 = u2 and Q3 = \u2212u3 + 12 u\n\n2\nx , respectively. All numerical simulations reported in this\n\nsection were performed on a spatial domain of size L = 100 or L = 800 (depending on the case at hand) with\ncorresponding number of spatial grid points (Fourier modes) NS = 2048, NS = 16384 respectively and time interval\n[0,T ] with T = 5,10,20,30 or 60. In this section, the renormalization factors were computed by enforcing a single\nconservation law. Numerical convergence and accuracy were quantified by monitoring the error between two\nsuccessive iterations\n\nmax\nx,t\n|u(n+1)(x, t)\u2212u(n)(x, t)|,\n\n\n\n8 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\nand the quantities\n\n\u03b4u(t)\u2261max\nx\n|u(x, t)\u2212uex(x, t)|, (4.3)\n\n\u03b4Q j(t)\u2261Q j[u(x, t)]\u2212Q j[u0(x)] , j = 1,2,3, (4.4)\n\nwhere u0(x) = 2\u03b2 2sech\n2(\u03b2x) is the initial condition associated with Eq. (4.1), for the parameters \u03b1 = 6,\u03b5 = 1,\n\nwhere uex(x, t) is the one-parameter soliton solution for the KdV given in Eq. (4.2). For all simulations reported\nhere, convergence tolerance was set near 1\u00d7 10\u221216. All functionals Q j, j = 1,2,3, are defined in Eq. (2.2). We\nperform numerical experiments on the KdV equation while conserving one of the following quantities: mass,\nmomentum, or Hamiltonian. The renormalization factor R(t) in each case, is given by:\n\nmass: R(t) =\nQ1[u0(x)]\nQ1[v(x, t)]\n\n, (4.5)\n\nmomentum: R(t) =\n(\n\nQ2[u0(x)]\nQ2[v(x, t)]\n\n)1/2\n, (4.6)\n\nHamiltonian: A(t)R3(t)\u2212B(t)R2(t)\u2212C3 = 0, (4.7)\nwhere A(t) =\u2212\n\n\u222b\nR v\n\n3(x, t)dx, B(t) =\u2212 12\n\u222b\nR v\n\n2\nx dx and C3 is the initial value of the Hamiltonian. All spatial integrals\n\nare computed to spectral accuracy with the use of fast Fourier transform. We initialize the TDSR algorithm with\na space-time random function v(1)(x, t) constructed by superimposing several Gaussians each being centered at a\nrandom location and having a random time-dependent amplitude. The centers and amplitudes are sampled from\na uniform distribution on the interval [\u2212L/2,L/2] and [\u22121,1] respectively. To ensure the initial guess v(1)(x, t)\nsatisfies the underlying boundary conditions, we mollify it with \u03c7(x). Thus, we have\n\nv(1)(x, t) =\n\u2211\n\nNG\nn=1 an(t)exp\n\n[\n\u2212\n( x\u2212cn\n\nd\n\n)2]\nmaxx,t |\u2211\n\nNG\nn=1 an(t)exp\n\n[\n\u2212\n( x\u2212cn\n\nd\n\n)2]|\u03c7(x), (4.8)\nwhere NG denotes the number of Gaussians with cn and an(t) representing their centres and time-varying ampli-\ntudes and d is the width. Here, \u03c7(x) is the mollifier with unit peak amplitude defined by:\n\n\u03c7(x) =\n\n\uf8f1\uf8f2\n\uf8f3exp\n\n(\nb\na2\n\u2212 b\n\na2\u2212x2\n\n)\n, if x \u2208 (\u2212a,a)\n\n0, if |x|> a,\n(4.9)\n\nwith arbitrary mollifier parameters a and b. As expected, the numerical result agrees well with the exact solution.\nIn generating Fig. 1, conservation of momentum is imposed in which case the renormalization parameter R(t) is\ncomputed from Eq. (4.6). One could instead reach the same conclusion by using a different dynamic renormal-\nization process emanating from either conservation of mass or Hamiltonian. This numerical experiment reveals\nthe simplistic (albeit powerful) nature of our proposed method as measured by its easy formulation, actual im-\nplementation, robustness to initial guesses and its ability to impose conservation laws \u201con-demand\u201d. To further\ncharacterize the numerical performance of the TDSR scheme, we have investigated its temporal convergence prop-\nerties by measuring the space-time maximum error between the numerically obtained solution to the KdV equation\n(relative to its exact solution) and all conservation laws, as quantified by Eqs. (4.3) and (4.4), as a decreasing func-\ntion of time step \u2206t. It is evident from Fig. 2(a) that the maximum (over time) error in the solution decreases at\na fourth order rate. This trend seems to persist independently of the choice of specific conservation law. When\nconservation of mass is imposed, the error in the Hamiltonian and momentum reduces with decreasing \u2206t. Similar\nscenarios occur when imposing conservation of momentum or Hamiltonian \u2013 see Fig. 2 (b)-(d). We remark that\nto conserve the Hamiltonian structure, we need to solve a cubic equation defined by Eq. (4.7). As such, there are\nthree possible expressions for the renormalization factor, of which only one is feasible. It turns out that the right\nexpression yields R(0)v(x,0) = u0(x) at any Duhamel iteration, while the other two roots violate this criterion.\nFinally, for cases where the renormalization factor satisfies an associated equation that lacks exact solution, one\nneeds to resort to a root finding algorithms such as the Newton\u2019s method. It is interesting to note that when it\ncomes to long time simulations, the TDSR performs optimally when imposing conservation of momentum rather\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 9\n\n0 5 10 15 20\nt\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\nj/\nu\n(t\n\n)j\njju\n\ne\nx\n(x\n\n;t\n)j\nj 1\n\n#10 -10 (c)\n\nFIGURE 1. (a) A random space-time initial guess constructed from a linear superposition of randomly\ncentered Gaussians with random amplitudes \u2013 see Eq.(4.8). (b) Numerical solution for the KdV equation\nobtained from the TDSR algorithm after 30 Duhamel iterations. Parameters are: T = 20,\u2206t = 0.025,L =\n100,NS = 2048 (Fourier modes). Here, the wave speed is 4\u03b2 2 = 2/5. This figure was generated by impos-\ning conservation of momentum for which the renormalization parameter R(t) is computed from Eq. (4.6).\nThe soliton initial condition is u0(x) = 2\u03b2 2sech\n\n2(\u03b2x) with \u03b1 = 6 and \u03b5 = 1. (c) Time evolution of the\nrelative error between the TDSR and the exact solution. Mollifier parameters are a = 0.95\u00d7 L2 , b = 1.\n\n10 -2 10 -1 10 0\n\n\"t\n\n10 -10\n\n10 -5\n\n10 0\n\nm\na\nx\n\ntj/\nu\n(t\n\n)j\n=\njju\n\ne\nx\n(x\n\n;t\n)j\nj 1\n\n(a)\n\n10 -2 10 -1 10 0\n\n\"t\n\n10 -30\n\n10 -15\n\n10 0\n\nm\na\nx\n\ntj/\nQ\n\n1\n(t\n\n)j\n=\njQ\n\n1\n[u\n\n0\n]j\n\n(b)\n\n10 -2 10 -1 10 0\n\n\"t\n\n10 -15\n\n10 -10\n\n10 -5\n\nm\na\nx\n\ntj/\nQ\n\n2\n(t\n\n)j\n=\njQ\n\n2\n[u\n\n0\n]j\n\n(c)\n\n10 -2 10 -1 10 0\n\n\"t\n\n10 -15\n\n10 -10\n\n10 -5\n\n10 0\n\nm\na\nx\n\ntj/\nQ\n\n3\n(t\n\n)j\n=\njQ\n\n3\n[u\n\n0\n]j\n\n(d)\n\nFIGURE 2. The relative error in (a) TDSR solution, (b) mass, (c) momentum, and (d) Hamiltonian. Pa-\nrameters are T = 10, L = 100, NS = 2048. The renormalization factor R(t) is computed by enforcing either\nconservation of mass (red), momentum (green), or Hamiltonian (blue).\n\nthan mass or Hamiltonian. Indeed, we have tested the TDSR method on the long-time evolution of the 1-soliton so-\nlution for the KdV equation while conserving momentum (the L2 norm of the solution). The numerical experiment\nwas performed with parameters T = 240,\u2206t = 0.1875,NS = 4096,L = 300 using the idea of multi-blocking with\nMb = 8 time blocks (see remark below). The relative error in the solution, mass and Hamiltonian at end time were\nin the order of 10\u22126, 10\u22127 and 10\u221210 respectively, while the relative error in momentum remained near machine\nprecision.\nRemark: Below, we describe the idea of multi-blocking used when the time interval is too large for the renor-\nmalized Picard iterations to converge (this is not due to a CFL-type restriction prevalent in generic time-stepping\nschemes). The idea is to divide the full time interval [0,T ] into Mb sub-intervals such that [0,T ] = \u222a\n\nMb\ni=1[Ti\u22121,Ti]\n\nwith T0 = 0. For a fixed i, the quantity Ti\u2212Ti\u22121 is chosen sufficiently large so that the spectral renormalization\nalgorithm is efficient and convergent. On the first segment [0,T1], the solution of the TDSR scheme with the initial\ncondition u0(x) is obtained from the iteration:\n\nv(n+1)(x, t) =\n1\n\nR(n)(t)\n\n(\netL [u0(x)]+\n\n\u222b t\n0\n\ne(t\u2212\u03c4)L N [R(n)(\u03c4)v(n)(x,\u03c4)]d\u03c4\n)\n,\n\n\n\n10 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\nwhile on the second segment [T1,T2], from:\n\nv(n+1)(x, t) =\n1\n\nR(n)(t)\n\n(\ne(t\u2212T1)L [u(x,T1)]+\n\n\u222b t\nT1\n\ne(t\u2212T1\u2212\u03c4)L N [R(n)(\u03c4)v(n)(x,\u03c4)]d\u03c4\n)\n.\n\nThe renormalization factor R(n)(t) (corresponding to a single conservation law) is obtained from\n\nQm\n(\n\nR(n)(t)v(n)(x, t)\n)\n=Cm.\n\nThis process is repeated Mb times until final time T is reached. It should be pointed out that the number of\nsegment Mb is chosen such that the Duhamel fixed point iteration, without renormalization, would diverge on any\ngiven sub interval [Ti\u22121,Ti].\n\n4.2. Zabusky-Kruskal experiment. Our goal in this section is to reproduce the well-known numerical results\nof Zabusky and Kruskal on the KdV equation [44] using our algorithm. Their simulation displays rich nonlinear\ndynamics which, as such, represents a challenge for numerical methods as far as the choice of time-steps, long-\ntime accuracy and stability are concerned [10, 13, 15, 36]. To this end, we apply the TDSR method on the KdV\nEq. (4.1) with \u03b1 = 1 and \u03b5 = 0.022 subject to periodic boundary conditions u(x+2, t) = u(x, t) and initial condition\nu0(x) = cos(\u03c0x). Figure 3 (a) shows the Zabusky-Kruskal results while rigorously conserving the momentum\u222b 2\n\n0 u\n2dx. We compare our findings with those obtained using the ETDRK4 method [25]. Note that, in order to keep\n\nthe level of solution accuracy of the ETDRK4 method comparable with that of the TDSR scheme while conserving\nmomentum, the time step \u2206tET D (of the ETDRK4) has to be about one order of magnitude lesser than its TDSR\ncounterpart (\u2206tT DSR). This can be seen by gradually reducing the relative time step (\u2206tr = \u2206tET D/\u2206tT DSR) while\nmonitoring the relative difference between the two numerical solutions. At \u2206tr = 0.125, this difference was seen to\ndrop to O(10\u22126). Using the same spatio-temporal discretization as before, we checked the TDSR results\u2019 fidelity\nup to 20 recurrence times. This was done, by monitoring the time evolution of the first six conserved quantities of\nthe KdV equation with \u03b1 = 1 and \u03b5 = 0.022 given by:\n\nQ1 = u, Q2 = u\n2\n\nQ3 =\u2212\nu3\n\n6\n+\n\n\u03b5\n2u2x\n2\n\n, Q4 =\nu4\n\n4\n\u22123\u03b52uu2x +\n\n9\u03b54u2xx\n5\n\n,\n\nQ5 =\nu5\n\n5\n\u22126\u03b52u2u2x +\n\n36\u03b54uu2xx\n5\n\n\u2212\n108\u03b56u2xxx\n\n35\n,\n\nQ6 =\nu6\n\n6\n\u221210\u03b52u3u2x +18\u03b5\n\n4u2u2xx\u22125\u03b5\n4u4x\u2212\n\n108\u03b56uu2xxx\n7\n\n+\n120\u03b56u3xx\n\n7\n+\n\n36\u03b58u2xxxx\n7\n\n.\n\nClearly, the relative error in the momentum remains close to machine precision, while at the same time resulting\nin the conservation of the mass as well. This can be explained by considering the Fourier series representations of\nthe KdV solution u(x, t) = \u2211\u221em=\u2212\u221e u\u0302m(t)e\n\ni\u03c0mx, and its corresponding auxiliary function v(x, t) = \u2211\u221em=\u2212\u221e v\u0302m(t)e\ni\u03c0mx.\n\nWith this at hand, the (n+1)th renormalized Duhamel iterate takes the form:\n\nv\u0302(n+1)m (t) =\n1\n\nR(n)(t)\n\n(\nei\u03b5\n\n2m3\u03c03t u\u0302m(0)\u2212\n\u222b t\n\n0\nei\u03b5\n\n2m3\u03c03(t\u2212\u03c4) im\u03c0\n2\n\n\u221e\n\n\u2211\nl=\u2212\u221e\n\nu\u0302(n)l u\u0302\n(n)\nm\u2212ld\u03c4\n\n)\n(4.10)\n\nwhere (R(n)(t))2 =C2/\n(\u222b 2\n\n0\n\n(\nv(n)(x, t)\n\n)2\ndx\n)\n\n. Substituting m = 0 in Eq. (4.10), we obtain:\n\nv\u0302(n+1)0 (t) =\nu\u03020(0)\n\nR(n)(t)\n. (4.11)\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 11\n\n0 0.5 1 1.5 2\nx\n\n-1\n\n-0.5\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\nu\n\n(b)\n\n0 0.5 1 1.5 2\nx\n\n-1\n\n-0.5\n\n0\n\n0.5\n\n1\n\n1.5\n\nu\n\n(c)\n\nTDSR\nETDRK4\n\n0 0.5 1 1.5 2\nx\n\n-1\n\n-0.5\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nu\n\n(e)\n\nFIGURE 3. (a) Space-time contour plot for the KdV solution with \u03b1 = 1,\u03b5 = 0.022 and initial condition\nu(x,0) = cos(\u03c0x). Other parameters are \u2206t \u2248 0.0008, L = 2, NS = 256 and the time block size T1 \u2248 0.016\n(multi-blocking with Mb sub-intervals of equal size). (b) The solution at T = 3.6/\u03c0 depicting the fission\nof the initial condition into an eight soliton train. (c) The solution at one recurrence time T = tR = 30.4/\u03c0\nobtained via TDSR Simpson showing good agreement with the fourth-order accurate (in time) ETDRK4\nsolution. A time step of \u2206t = 0.0001 (for the ETDRK4 scheme) was used to obtain a solution of comparable\naccuracy to ours. (d) A space-time contour plot for the solution obtained from the TDSR algorithm using\nthe same spatio-temporal discretization as in (a), over the time span [19tR,20tR]. The stable numerical\nsimulation produced accurate results, as evidenced by the minor relative errors in the first six conserved\nquantities (see Fig. 4). (e) The solution at T = 20tR obtained using TDSR Simpson.\n\nUsing the identity u\u0302(n+1)0 (t) = R\n(n+1)(t)v\u0302(n+1)0 (t) we find\n\nu\u0302(n+1)0 (t) =\n\n\u221a\u221a\u221a\u221a\u221a\u221a\n\u222b 2\n\n0\n\n(\nv(n)(x, t)\n\n)2\ndx\n\n\u222b 2\n0\n\n(\nv(n+1)(x, t)\n\n)2\ndx\n\nu\u03020(0). (4.12)\n\nFor the initial condition considered here u0(x) = cos(\u03c0x), one finds u\u03020(0) = 0, resulting in u\u0302\n(n+1)\n0 (t) \u2261 0, i.e.,\n\nthe mass is also preserved at every Duhamel iterate. Additionally, the relative errors of the other four conserved\nquantities are within O(10\u22125). In particular, some aspects of our scheme (such as solution accuracy) outperforms\nother well known conservative numerical methods to simulate the KdV equation, such as the one developed in [13].\nIn [13], the authors develop an operator splitting scheme in conjunction with a finite volume spatial discretization\nto locally conserve the mass and momentum. While their scheme demonstrates impressive long time stability\nproperties (Zabusky-Kruskal dynamics), there were some phase errors at T = 20tR, that arise from the global\n(absolute) errors in the conservation of the Hamiltonian (\u223c 10\u22123).\n\nPresently, it is unclear if there are other finite volume based schemes capable of incorporating more than two\nconserved quantities for the KdV equation. Also, it is worth to mention that the ETDRK4 scheme (which does not\nconserve momentum at the local or global level for the KdV), suffers from numerical instabilities (for \u2206t \u2248 0.0008)\nand for time integration up to T = 20tR.\n\nFinally, we examine the performance of the TDSR method by measuring the local errors in the mass and\nmomentum. Specifically, we compute the errors for mass and momentum at end time T = 20tR respectively\ndefined by\n\nE1(x,T ) = ut +\u03b1uux + \u03b5\n2uxxx, (4.13)\n\n\n\n12 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\n50 100 150 200\n0\n\n0.5\n\n1\n\n1.5\n\nj/\nQ\n\n1\n(t\n\n)j\n\n#10 -15 (a)\n\n50 100 150 200\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\n4.5\n\nj/\nQ\n\n2\n(t\n\n)j\n=\njQ\n\n2\n[u\n\n0\n]j\n\n#10 -16 (b)\n\n50 100 150 200\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\nj/\nQ\n\n3\n(t\n\n)j\n=\njQ\n\n3\n[u\n\n0\n]j\n\n#10 -5 (c)\n\n50 100 150 200\nt\n\n0\n\n1\n\n2\n\n3\n\n4\n\nj/\nQ\n\n4\n(t\n\n)j\n=\njQ\n\n4\n[u\n\n0\n]j\n\n#10 -7 (d)\n\n50 100 150 200\nt\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nj/\nQ\n\n5\n(t\n\n)j\n=\njQ\n\n5\n[u\n\n0\n]j\n\n#10 -5 (e)\n\n50 100 150 200\nt\n\n0\n\n2\n\n4\n\n6\n\n8\n\nj/\nQ\n\n6\n(t\n\n)j\n=\njQ\n\n6\n[u\n\n0\n]j\n\n#10 -7 (f)\n\nFIGURE 4. Errors in conserved quantities for the Zabusky-Kruskal test case monitored over the time-span\n[0, 20tR]; computational parameters for the simulation can be found in caption of Fig. 3: (a) absolute error\nin the mass (the initial mass Q1[u0] = 0), relative errors in (b) momentum (L2-norm), (c) Hamiltonian,\n(d) fourth conserved quantity (Q4[u0]), (e) fifth conserved quantity (Q5[u0]) and (f) sixth conserved quan-\ntity (Q6[u0]). By construction, the relative error in the conservation of momentum is kept near machine\nprecision, while the absolute error in mass remains at \u223c 10\u221215.\n\n0 1 2\nx\n\n-5\n\n0\n\n5\n\nE 1\n(x\n\n;t\n=\n\n2\n0\nt R\n\n)\n\n#10 -5 (a)\n\n0 1 2\nx\n\n-5\n\n0\n\n5\n\nE 2\n(x\n\n;t\n=\n\n2\n0\nt R\n\n)\n\n#10 -5 (b)\n\nFIGURE 5. A snapshot of the local errors in (a) conservation of mass (E1(x, t)) and (b) conser-\nvation of momentum (E2(x, t)) as a function of x at time T = 20tR.\n\nE2(x,T ) = (u\n2/2)t +\n\n(\n\u03b1\n\n3\nu3 + \u03b52\n\n(\nuuxx\u2212u2x/2\n\n))\nx\n. (4.14)\n\nThe time derivatives are computed using the fourth-order backward differentiation formula [35]:\n\nut(xm,T )\u2248\n\n(\n25u(xm,T )\u221248u(xm,T \u2212\u2206t)+36u(xm,T \u22122\u2206t)\u221216u(xm,T \u22123\u2206t)+3u(xm,T \u22124\u2206t)\n\n)\n12\u2206t\n\n, (4.15)\n\nwhereas, all the spatial derivatives were computed to spectral accuracy with the use of fast Fourier transforms.\nFig. 5 shows the variations of E1 and E2 as a function of x at time T = 20tR. Remarkably, the local errors remain\nrelatively small even over such long time intervals.\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 13\n\n0.01 0.015 0.02\nt\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nj/\nu\n(t\n\n)j\njju\n\ne\nx\n(x\n\n;t\n)j\nj 1\n\n#10 -4 (b)\n\nFIGURE 6. (a) Travelling wave solution for the Allen-Cahn equation with simulation parameters: L = 4,\n\u03b5 = 0.05, NS = 1024, \u2206t = 0.000125, and end time T = 0.02. Solution advanced to the final time with the\nidea of multi-blocking with sub-interval size T1 = 0.01. The error in solution was restricted to 6.3\u00d710\u22124\nwith this parameter choice. (b) The absolute error, in time, between the TDSR numerical and the exact\nsolutions in the time interval [0.01,0.02].\n\n4.3. Travelling waves for the Allen-Cahn equation. In this section, we apply the TDSR method on the Allen-\nCahn equation, a prototypical reaction-diffusion type equation that arises in material science [8]. It is given by\n\nut = Duxx + \u03b3(u\u2212u3), (4.16)\nwhere D > 0 is the diffusion coefficient and \u03b3 measures the strength of reaction. The Allen-Cahn equation is\ndissipative in nature. In fact when subject to homogeneous Dirichlet/Neumann boundary conditions (considered\nin this paper), multiplying Eq. (4.16) by 2u and integrating over the whole domain leads to the dissipation rate\nEq. (2.3) with density and flux:\n\n\u03c1 = u2, F = 2Du2x\u22122\u03b3(u\n2\u2212u4). (4.17)\n\nIn this case, as we shall see later, the renormalization factor R(t) obeys a nonlinear ordinary differential equation.\nWe present numerical results on two canonical problems associated with the Allen-Cahn equation: (i) dynamics\nof traveling waves and (ii) observation of meta-stable dynamics. Both examples represent a departure from the\nperiodic case for which the linear operator L is diagonalizable. Indeed, for the Allen-Cahn equation, the discrete\nrepresentation of L is now dense as is the case when using spectral differentiation matrices. Thus, the semi-\ngroup exp(tL ) forms a rank-3 tensor. We implement the TDSR method on the Allen-Cahn Eq. (4.16) to compute\ntraveling waves with diffusion coefficient D = 1 and large reaction parameter \u03b3 that scales like \u03b5\u22122, with \u03b5 \ufffd 1.\nEquation (4.16) is subject to the initial condition u(x,0) = 0.5\u2212 0.5tanh\n\n(\nx/(2\n\u221a\n\n2\u03b5)\n)\n\nand Neumann boundary\nconditions: ux(x, t)\u2192 0 as |x| \u2192 \u221e. Interestingly enough, the Allen-Cahn Eq. (4.16) admits an exact travelling\nwave solution given by u(x, t) = 0.5\u2212 0.5tanh\u03be/(2\n\n\u221a\n2\u03b5), \u03be = x\u2212 3t/(\n\n\u221a\n2\u03b5) [24]. It is the aim of this section\n\nto reproduce this exact solution using the TDSR while enforcing the dissipation rate equation given in (2.3) and\n(4.17). We proceed by substituting the ansatz u(x, t) = R(t)v(x, t) into Eq. (4.16); multiply by 2u and integrate the\nresulting system over the whole computational domain to obtain a first order dynamical system for the variable\np(t)\u2261 r(t)R2(t):\n\nd p\ndt\n\n= (\u2212a(t)+2\u03b3)p\u2212b(t)p2, (4.18)\n\nwhere the expressions for the time-dependent coefficients r(t), a(t) and b(t)> 0 are given by (here \u2126 denotes the\nspatial domain of the Allen-Cahn equation)\n\nr(t) =\n\u222b\n\n\u2126\n\nv2(x, t)dx, a(t) =\n2D\n\u222b\n\n\u2126\nv2x(x, t)dx\nr(t)\n\n, b(t) =\n2\u03b3\n\u222b\n\n\u2126\nv4(x, t)dx\nr2(t)\n\n. (4.19)\n\nThe presence of the large coefficient \u03b3 \u223c \u03b5\u22122 in Eq.(4.18) causes the differential equation to become stiff,\nthus severely limiting the choice of time-steps. With this in mind, we use an implicit scheme (such as Crank-\nNicolson) to time-step (4.18) [35]. The coefficients r(t), a(t) and b(t) are computed to spectral accuracy using\n\n\n\n14 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\nFIGURE 7. (a) Spatio-temporal field distribution illustrating the meta-stable dynamics. Simulations were\nperformed using the idea of multi-blocking with block size T1 = 8 and final time T = 80. Computational\nparameters: NS = 256, \u2206t \u2248 0.016. (b) Comparison with the ETDRK4 scheme (at end time), showing a\ngood agreement with the TDSR trapezoidal result.\n\nClenshaw-Curtis quadrature method [39]. The spatial domain \u2126 is truncated, in which case, x \u2208 [\u2212L/2,L/2]\nand taking advantage of the spatial decay of ux to enforce homogeneous Neumann boundary conditions. Several\nremarks are in order: (i) The Chebyshev (Chebyshev-Lobatto) series representation is originally developed for\nfunctions defined on the interval [\u22121,1]. It can, nonetheless be applied on the interval [\u2212L/2,L/2] using a linear\ntransformation. (ii) We incorporate the homogeneous Neumann boundary conditions following similar procedure\nas outlined in [29]. For example, the second derivative is represented by the matrix product DD0 with D being\nthe standard first order spectral differentiation matrix while D0 is the first order differentiation matrix whose first\nand last rows have been replaced by the zero-vector respectively. This implicitly enforces the underlying boundary\nconditions. The numerical results obtained in this section are summarized in Fig. 6 where a surface plot describing\nthe time evolution of the traveling wave as well as its numerical accuracy are shown.\n\n4.4. Meta-stable dynamics of the Allen-Cahn equation. Our last example is concerned with the dynamics of\na meta-stable state associated with the Allen-Cahn Eq. (4.16) with parameters D = 0.01,\u03b3 = 1 and subject to the\nboundary conditions u(\u22121, t) =\u22121, u(1, t) = 1 and initial condition u(x,0) = 0.53x+0.47sin(\u22121.5\u03c0x). This test\nbed case is particularly interesting since the dynamics of an initial hump is observed to be meta-stable, i.e., it\nremains unchanged over long time, before abruptly vanishing. This type of rapid change in the wave profile over\nshort time scales inevitably creates numerical challenges. Such dynamic metastability was numerically observed\nby Kassam and Trefethen [25] using the modified ETDRK4 scheme. In this section, we demonstrate the robust-\nness of our TDSR method by reproducing this type of abrupt transition from a meta-stable state to another stable\nwavefunction profile. Here, the renormalization factor R(t) is governed by the same nonlinear ordinary differential\nequation Eq. (4.18) with the exception that now the parameters are D = 0.01,\u03b3 = 1 with a spatial domain [\u22121,1].\nFew remarks are in order: (i) To simplify the computation, we first homogenize the boundary conditions by de-\nriving a new evolution equation on which the TDSR method is implemented. (ii) To impose Dirichlet boundary\nconditions, the operator d2/dx2 is approximated by D2 where D is the first order spectral differentiation matrix\n[39]. The time evolution of the Allen-Cahn front is shown in Fig. 7 (a). As expected, our method is indeed ca-\npable of reproducing those well known results. We have also compared our results with those obtained using the\nETDRK4 scheme and found good agreement (see Fig. 7 (b) for a comparison at end time).\n\n5. NUMERICAL IMPLEMENTATION OF TDSR WITH MULTI-CONSERVATION LAWS\n\nSo far we have addressed several cases where a single conservation law is \u201cinjected\u201d into the numerical sim-\nulations. In this section, we shall present results when multiple conservation laws are enforced. There are three\nchoices that we considered: conservation of (i) mass and momentum; (ii) mass and Hamiltonian ; (iii) mass, mo-\nmentum and Hamiltonian. All numerical results reported in this section are for the KdV equation (4.1), subject to\nrapidly decaying boundary conditions with \u03b1 = 6 and \u03b5 = 1.\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 15\n\nWe remark that the pseudo initial conditions f j(x), j = 1,2, \u00b7 \u00b7 \u00b7 ,N, introduced in the TDSR formulation (see\nEqs. 2.11-2.13) are crucial for the success of the method. They greatly control the scheme\u2019s convergence and allow\nthe renormalization factors to act as \u201cexpansion\u201d coefficients. Our numerical tests strongly indicate that taking none\nof the pseudo-initial conditions be identically equal to zero, (albeit satisfying the underlying boundary conditions),\nin order for the scheme to converge. With this in mind, a natural and important issue that immediately arises is\nhow to choose them? Our extensive numerical experiments seem to suggest that the natural choice f j(x)\u2261\u03b1 ju0(x)\nwith non-zero constant \u03b1 j\u2019s does not lead to convergence. However, several other possible choices for f j are given\nby f j(x) = \u03b1 j(x)u0(x) where \u03b1 j(x) are spatially localized functions. For evolution equations in (1+1)D subject\nto rapidly decaying boundary conditions (such as the KdV, NLS, mKdV), the algorithm seems to converge (at least\nover some time interval) when the first (N\u22121) f j(x) are chosen to belong to the class of bell-shaped, sign-definite\nfunctions. For example, f j \u2208 {sech(x),e\u2212x\n\n2\n,sech2(x)} for j = 1,2, \u00b7 \u00b7 \u00b7(N\u22121), while the entire set of pseudo-initial\n\nconditions is required to satisfy the normalization condition in Eq. (2.9). A full characterization on the choice of\nthe pseudo-initial conditions f j(x) is the subject for future work.\n\n5.1. Conservation of mass and momentum. Here the KdV solution is decomposed in the form: u(x, t) =\nR1(t)v1(x, t)+R2(t)v2(x, t), where R1(t) and R2(t) are computed from the coupled system\n\nQ j [R1(t)v1(x, t)+R2(t)v2(x, t)] = Q j\n[\n2\u03b2 2sech2(\u03b2x)\n\n]\n, j = 1,2. (5.1)\n\nWe choose f1(x) = (1/300)sech\n(\n\nx/\n\u221a\n\n600\n)\n\n, and f2(x) = u0(x)\u2212 f1(x). The explicit expressions for R1(t) and\nR2(t) can be obtained from the coupled system\n\nR1(t) =\nC1\u2212A2(t)R2(t)\n\nA1(t)\n, and \u00b51(t)R\n\n2\n2 +\u00b52(t)R2 +\u00b53(t) = 0,\n\nwhere\n\u00b51(t) = A3A\n\n2\n2 +A4A\n\n2\n1\u22122A1A2A5, \u00b52(t) = 2A1A5C1\u22122A2A3C1, \u00b53(t) = A3C\n\n2\n1 \u2212C2A\n\n2\n1,\n\nand\n\nA1(t) =\n\u222b\nR\n\nv1(x, t)dx, A2(t) =\n\u222b\nR\n\nv2(x, t)dx, A3(t) =\n\u222b\n\n\u221e\n\n\u2212\u221e\nv21(x, t)dx,\n\nA4(t) =\n\u222b\nR\n\nv22(x, t)dx, A5(t) =\n\u222b\nR\n\nv1(x, t)v2(x, t)dx.\n\nNumerical tests indicate that the algorithm converges to the correct solution when using the root R2(t) = [\u2212\u00b52(t)+\u221a\n\u00b5\n\n2\n2 (t)\u22124\u00b51(t)\u00b53(t)]/[2\u00b51(t)], while the other one causes the TDSR algorithm to diverge. The correct root was\n\nfound to always satisfy R2(0)v2(x,0) = f2(x) at any Duhamel iteration while the other consistently violated it.\nFigure 8 shows results of TDSR simulations where mass and momentum are both conserved. Another interesting\nnumerical experiment (discussed below), is related to interaction (or collision) between two 1-soliton solutions to\nthe KdV equation. The corresponding initial condition is\n\nu0(x) = 2\u03b2\n2\n1 sech\n\n2 (\u03b21x)+2\u03b2\n2\n2 sech\n\n2 (\u03b22(x\u2212 x0)) , (5.2)\nwith \u03b21 =\n\n1\u221a\n10\n, \u03b22 =\n\n1\n2\n\u221a\n\n10\nand an initial separation of x0 = 40. We simulated this to end time T = 200 using the idea\n\nof multi-blocking (see Fig. 9). The solitons interact elastically, mainly emerging unscathed from the interaction,\nsuffering only from a phase shift as expected. As before, we prescribed f1(x) = (1/300)sech\n\n(\nx/\n\u221a\n\n600\n)\n\n, while\nf2(x) = u0(x)\u2212 f1(x).\n\n\n\n16 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\n0 50\nt\n\n0\n\n0.5\n\n1\n\nj/\nu\n(t\n\n)j\n=\njju\n\ne\nx\n(x\n\n;t\n)j\nj 1\n\n#10 -4\n\n0 50\nt\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nm\na\nx\n\ntj/\nQ\n\n1\n(t\n\n)j\n=\njQ\n\n1\n[u\n\n0\n]j #10 -16\n\n0 50\nt\n\n0\n\n2\n\n4\n\n6\n\n8\n\nm\na\nx\n\ntj/\nQ\n\n2\n(t\n\n)j\n=\njQ\n\n2\n[u\n\n0\n]j #10 -16\n\n0 50\nt\n\n0\n\n0.5\n\n1\n\nm\na\nx\n\ntj/\nQ\n\n3\n(t\n\n)j\n=\njQ\n\n3\n[u\n\n0\n]j #10 -6\n\nFIGURE 8. (a) Time evolution of the relative error between the numerically obtained solution for the KdV\nequation compared to its exact solution given in Eq. (4.3). Time evolution of the conserved quantities given\nin Eq. (4.4): mass (b), momentum (c) and Hamiltonian (d). Note that the relative errors in conservation\nof mass and momentum stay close to machine precision. Parameters are: L = 800, \u2206t = 0.5, T = 60,\nNs = 16384 with the renormalization factors obtained from system (5.1) for j=1, 2. Note the renormalized\nDuhamel iterations converge over such a large time-span, a significant improvement over when a single\nquantity is conserved.\n\nFIGURE 9. Interaction between two 1-soliton solution for the KdV equation with \u03b1 = 6,\u03b5 = 1 and initial\ncondition u(x,0) = (1/5)sech2\n\n(\nx/\n\u221a\n\n10\n)\n+(1/20)sech2\n\n(\n(x\u221240)/2\n\n\u221a\n10\n)\n. Other parameters are \u2206t = 0.5,\n\nL= 800, NS = 16384 and the time block size T1 = 20. The renormalization factors are obtained by enforcing\nthe conservation of mass and momentum simultaneously. The relative error in Hamiltonian \u2248 7.86\u00d710\u22126\nat T = 200, while relative error in mass\u2248 1.9\u00d710\u221216 and momentum\u2248 5.9\u00d710\u221216 are kept near machine\nprecision.\n\n0 10 20 30\nt\n\n0\n\n0.5\n\n1\n\nj/\nu\n(t\n\n)j\n=\njju\n\ne\nx\n(x\n\n;t\n)j\nj 1\n\n#10 -4\n\n0 10 20 30\nt\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nm\na\nx\n\ntj/\nQ\n\n1\n(t\n\n)j\n=\njQ\n\n1\n[u\n\n0\n]j\n\n#10 -16\n\n0 10 20 30\nt\n\n0\n\n1\n\n2\n\n3\n\nm\na\nx\n\ntj/\nQ\n\n2\n(t\n\n)j\n=\njQ\n\n2\n[u\n\n0\n]j\n\n#10 -7\n\n0 10 20 30\nt\n\n0\n\n1\n\n2\n\n3\n\n4\n\nm\na\nx\n\ntj/\nQ\n\n3\n(t\n\n)j\n=\njQ\n\n3\n[u\n\n0\n]j\n\n#10 -16\n\nFIGURE 10. (a) Time evolution of the relative error between the numerically obtained (TDSR) solution\nto the KdV equation with conservation of mass and Hamiltomian compared to its exact solution given in\nEq. (4.3). Time evolution of the conserved quantities given in Eq. (4.4): mass (b), momentum (c) and\nHamiltonian (d). Note that the relative errors in conservation of mass and Hamiltonian now stay close to\nmachine precision. Parameters are: L = 800, \u2206t = 0.5, T = 30, Ns = 16384 with the renormalization factors\nobtained from system (5.1) for j=1, 3.\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 17\n\n0 2 4\nt\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nj/\nu\n(t\n\n)j\n=\njju\n\ne\nx\n(x\n\n;t\n)j\nj 1\n\n#10 -4\n\n0 2 4\nt\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nm\na\nx\n\ntj/\nQ\n\n1\n(t\n\n)j\n=\njQ\n\n1\n[u\n\n0\n]j\n\n#10 -16\n\n0 2 4\nt\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nm\na\nx\n\ntj/\nQ\n\n2\n(t\n\n)j\n=\njQ\n\n2\n[u\n\n0\n]j\n\n#10 -16\n\n0 2 4\nt\n\n0\n\n2\n\n4\n\n6\n\nm\na\nx\n\ntj/\nQ\n\n3\n(t\n\n)j\n=\njQ\n\n3\n[u\n\n0\n]j\n\n#10 -16\n\nFIGURE 11. (a) Time evolution of the relative error between the numerically obtained solution, from\nTDSR with conservation of mass, momentum and Hamiltonian, for the KdV equation compared to its\nexact solution given in Eq. (4.3). Time evolution of the conserved quantities given in Eq. (4.4): mass\n(b), momentum (c) and Hamiltonian (d). Parameters are: \u2206t = 0.5, T = 5, Ns = 2048, L = 100 with the\nrenormalization factors obtained from Eqs.(5.3).\n\n5.2. Conservation of mass and Hamiltonian. In this case, the renormalization factors R1(t) and R2(t) are now\ncomputed from the coupled system of equations given in (5.1) with j taking the values 1 and 3. Like before,\nwe pick the pseudo-initial conditions to be f1(x) = (1/300)sech\n\n(\nx/\n\u221a\n\n600\n)\n\nand f2(x) = u0(x)\u2212 f1(x). We solved\nsystem (5.1) for j = 1,3 using the Newton\u2019s method. Our findings are similar to those reported for the simultaneous\nconservation of mass and momentum, i.e., conservation of mass and Hamiltonian are achieved (see Fig. 10).\n\n5.3. Conservation of mass, momentum and Hamiltonian. Lastly, we seek three renormalization factors R1(t),\nR2(t) and R3(t) that satisfy u(x, t) = \u2211\n\n3\nj=1 R j(t)v j(x, t) and obey the conservation laws\n\nQ j\n\n[\n3\n\n\u2211\n`=1\n\nR`(t)v`(x, t)\n\n]\n= Q j\n\n[\n2\u03b2 2sech2(\u03b2x)\n\n]\n, j = 1,2,3. (5.3)\n\nUnlike the previous cases, of mass-momentum and mass-Hamiltonian conservation, two of the current conserved\nquantities now are nonlinear functionals further limiting the choices for the pseudo-initial conditions as far as\nthe convergence over large time intervals is concerned. It turns out that convergence over moderately long time\nintervals (T = 5) is achieved with pseudo-initial conditions in the Duhamel integral formulas Eqs. (2.11) and (2.13)\nas f3(x) = 0.15exp(\u2212x2), f2(x) = 0.05exp(\u2212x2) and f1(x) = u0(x)\u2212 f2(x)\u2212 f3(x). The renormalization factors\nare found from the coupled system of equations derived from enforcing the conservation of mass, momentum and\nconservation of Hamiltonian simultaneously Eq. (5.1) for j = 1,2,3, using the Newton\u2019s root finding method. The\nresults are depicted in Fig. 11 where the error in the momentum, Hamiltonian and mass are kept at the level of\nmachine precision.\n\n6. TDSR METHOD: MULTI-DIMENSIONAL TEST CASE\n\nThe nonlinear Schro\u0308dinger equation\n\niut +V (x)u+\u22072u+ |u|2u = 0, (6.1)\nplays an important role in modeling fundamental physics ranging from photonics, Bose-Einstein condensation to\nfluid mechanics [1]. Depending on the physics at hand, \u22072 = \u2202 2/\u2202x2 +\u2202 2/\u2202y2 denotes wave diffraction, V is the\nrefractive index, photonic lattice or an external potential and |u|2 measuring the intensity or density of a complex\nvalued wavefunction u. As such, using the TDSR method to simulate the NLS equation would seem natural. In\nthe absence of any external potential (V (x) = 0), Eq. (6.1) admits a special class of solutions known as the Townes\n\n\n\n18 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\n0 1 2\nt\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nj/\nu\n(t\n\n)j\n=\njju\n\n0\n(x\n\n;y\n)j\nj 1\n\n#10 -5 (a)\n\n0 1 2\nt\n\n0\n\n1\n\n2\n\n3\n\nm\na\nx\n\ntj/\nQ\n\n1\n(t\n\n)j\n=\njQ\n\n1\n[u\n\n0\n(x\n\n;y\n)]\nj #10\n\n-16 (b)\n\nFIGURE 12. (a) The error in the TDSR solution, as defined relative to the initial profile of the Townes\nsoliton, (b) the relative error in the power as a function of time t. As one can see, it is kept close to machine\nprecision over the entire time span [0,2]. Parameters are: \u2206t = 0.05, a square spatial domain with L = 40 ,\nand a spatial discretization of 512\u00d7512 for the computations.\n\nsolitons. They are of the form u(x, t) = U\u03bb (x)ei\u03bb\n2t ,\u03bb \u2208 R with real valued function U\u03bb satisfying the boundary\n\nvalue problem\n\n\u2207\n2U\u03bb +U\n\n3\n\u03bb\n= \u03bb 2U\u03bb . (6.2)\n\nThe numerical computation of the Townes soliton is well documented in the literature and can be achieved with\nthe use of various boundary value problem solvers (Quasi-Newton methods, spectral renormalization, etc. [3, 26,\n27, 41]). While the implementation of the TDSR method on the NLS equation was first reported in [11] it was\nexemplified in one spatial dimension for which the semi-group exp(it\u2202 2/\u2202x2) lives in (1+ 1) dimensions. Here,\nthe (1+2)D NLS equation, subject to sufficiently rapidly decaying boundary conditions is chosen as a prototypical\nexample to demonstrate the applicability of the TDSR scheme in multiple spatial dimensions. The NLS Eq. (6.1)\nadmits three conservation laws: power Q1 = |u|2, Hamiltonian Q3 =\u2212 14 |u|\n\n4 + 12 |\u2207u|\n2 and momentum Q2 = u\u2207u\u2217.\n\nWe were able to reproduce the time evolution of the Townes soliton, to within a relative error of \u223c 1.8\u00d7 10\u22125,\nwhen the algorithm was implemented on time interval [0,2] using time step of size \u2206t = 0.05 and number of spatial\n(square) grid points NS = 512. We rigorously conserve the power Q1, and as can be seen in Fig. 12 (b), the relative\nerror in power stays close to machine precision.\n\n7. CONCLUSIONS AND FUTURE DIRECTIONS\n\nIn 2005 Ablowitz and Musslimani proposed the spectral renormalization method as a tool to numerically ap-\nproximate solutions to nonlinear boundary value problems. Since then, it has been successfully used in many\nphysical settings that include photonics [42], Bose-Einstein condensation [7], Kohn-Sham density functional the-\nory [17], and water waves [5]. In 2016, Cole and Musslimani proposed the time dependent spectral renormalization\nmethod to simulate evolution equations with periodic boundary conditions. This important idea brings two novel\naspects: (i) it extends the original steady state spectral renormalization method to the time domain, thus offering\na unifying approach by which time-independent as well as evolution equations are solved by the same numerical\nscheme, (ii) it allowed the inclusion of certain physics such as conservation and dissipation laws. In this paper we\nhave significantly empowered the computational capabilities of the TDSR method that allows the (i) enforcement\nof several conservation laws or dissipation rate equations, (ii) flexibility to apply other non-periodic boundary\nconditions. We have successfully demonstrated these ideas on prototypical dynamical systems of physical signif-\nicance. Examples include the Korteweg-de Vries equation and dynamics of fronts modeled by the Allen-Cahn\nequation. We conclude this section by making a remark regarding possible application of the TDSR to weak wave\nturbulence. Wave turbulence describes the chaotic interactions of dispersive wavetrains (analogs to eddies) when\nan external forcing term added to the underlying nonlinear evolution equations are mediated by dissipative forces\n(see [31\u201333] and references contained). Numerical investigations of these phenomena is a challenging task, requir-\ning very long time runs for statistical equilibrium to be reached. Carefully designed numerical integrators are used\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 19\n\n(see [30]) to integrate the potentially stiff, underlying nonlinear field equations over long time. In such scenarios,\na close control over the conserved quantities of the dynamical system can prove vital to ensure long time accuracy\nof the solution. The application of the TDSR to this field thus seems natural and is kept for future work.\n\nREFERENCES\n\n[1] Mark J Ablowitz. Nonlinear dispersive waves: asymptotic analysis and solitons, volume 47. Cambridge\nUniversity Press, 2011.\n\n[2] Mark J Ablowitz, Xu-Dan Luo, and Ziad H Musslimani. Discrete nonlocal nonlinear schro\u0308dinger systems:\nIntegrability, inverse scattering and solitons. Nonlinearity, 33(7):3653, 2020.\n\n[3] Mark J. Ablowitz and Ziad H. Musslimani. Spectral renormalization method for computing self-localized\nsolutions to nonlinear systems. Opt. Lett., 30(16):2140\u20132142, Aug 2005.\n\n[4] Mark J Ablowitz and Ziad H Musslimani. Integrable discrete p t symmetric model. Physical Review E,\n90(3):032912, 2014.\n\n[5] MJ Ablowitz, AS Fokas, and ZH Musslimani. On a new non-local formulation of water waves. Journal of\nFluid Mechanics, 562:313, 2006.\n\n[6] MJ Ablowitz and JF Ladik. A nonlinear difference scheme and inverse scattering. Studies in Applied Mathe-\nmatics, 55(3):213\u2013229, 1976.\n\n[7] Eric Akkermans, Sankalpa Ghosh, and Ziad H Musslimani. Numerical study of one-dimensional and in-\nteracting Bose\u2013Einstein condensates in a random potential. Journal of Physics B: Atomic, Molecular and\nOptical Physics, 41(4):045302, 2008.\n\n[8] Samuel M. Allen and John W. Cahn. A microscopic theory for antiphase boundary motion and its application\nto antiphase domain coarsening. Acta Metallurgica, 27(6):1085 \u2013 1095, 1979.\n\n[9] Uri M Ascher and Chen Greif. A first course on numerical methods. SIAM, 2011.\n[10] Uri M Ascher and Robert I McLachlan. On symplectic and multisymplectic schemes for the kdv equation.\n\nJournal of Scientific Computing, 25(1):83\u2013104, 2005.\n[11] Justin T Cole and Ziad H Musslimani. Time-dependent spectral renormalization method. Physica D: Non-\n\nlinear Phenomena, 358:15\u201324, 2017.\n[12] Steven M Cox and Paul C Matthews. Exponential time differencing for stiff systems. Journal of Computa-\n\ntional Physics, 176(2):430\u2013455, 2002.\n[13] Yanfen Cui and De-kang Mao. Numerical method satisfying the first two conservation laws for the korteweg\u2013\n\nde vries equation. Journal of Computational Physics, 227(1):376\u2013399, 2007.\n[14] Philip J Davis. On the numerical integration of periodic analytic functions. On numerical approximation,\n\npages 21\u201323, 1959.\n[15] Daisuke Furihata. Finite difference schemes for \u2202u\n\n\u2202 t = (\n\u2202\n\n\u2202x )\n\u03b1 \u03b4g\n\n\u03b4u that inherit energy conservation or dissipation\nproperty. Journal of Computational Physics, 156(1):181\u2013205, 1999.\n\n[16] Sergei Konstantinovich Godunov. A difference scheme for numerical solution of discontinuous solution of\nhydrodynamic equations. Math. Sbornik, 47:271\u2013306, 1959.\n\n[17] Juri Grossi, Ziad H. Musslimani, Michael Seidl, and Paola Gori-Giorgi. Kohn-Sham equations with func-\ntionals from the strictly-correlated regime: Investigation with a spectral renormalization method. Journal of\nPhysics. Condensed matter, 32(47), 2020.\n\n[18] Ernst Hairer, Christian Lubich, and Gerhard Wanner. Geometric numerical integration: structure-preserving\nalgorithms for ordinary differential equations, volume 31. Springer Science & Business Media, 2006.\n\n[19] Charles Hirsch. Numerical computation of internal and external flows. vol. 2-computational methods for\ninviscid and viscous flows(book). Chichester, England and New York, John Wiley & Sons, 1990, 708, 1990.\n\n[20] Arieh Iserles. On the numerical quadrature of highly-oscillating integrals i: Fourier transforms. IMA Journal\nof Numerical Analysis, 24(3):365\u2013391, 2004.\n\n[21] AL Islas, DA Karpeev, and CM Schober. Geometric integrators for the nonlinear schro\u0308dinger equation.\nJournal of computational physics, 173(1):116\u2013148, 2001.\n\n\n\n20 SATHYANARAYANAN CHANDRAMOULI, ASEEL FARHAT, AND ZIAD MUSSLIMANI\n\n[22] A.L. Islas and C.M. Schober. Multi-symplectic methods for generalized schro\u0308dinger equations. Future\nGeneration Computer Systems, 19(3):403\u2013413, 2003. Special Issue on Geometric Numerical Algorithms.\n\n[23] A.L. Islas and C.M. Schober. Backward error analysis for multisymplectic discretizations of hamiltonian\npdes. Mathematics and Computers in Simulation, 69(3):290\u2013303, 2005. Nonlinear Waves: Computation and\nTheory III.\n\n[24] Darae Jeong, Seunggyu Lee, Dongsun Lee, Jaemin Shin, and Junseok Kim. Comparison study of numerical\nmethods for solving the Allen\u2013Cahn equation. Computational Materials Science, 111:131\u2013136, 2016.\n\n[25] Aly-Khan Kassam and Lloyd N Trefethen. Fourth-order time-stepping for stiff PDEs. SIAM Journal on\nScientific Computing, 26(4):1214\u20131233, 2005.\n\n[26] Panayotis G Kevrekidis, Dimitri J Frantzeskakis, and Ricardo Carretero-Gonza\u0301lez. Emergent nonlinear phe-\nnomena in Bose-Einstein condensates: theory and experiment, volume 45. Springer Science & Business\nMedia, 2007.\n\n[27] Yuri S Kivshar and Govind P Agrawal. Optical solitons: from fibers to photonic crystals. Academic press,\n2003.\n\n[28] Randall J LeVeque et al. Finite volume methods for hyperbolic problems, volume 31. Cambridge university\npress, 2002.\n\n[29] Yi-Xin Liu and Hong-Dong Zhang. Exponential time differencing methods with Chebyshev collocation for\npolymers confined by interacting surfaces. The Journal of chemical physics, 140(22):224101, 2014.\n\n[30] AJ Majda, DW McLaughlin, and EG Tabak. A one-dimensional model for dispersive wave turbulence.\nJournal of Nonlinear Science, 7(1):9\u201344, 1997.\n\n[31] Sergey Nazarenko. Wave turbulence, volume 825. Springer Science & Business Media, 2011.\n[32] Alan C Newell, Sergey Nazarenko, and Laura Biven. Wave turbulence and intermittency. Physica D: Non-\n\nlinear Phenomena, 152:520\u2013550, 2001.\n[33] Alan C Newell and Benno Rumpf. Wave turbulence. Annual review of fluid mechanics, 43:59\u201378, 2011.\n[34] Sheehan Olver. Numerical approximation of highly oscillatory integrals. PhD thesis, University of Cam-\n\nbridge, 2008.\n[35] Alfio Quarteroni, Riccardo Sacco, and Fausto Saleri. Numerical mathematics, volume 37. Springer Science\n\n& Business Media, 2010.\n[36] JM Sanz-Serna. An explicit finite-difference scheme with exact conservation properties. Journal of Compu-\n\ntational Physics, 47(2):199\u2013210, 1982.\n[37] A. Taflove. Advances in Computational Electrodynamics: The Finite-difference Time-domain Method. Artech\n\nHouse antenna library. Artech House, 1998.\n[38] Thiab R Taha and Mark I Ablowitz. Analytical and numerical aspects of certain nonlinear evolution equations.\n\niii. numerical, Korteweg-de Vries equation. Journal of Computational Physics, 55(2):231\u2013253, 1984.\n[39] Lloyd N Trefethen. Spectral methods in MATLAB, volume 10. Siam, 2000.\n[40] J. A. C. Weideman and B. M. Herbst. Split-step methods for the solution of the nonlinear Schro\u0308dinger\n\nequation. SIAM Journal on Numerical Analysis, 23(3):485\u2013507, 1986.\n[41] Jianke Yang. Newton-conjugate-gradient methods for solitary wave computations. Journal of Computational\n\nPhysics, 228(18):7007\u20137024, 2009.\n[42] Jianke Yang and Ziad H Musslimani. Fundamental and vortex solitons in a two-dimensional optical lattice.\n\nOptics letters, 28(21):2094\u20132096, 2003.\n[43] L Minah Yang, Ian Grooms, and Keith A Julien. The fidelity of exponential and IMEX integrators for\n\nwave turbulence: Introduction of a new near-minimax integrating factor scheme. Journal of Computational\nPhysics, page 109992, 2020.\n\n[44] Norman J Zabusky and Martin D Kruskal. Interaction of \u201csolitons\u201d in a collisionless plasma and the recur-\nrence of initial states. Physical review letters, 15(6):240, 1965.\n\n\n\nTIME-DEPENDENT DUHAMEL RENORMALIZATION METHOD WITH MULTIPLE CONSERVATION AND DISSIPATION LAWS 21\n\n(S. Chandramouli) DEPARTMENT OF MATHEMATICS, FLORIDA STATE UNIVERSITY, TALLAHASSEE, FL 32306, USA\nEmail address: schandra@math.fsu.edu\n\n(A. Farhat) DEPARTMENT OF MATHEMATICS, FLORIDA STATE UNIVERSITY, TALLAHASSEE, FL 32306, USA\nEmail address: afarhat@fsu.edu\n\n(Z. Musslimani) DEPARTMENT OF MATHEMATICS, FLORIDA STATE UNIVERSITY, TALLAHASSEE, FL 32306, USA\nEmail address: musliman@math.fsu.edu\n\n\n\t1. Introduction\n\t2. TDSR and Duhamel principle\n\t3. Time integration with various boundary conditions\n\t3.1. Periodic and decaying boundary conditions\n\t3.2. Time integration: non-periodic boundary conditions\n\n\t4. Numerical Implementation of TDSR: One conservation or dissipation law with various boundary conditions\n\t4.1. The KdV equation.\n\t4.2. Zabusky-Kruskal experiment \n\t4.3. Travelling waves for the Allen-Cahn equation.\n\t4.4. Meta-stable dynamics of the Allen-Cahn equation.\n\n\t5. Numerical Implementation of TDSR with multi-conservation laws\n\t5.1. Conservation of mass and momentum\n\t5.2. Conservation of mass and Hamiltonian\n\t5.3. Conservation of mass, momentum and Hamiltonian\n\n\t6. TDSR method: Multi-dimensional test case\n\t7. Conclusions and future directions\n\tReferences\n\n"}
{"Title": "Simulating local fields in carbon nanotube reinforced composites for infinite strip with voids", "Authors": "Mohamed Nasser, El Mostafa Kalmoun, Vladimir Mityushev, Natalia Rylko", "Abstract": "  We consider the steady heat conduction problem within a thermal isotropic and homogeneous infinite strip composite reinforced by uniformly and randomly distributed non-overlapping carbon nanotubes (CNTs) and containing voids. We treat the CNTs as thin perfectly conducting elliptic inclusions and assume the voids to be of circular shape and act as barriers to heat flow. We also impose isothermal conditions on the external boundaries by assuming the lower infinite wall to be a heater under a given temperature, and the upper wall to be a cooler that can be held at a lower fixed temperature. The equations for the temperature distribution are governed by the two-dimensional Laplace equation with mixed Dirichlet-Neumann boundary conditions. The resulting boundary value problem is solved using the boundary integral equation with the generalized Neumann kernel. We illustrate the performance of the proposed method through several numerical examples including the case of the presence a large number of CNTs and voids.      ", "Subject": "Numerical Analysis (math.NA)", "ID": "arXiv:2201.00003", "Text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating local fields in carbon nanotube\nreinforced composites for infinite strip with voids\n\nMohamed Nassera, El Mostafa Kalmounb, Vladimir Mityushevc,\nand Natalia Rylkoc\n\naMathematics Program, Department of Mathematics, Statistics and Physics,\nCollege of Arts and Sciences, Qatar University, Doha, Qatar\n\nbSchool of Science and Engineering, Al Akhawayn University in Ifrane,\nPO Box 104, Ifrane 53000, Morocco\n\ncFaculty of Computer Science and Telecommunications,\nCracow University of Technology, Krako\u0301w, Poland\n\nAbstract\n\nWe consider the steady heat conduction problem within a thermal isotropic\nand homogeneous infinite strip composite reinforced by uniformly and ran-\ndomly distributed non-overlapping carbon nanotubes (CNTs) and containing\nvoids. We treat the CNTs as thin perfectly conducting elliptic inclusions and\nassume the voids to be of circular shape and act as barriers to heat flow. We\nalso impose isothermal conditions on the external boundaries by assuming the\nlower infinite wall to be a heater under a given temperature, and the upper\nwall to be a cooler that can be held at a lower fixed temperature. The equa-\ntions for the temperature distribution are governed by the two-dimensional\nLaplace equation with mixed Dirichlet-Neumann boundary conditions. The\nresulting boundary value problem is solved using the boundary integral equa-\ntion with the generalized Neumann kernel. We illustrate the performance of\nthe proposed method through several numerical examples including the case\nof the presence a large number of CNTs and voids.\n\nKeywords. Local fields in 2D composites, Boundary integral equation,\nCarbon nanotube composites\n\n1 Introduction\n\nNanofibers embedded in polymer matrices have attracted attention as one of the\nreinforcements for composite materials. Carbon nanotubes (CNTs) reinforced poly-\nmer nanocomposites are considered as conventional micro- and macro-composites\n[1]. Their thermal, mechanical, and electric properties are determined by experi-\nmental and theoretical investigations [2, 3, 4]. CNTs are considered as perfectly\nconducting inclusions, which suggests imposing Dirichlet boundary conditions on\nthe boundary of CNTs. On the other hand, the classical problems for materials\nwith holes in porous media and materials with voids and insulting inclusions are\nmodeled by the Neumann boundary condition [5, 6].\n\nThe present paper is devoted to the heat conduction within a 2D (two-dimensional)\nthermal isotropic and homogeneous nanocomposite, which takes the form of an infi-\nnite strip, when it is reinforced by non-overlapping and randomly distributed CNTs\n\n1\n\nar\nX\n\niv\n:2\n\n20\n1.\n\n00\n00\n\n3v\n1 \n\n [\nm\n\nat\nh.\n\nN\nA\n\n] \n 2\n\n8 \nD\n\nec\n 2\n\n02\n1\n\n\n\n2\n\nand contains defects and voids. In particular, we are interested in studying the\neffect of CNTs as well as of the presence of voids on the macroscopic conductive and\nmechanical properties of this composite. Owing to the superconductivity of CNTs\nand the extremely low conductivity of voids, we can assume that the conductiv-\nity of CNTs, of the polymer host and of voids to be governed by the inequalities\n\u03bbc \ufffd \u03bb \ufffd \u03bb0. Such an assumption leads to a mixed boundary value problem\nwhere the latter inequality becomes +\u221e \ufffd \u03bb \ufffd 0. The host conductivity can be\nnormalized to unity, i.e., \u03bb = 1.\n\nTheoretical investigation of mixed boundary value problems by integral equa-\ntions can be found in [7, 8]. In the same time, implementation of numerical methods\nfor large number of inclusions and holes is still a challenging problem of applied and\ncomputational mathematics. We propose in this work a fast and effective algorithm\nfor the numerical solution of the formulated mixed boundary value problem. The\nmethod is based on the boundary integral equation with the generalized Neumann\nkernel. The integral equation has been used in [8] to solve a similar mixed bound-\nary value problem related to the capacity of generalized condensers. The proposed\nmethod can be even employed when the number of perfectly conducting inclusions\nand holes is very large.\n\nAs a result of simulations, we first study the 2D local fields for three types of me-\ndia. In the first type, we consider the case of pure m void cracks with m = 5, 30, 50.\nThe second type consists of pure ` CNT inclusions with ` = 5 and 200. Finally, we\ntreat the case of a large number of combined inclusions and holes by considering\neither 2000 of one of the two or 1000 of each. Afterward, we take up the systematic\ninvestigation of the effective conductivity of the considered composites. It is im-\nportant in applications to predict the macroscopic properties of composites which\ndepend on the concentration of perfectly conducting CNTs as well as on the concen-\ntration of holes and voids. It is worth noting that the notation of concentration are\ndifferent for slit shapes of CNTs and circular shapes of holes. The performed sim-\nulations of local fields and computation of their averaged conductivities for various\nconcentrations allows to establish the dependence of the macroscopic conductivity\non the main geometrical parameters.\n\n2 Problem formulation\n\nLet us consider a channel medium embedding m inhomogeneities in the form of `\nnanofillers and p = m \u2212 ` holes (voids). As many nanofillers (e.g, carbon nanon-\ntubes [9]) have cross sections of elliptical shapes, we model them as ellipses C1, . . . , C`.\nFurthermore, we represent the non-conducting holes by inner circles C`+1, . . . , Cm.\nThe top and bottom infinite walls of the channel are denoted respectively by C \u20320\nand C \u2032\u20320 , which yields a multiply connected domain \u2126 of connectivity m + 1 with a\nboundary set C =\n\n\u22c3m\nk=0Ck where C0 = C\n\n\u2032\n0 \u222aC \u2032\u20320 . An example of this domain for the\n\ncase of ` = 4 and m = 7 is illustrated in Figure 1.\nThe medium matrix without inhomogeneities is supposed to be homogeneous\n\nand isotropic with a constant thermal conductivity \u03bb = 1. We also assume that\nconduction is the only dominating mechanism of heat transfer in the medium. Ex-\ncept being non-overlapping, no other restriction is imposed on the inhomogeneities\nas they can be placed at random orientation and position.\n\nThe nanofillers are treated as heat superconductors with an almost uniform tem-\nperature distribution within each one. Therefore the temperature T is assumed to\n\n\n\n3\n\nbe fixed to an indeterminate constant value \u03b4k along each ellipse Ck for k = 1 . . . , `.\nThis assumption is consistent with the numerical results reported in [10] for CNT\nreinforced polymer composites. Furthermore, by the law of energy conservation in\nsteady-state heat conduction, there should be no net thermal flow through each\nnanofiller. This constraint is written by means of the net heat flux boundary con-\ndition (1e).\n\nOn the other hand, the curves C`+1, . . . , Cm are assumed to be perfect insulators\nand therefore they act as barriers to heat flow. Henceforth, the Neumann boundary\ncondition (1f) is imposed along the holes contours. Finally, isothermal conditions\nare imposed on the external boundaries by assuming that the lower infinite wall is\na heater of temperature T1, and the upper wall acts as a heat sink, which can be\nheld at a fixed temperature T0 < T1. Thee two values T0 and T1 of the temperature\non the external boundaries are normalized to 0 and 1, respectively.\n\nUnder steady-state conditions, Fourier\u2019s law of heat conduction and the above\nspecified heat boundaries conditions yield the temperature distribution T governed\nby the following mixed Dirichlet-Neumann boundary value problem:\n\n\u2206T = 0 in \u2126, (1a)\n\nT = 0 on C \u20320, (1b)\n\nT = 1 on C \u2032\u20320 , (1c)\n\nT = \u03b4k on Ck, k = 1, 2, . . . , `, (1d)\u222b\nCk\n\n\u2202T\n\n\u2202n\nds = 0 k = 1, 2, . . . , `, (1e)\n\n\u2202T\n\n\u2202n\n= 0 on Ck, k = `+ 1, `+ 2, . . . ,m, (1f)\n\nwhere \u2202T/\u2202n denotes the normal derivative of T , and \u03b41, . . . , \u03b4m are undetermined\nreal constants that need to be found alongside the distribution temperature T .\n\nFigure 1: Geometry of the problem (for ` = 4 and m = 7).\n\n3 The integral equation method\n\nThe boundary integral equation with the generalized Neumann kernel is not di-\nrectly applicable to the above boundary value problem (1) because of the external\nboundary component. However, the boundary value problem (1) is invariant under\n\n\n\n4\n\nconformal mapping. The mapping function\n\nz = \u03a6(\u03b6) =\n1\n\n\u03c0\nlog\n\n1 + \u03b6\n\n1\u2212 \u03b6\n+\n\ni\n\n2\n\nconformally maps the unit disk |\u03b6| < 1 onto the infinite strip 0 < Im z < 1. Thus,\nthe inverse mapping\n\n\u03b6 = \u03a6\u22121(z) = tanh\n\n(\n\u03c0z\n\n2\n\u2212\n\u03c0i\n\n4\n\n)\nconformally maps the infinite strip 0 < Im z < 1 onto the unit disk |\u03b6| < 1, the real\naxis onto the lower half of the unit circle, the line Im z = 1 onto the upper half of\nthe unit circle, and satisfies \u03a6\u22121(\u00b1\u221e + 0i) = \u00b11. Consequently, the function \u03a6\u22121\nmaps the multiply connected domain \u2126 in the z-plane (the physical domain) onto a\nmultiply connected domain G in the \u03b6-plane interior of the unit circle and exterior\nof m smooth Jordan curves (the computational domain). In Figure 2, we display\nthe result of the conformal mapping of the example shown in Figure 1.\n\nFigure 2: The computational domain G corresponding to the physical domain in\nFigure 1.\n\nIt follows that the harmonic function T can be written as\n\nT (z) = U(\u03a6\u22121(z))\n\nin which the function U is the solution of the following boundary value problem in\nthe \u03b6-plane:\n\n\u2206U = 0 in G, (2a)\n\nU = 0 on \u0393\u20320, (2b)\n\nU = 1 on \u0393\u2032\u20320, (2c)\n\nU = \u03b4k on \u0393k, k = 1, 2, . . . , `, (2d)\u222b\n\u0393k\n\n\u2202U\n\n\u2202n\nds = 0 k = 1, 2, . . . , `, (2e)\n\n\u2202U\n\n\u2202n\n= 0 on \u0393k, k = `+ 1, `+ 2, . . . ,m, (2f)\n\n\n\n5\n\nwhere \u0393\u20320 = \u03a6\n\u22121(C \u20320), \u0393\n\n\u2032\u2032\n0 = \u03a6\n\n\u22121(C \u2032\u20320 ), and \u0393k = \u03a6\n\u22121(Ck) for k = 1, 2, . . . ,m. Note\n\nthat the restriction of the function U(\u03b6) on the external boundary is discontinuous\nat \u03b6 = \u00b11. However, the function U can be cast into the form\n\nU(\u03b6) = u0(\u03b6) + u(\u03b6)\n\nwhere u(\u03b6) is a harmonic function in G, and\n\nu0(\u03b6) =\n1\n\n\u03c0\nIm log\n\n1\u2212 \u03b6\n1 + \u03b6\n\n+\n1\n\n2\n.\n\nThe function u0(\u03b6) is harmonic in G with u0(\u03b6) = 0 on the upper half of the unit\ncircle and u0(\u03b6) = 1 on the lower part. The function u(\u03b6) is the solution of the\nboundary value problem\n\n\u2206u(\u03b6) = 0 if \u03b6 \u2208 G, (3a)\nu(\u03b6) = 0 if \u03b6 \u2208 \u03930, (3b)\n\nu(\u03b6) = \u03b4k \u2212\n1\n\n\u03c0\nIm log\n\n1\u2212 \u03b6\n1 + \u03b6\n\n\u2212\n1\n\n2\nif \u03b6 \u2208 \u0393k, k = 1, 2, . . . , `, (3c)\u222b\n\n\u0393k\n\n\u2202u\n\n\u2202n\nds = 0 k = 1, 2, . . . , `, (3d)\n\n\u2202u\n\n\u2202n\n\n\u2223\u2223\u2223\u2223\n\u03b6\n\n= \u2212\n\u2202u0\n\u2202n\n\n\u2223\u2223\u2223\u2223\n\u03b6\n\nif \u03b6 \u2208 \u0393k, k = `+ 1, `+ 2, . . . ,m, (3e)\n\nwhere \u03930 is the unit circle.\nFor the orientation of the boundary components of G, we assume that \u03930 is\n\noriented counterclockwise and the other curves \u03931, . . . ,\u0393m are oriented clockwise.\nWe assume that each boundary component \u0393k, k = 0, 1, . . . ,m, is parametrized by a\n2\u03c0-periodic function \u03b7k(t), t \u2208 Jk := [0, 2\u03c0] such that \u03b7\u2032k(t) 6= 0. Let J be the disjoint\nunion of the m + 1 intervals J0, . . . , Jm, the whole boundary \u0393 is parametrized by\nthe complex function \u03b7 defined on J by [11, 12]\n\n\u03b7(t) =\n\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n\n\u03b70(t), t \u2208 J0,\n\u03b71(t), t \u2208 J1,\n\n...\n\u03b7m(t), t \u2208 Jm.\n\nNote that the unit circle \u03930 is parametrized by \u03b70(t) = e\nit, t \u2208 J0 = [0, 2\u03c0].\n\nLet n(\u03b6) be the unit outward normal vector at \u03b6 \u2208 \u0393 and let \u03bd(\u03b6) be the angle\nbetween the normal vector n(\u03b6) and the positive real axis. Then, for \u03b6 = \u03b7(t) \u2208 \u0393,\n\nn(\u03b6) = ei\u03bd(\u03b6) = \u2212i\n\u03b7\u2032(t)\n\n|\u03b7\u2032(t)|\n. (4)\n\nThus\n\n\u2202u0\n\u2202n\n\n= \u2207u0 \u00b7 n = cos \u03bd\n\u2202u0\n\u2202x\n\n+ sin \u03bd\n\u2202u0\n\u2202y\n\n= Re\n\n[\nei\u03bd\n(\n\u2202u0\n\u2202x\n\u2212 i\n\n\u2202u0\n\u2202y\n\n)]\n. (5)\n\nThe harmonic function u0(\u03b6) is the real part of a single-valued analytic function\nf0(\u03b6), i.e., u0(\u03b6) = Re[f0(\u03b6)], where\n\nf0(\u03b6) =\n1\n\n\u03c0i\nlog\n\n1\u2212 \u03b6\n1 + \u03b6\n\n+\n1\n\n2\n, (6)\n\n\n\n6\n\nand the branch of the logarithm function is chosen such that log 1 = 0. Then by the\nCauchy-Riemann equations, we have\n\nf \u20320(\u03b6) =\n\u2202u0(\u03b6)\n\n\u2202x\n\u2212 i\n\n\u2202u0(\u03b6)\n\n\u2202y\n,\n\nwhich, in view of (4) and (5), implies that\n\n|\u03b7\u2032(t)|\n\u2202u0\n\u2202n\n\n\u2223\u2223\u2223\u2223\n\u03b7(t)\n\n= Re [\u2212i\u03b7\u2032(t) f \u20320(\u03b7(t))] , \u03b7(t) \u2208 \u0393k, k = `+ 1, . . . ,m. (7)\n\nSince\n\nf \u20320(\u03b6) =\ni\n\n\u03c0\n\n(\n1\n\n1\u2212 \u03b6\n+\n\n1\n\n1 + \u03b6\n\n)\n,\n\nit follows that for \u03b7(t) \u2208 \u0393k and k = `+ 1, . . . ,m,\n\n|\u03b7\u2032(t)|\n\u2202u0\n\u2202n\n\n\u2223\u2223\u2223\u2223\n\u03b6=\u03b7(t)\n\n=\n1\n\n\u03c0\nRe\n\n[\n\u03b7\u2032(t)\n\n1\u2212 \u03b7(t)\n+\n\n\u03b7\u2032(t)\n\n1 + \u03b7(t)\n\n]\n. (8)\n\nThe harmonic function u can be assumed to be a real part of an analytic function\nf(\u03b6), \u03b6 \u2208 G. The boundary conditions (3b) and (3c) give the real parts of the\nfunction f(\u03b6) on \u0393k for k = 0, 1, . . . , `. Specifically, we have\n\nRe[f(\u03b7(t))] = 0, \u03b7(t) \u2208 \u03930 (9)\n\nand\n\nRe[f(\u03b7(t))] = \u03b4k \u2212\n1\n\n\u03c0\nIm log\n\n1\u2212 \u03b7(t)\n1 + \u03b7(t)\n\n\u2212\n1\n\n2\nif \u03b7(t) \u2208 \u0393k, k = 1, . . . , `. (10)\n\nFor the remaining boundary components \u0393k for k = ` + 1, . . . ,m, we use the con-\ndition (3e) to determine the boundary condition on f(\u03b7). By the Cauchy-Riemann\nequations, we can show using the same arguments as in (7) that\n\n|\u03b7\u2032(t)|\n\u2202u\n\n\u2202n\n\n\u2223\u2223\u2223\u2223\n\u03b7(t)\n\n= Re [\u2212i\u03b7\u2032(t) f \u2032(\u03b7(t))] (11)\n\nThus, for \u03b7(t) \u2208 \u0393k for k = `+ 1, . . . ,m, it follows from (3e), (8), and (11) that\n\nRe [\u2212i\u03b7\u2032(t) f \u2032(\u03b7(t))] = \u2212\n1\n\n\u03c0\nRe\n\n[\n\u03b7\u2032(t)\n\n1\u2212 \u03b7(t)\n+\n\n\u03b7\u2032(t)\n\n1 + \u03b7(t)\n\n]\n.\n\nIntegrating with respect to the parameter t for t \u2208 Jk, k = `+ 1, . . . ,m, we obtain\n\nRe [\u2212if(\u03b7(t))] =\n1\n\n\u03c0\nlog\n\n\u2223\u2223\u2223\u22231\u2212 \u03b7(t)1 + \u03b7(t)\n\u2223\u2223\u2223\u2223+ \u03b4k, (12)\n\nwhere the integration constants \u03b4k are undetermined. The constants \u03b4k, k = 1, . . . ,m\nin (10) and (12) are determined so that f(z) is a single-valued analytic function.\n\nSince we are interested in the function u, the real part of f , we may assume that\nc = f(\u03b1) is real for some given point \u03b1 in G. Define an analytic function g(\u03b6) in\nthe domain G through\n\nf(\u03b6) = (\u03b6 \u2212 \u03b1)g(\u03b6) + c. (13)\n\n\n\n7\n\nDefine also\nA(t) = e\u2212i\u03b8(t)(\u03b7(t)\u2212 \u03b1), (14)\n\nwhere \u03b8(t) is the piecewise constant function given by\n\n\u03b8(t) =\n\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\n0, t \u2208 J0,\n...\n\n0, t \u2208 J`,\n\u03c0/2, t \u2208 J`+1,\n\n...\n\u03c0/2, t \u2208 Jm.\n\n(15)\n\nThus\ne\u2212i\u03b8(t)f(\u03b7(t)) = A(t)g(\u03b7(t)) + e\u2212i\u03b8(t)c,\n\nwhich implies that\n\nRe[A(t)g(\u03b7(t))] = Re[e\u2212i\u03b8(t)f(\u03b7(t))]\u2212 c cos \u03b8(t).\n\nOn the basis of the conditions (9), (10), and (12), the function g(z) satisfies the\nRiemann-Hilbert problem\n\nRe[A(t)g(\u03b7(t))] = \u03b3(t) + h(t), (16)\n\nwhere\n\nh(t) =\n\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\n\u2212c, t \u2208 J0,\n\u03b41 \u2212 12 \u2212 c, t \u2208 J1,\n\n...\n\u03b4` \u2212 12 \u2212 c, t \u2208 J`,\n\u03b4`+1, t \u2208 J`+1,\n\n...\n\u03b4m, t \u2208 Jl+p.\n\n, \u03b3(t) =\n\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\n0, t \u2208 J0,\n\u2212 1\n\u03c0\n\nIm log\n1\u2212\u03b7(t)\n1+\u03b7(t)\n\n, t \u2208 J1,\n...\n\n\u2212 1\n\u03c0\n\nIm log\n1\u2212\u03b7(t)\n1+\u03b7(t)\n\n, t \u2208 Jl,\n1\n\u03c0\n\nlog\n\u2223\u2223\u22231\u2212\u03b7(t)1+\u03b7(t) \u2223\u2223\u2223 , t \u2208 J`+1,\n\n...\n1\n\u03c0\n\nlog\n\u2223\u2223\u22231\u2212\u03b7(t)1+\u03b7(t) \u2223\u2223\u2223 , t \u2208 Jm.\n\n(17)\n\nIt is clear that the function \u03b3 is known and the piecewise constant function h is\nunknown and should be determined. Let \u00b5(t) = Im[A(t)g(\u03b7(t))], i.e., the boundary\nvalues of an analytic function g are given by\n\ng(\u03b7(t)) =\n\u03b3(t) + h(t) + i\u00b5(t)\n\nA(t)\n, t \u2208 J. (18)\n\nThus, in order to find the boundary values of the analytic function g, we need to\ndetermine the two unknown functions \u00b5 and h. These two functions can be computed\nusing the boundary integral equation with the generalized Neumann kernel [11, 12,\n13].\n\nLet H be the space of all real Ho\u0308lder continuous functions on \u0393, let I be the\nidentity operator, and let the integral operators N and M are defined on H by\n\nN\u00b5(s) =\n\n\u222b\nJ\n\n1\n\n\u03c0\nIm\n\n(\nA(s)\n\nA(t)\n\n\u03b7\u2032(t)\n\n\u03b7(t)\u2212 \u03b7(s)\n\n)\n\u00b5(t)dt, s \u2208 J,\n\nM\u00b5(s) =\n\n\u222b\nJ\n\n1\n\n\u03c0\nRe\n\n(\nA(s)\n\nA(t)\n\n\u03b7\u2032(t)\n\n\u03b7(t)\u2212 \u03b7(s)\n\n)\n\u00b5(t)dt, s \u2208 J.\n\n\n\n8\n\nThe kernel of the operator N is known as the generalized Neumann kernel. For\nmore details, see [11, 12, 13]. On account of [13], we have \u00b5 is the unique solution\nof the integral equation\n\n(I\u2212N)\u00b5 = \u2212M\u03b3. (19)\n\nAdditionally, the piecewise constant function h is given by\n\nh = [M\u00b5\u2212 (I\u2212N)\u03b3]/2. (20)\n\nWe compute approximations to the functions \u00b5 in (19) and h in (20) by the\nMATLAB function fbie from [12]. This function employs a discretization of the\nintegral equation (19) by the Nystro\u0308m method using the trapezoidal rule [14] to\nobtain an algebraic linear system of size (m+1)n\u00d7(m+1)n where n is the number of\ndiscretization points in each boundary component. The resulting system is solved by\napplying the generalized minimal residual method through the MATLAB function\ngmres. The matrix-vector multiplication in gmres is computed using the MATLAB\nfunction zfmm2dpart from the FMMLIB2D MATLAB toolbox [15]. The values of the\nother parameters in the function fbie are chosen as in [16]. For more details, we\nrefer the reader to [12].\n\n4 Computing the temperature distribution and\n\nthe heat flux\n\nBy computing \u00b5 and h, we obtain the boundary values of the function g through (18).\nThe values of the function g(\u03b6) for \u03b6 \u2208 G can be computed by the Cauchy integral\nformula. For the numerical computation of g(\u03b6) for \u03b6 \u2208 G, we use the MATLAB\nfunction fcau from [12]. Then, the values of f(\u03b6) can be computed by (13) and\nhence the values of the solution of the boundary value problem (2) is given for\n\u03b6 \u2208 G by\n\nU(\u03b6) = Re [f(\u03b6) + f0(\u03b6)] .\n\nWe deduce the values of the temperature distribution T (z) for any z \u2208 \u2126 by\n\nT (z) = Re\n[\nf(\u03a6\u22121(z)) + f0(\u03a6\n\n\u22121(z))\n]\n.\n\nMoreover, by computing the piecewise constant function h, we can compute as well\nthe values of the undetermined real constants c, \u03b41, . . . , \u03b4m from (16).\n\nThe function T (z) is the real part of the function\n\nF (z) = f(\u03a6\u22121(z)) + f0(\u03a6\n\u22121(z)), z \u2208 \u2126.\n\nAccording to the Cauchy-Riemann equations, it follows that the derivative of the\ncomplex potential F (z) on \u2126 is given by\n\nF \u2032(z) =\n\u2202T\n\n\u2202x\n\u2212 i\n\n\u2202T\n\n\u2202y\n.\n\nOne the other hand,\n\nF \u2032(z) =\nf \u2032(\u03a6\u22121(z))\n\n\u03a6\u2032(\u03a6\u22121(z))\n+\nf \u20320(\u03a6\n\n\u22121(z))\n\n\u03a6\u2032(\u03a6\u22121(z))\n, z \u2208 \u2126, (21)\n\n\n\n9\n\nwhere the denominator does not vanish in the domain \u2126 since \u03a6 is a conformal\nmapping. Therefore the heat flux can be expressed for z \u2208 D in terms of F \u2032(z) by\nthe formula\n\nq(z) = \u2212\n(\n\u2202T\n\n\u2202x\n,\n\u2202T\n\n\u2202y\n\n)\u2223\u2223\u2223\u2223\nz\n\n\u2261 \u2212F \u2032(z). (22)\n\nHence\n\u2202T\n\n\u2202y\n= \u2212 ImF \u2032(z). (23)\n\nThe derivatives f \u20320(\u03a6\n\u22121(z) and \u03a6\u2032(\u03a6\u22121(z)) in (21) can be computed analytically.\n\nSo, the values of the heat flux q can be estimated on the domain \u2126 by first approx-\nimating the derivatives of the boundary values of the analytic function f on each\nboundary components. This can be done by approximating the function f(\u03b7(t))\nusing trigonometric interpolating polynomials then differentiating. The values of\nf \u2032(\u03a6\u22121(z)), in the right-hand side of (21), can be then computed for z \u2208 \u2126 using\nthe Cauchy integral formula.\n\n5 Computing the effective thermal conductivity\n\nThe medium matrix without inhomogeneities is assumed to be homogeneous and\nisotropic. We will assume that the CNTs and the circular voids are in the part of\nthe domain between x = \u22121 and x = 1. Thus, the effective conductivity of a layer\nin the y-direction \u03bby is calculated by the formula (3.2.33) from the book [6, p. 53],\nwhich in our case on account of (23) becomes\n\n\u03bby = \u2212\n1\n\n2\n\n\u222b 1\n\u22121\n\n\u2202T\n\n\u2202y\n(x, 0) dx =\n\n1\n\n2\nIm\n\n[\u222b 1\n\u22121\nF \u2032(x) dx\n\n]\n. (24)\n\nSince\n\n\u03be0(t) = \u03a6(\u03b70(t)) = \u03a6(e\nit) =\n\n1\n\n\u03c0\nlog\n\n1 + eit\n\n1\u2212 eit\n+\n\ni\n\n2\n, 0 \u2264 t \u2264 2\u03c0,\n\nwhere for 0 < t < \u03c0, \u03be0(t) is on the line y = 1 and for \u03c0 < t < 2\u03c0, \u03be0(t) is on the\nreal line. Thus, for \u03c0 < t < 2\u03c0, we have\n\n\u03be0(t) =\n1\n\n\u03c0\nlog\n\n\u2223\u2223\u2223\u2223cot t2\n\u2223\u2223\u2223\u2223 .\n\nSince \u22121 = \u03be0(t1) and 1 = \u03be0(t2) where\n\n\u03c0 < t1 = 2\u03c0 \u2212 2 tan\u22121 (e\u03c0) < t2 = 2\u03c0 \u2212 2 tan\u22121\n(\ne\u2212\u03c0\n)\n< 2\u03c0. (25)\n\nConsequently, (24) can be written as\n\n\u03bby =\n1\n\n2\nIm\n\n[\u222b t2\nt1\n\nF \u2032(\u03be0(t))\u03be\n\u2032\n0(t) dt\n\n]\n. (26)\n\nIn combining (21) with the fact that \u03be\u20320(t) = ie\nit\u03a6\u2032(eit), we can see that\n\nF \u2032(\u03be0(t)) =\nf \u2032(\u03a6\u22121(\u03be0(t)))\n\n\u03a6\u2032(\u03a6\u22121(\u03be0(t)))\n+\nf \u20320(\u03a6\n\n\u22121(\u03be0(t)))\n\n\u03a6\u2032(\u03a6\u22121(\u03be0(t)))\n=\nf \u2032(eit)\n\n\u03a6\u2032(eit)\n+\nf \u20320(e\n\nit)\n\n\u03a6\u2032(eit)\n.\n\n\n\n10\n\nHence,\n\n\u03bby =\n1\n\n2\nIm\n\n[\u222b t2\nt1\n\n[\nieit\n(\nf \u2032(eit) + f \u20320(e\n\nit)\n)]\n\ndt\n\n]\n, (27)\n\nwhich implies that\n\n\u03bby =\n1\n\n2\nIm\n[\nf(eit2)\u2212 f(eit1)\n\n]\n+\n\n1\n\n2\nIm\n[\nf0(e\n\nit2)\u2212 f0(eit1)\n]\n. (28)\n\nThe second term in the right-hand side of (27) does not depend on the CNTs or the\nvoids. In view of (6) and (25), we have\n\n1\n\n2\nIm\n[\nf0(e\n\nit2)\u2212 f0(eit1)\n]\n\n= 1,\n\nand hence\n\n\u03bby = 1 +\n1\n\n2\nIm\n[\nf(eit2)\u2212 f(eit1)\n\n]\n. (29)\n\nSince eit1 and eit2 are on the unit circle \u03930, the external boundary of G, and taking\ninto account (13), (14), (15), and (18), Equation (29) can be written as\n\n\u03bby = 1 +\n1\n\n2\n[\u00b5(t2)\u2212 \u00b5(t1)] . (30)\n\nBy solving the integral equation (19), we obtain approximate values of \u00b5 at the\ndiscretization points. These values are employed to interpolate the approximate\nsolution \u00b5 on J0 by a trigonometric interpolation polynomial, which is then used to\napproximate the values of \u00b5(t1) and \u00b5(t2).\n\n6 Numerical results\n\nThe above proposed method with n = 211 is applied to compute the temperature\nfield T and the heat flux q for several examples. We will choose the CNTs and the\ncircular voids within the part of the domain between x = \u22121 and x = 1. To compute\nthe values of the temperature distribution T and the heat flux q, we discretize part\nof the domain \u2126, namely for \u22121.5 \u2264 x \u2264 1.5 and 0.0001 \u2264 y \u2264 0.9999. Afterwards,\nwe compute the values of the temperature distribution T and the heat flux q at\nthese points as described in Section 4.\n\n6.1 The domain \u2126 with only circular voids\n\nIn this subsection, we consider the domain \u2126 with m non-overlapping circular holes\nand without any CNT (i.e., ` = 0). We also assume that all circular holes have the\nsame radius r with the parametrization\n\n\u03b7j(t) = zj + re\n\u2212it, 0 \u2264 t \u2264 2\u03c0, j = 1, 2, . . . ,m,\n\nwhere z1, z2, . . . , zm are the centers of the circular holes. As these circular holes\nare chosen in the part of the domain \u2126 between x = \u22121 and x = 1, we define the\nconcentration c(m, r) of these voids to be the area of these circular holes over the\narea of the rectangle {(x, y) : \u22121 \u2264 x \u2264 1, 0 \u2264 y \u2264 1}, i.e.,\n\nc(m, r) =\nmr2\u03c0\n\n2\n. (31)\n\n\n\n11\n\nThe Clausius-Mossotti approximation (CMA) also known as Maxwel\u2019s formula\ncan be applied for dilute composites when the concentration (31) is sufficiently small.\nBelow, we write this formula for a macroscopically isotropic media with insulators\nof identical circular holes within the precisely established precision in [17]\n\n\u03bbe =\n1\u2212 c\n1 + c\n\n+O(c3). (32)\n\nExample 1 We consider m = 5 circular holes with the radius r for 0 < r < 0.2.\nFor Case I, we assume the centers of the holes to be set to \u22120.8 + 0.5i, \u22120.4 + 0.5i,\n0.5i, 0.4 + 0.5i, and 0.8 + 0.5i. The contour plot of T and |q| for r = 0.1 are shown\nin Figure 3 (first row). The approximate value of the effective thermal conductivity\nfor r = 0.1 is\n\n\u03bby = 0.8533491.\n\nWhen r is close to 0.2, the circular holes become adjacent to each other. To show\nthe effects of the radius r on the effective thermal conductivity \u03bby, we compute the\nvalues of \u03bby for several values of r, 0.00001 \u2264 r \u2264 0.19999. The obtained results\nare presented in Figure 4 where, by (31), the concentration of these 5 holes is\nc = c(5, r) = 5r2\u03c0/2 \u2248 7.854r2 for 0 < c < \u03c0/10 and 0 < r < 0.2. The values of\nthe estimated effective conductivity \u03bbe is given also in Figure 4. As one can expect,\nthere is a good agreement between \u03bby and \u03bbe for small values of c. In the same time,\nthe divergence of \u03bby and \u03bbe is observed for the concentrations greater than 0.1.\n\nFor Case II, the centers of the holes become \u22120.8+0.5i, \u22120.4+0.3i, 0.5i, 0.4+0.7i,\nand 0.8 + 0.5i, which means the centers are not anymore horizontally aligned as the\nsecond and fourth centers are now shifted by 0.2 up and down, respectively. This is\ndisplayed in Figure 3 (second row). The curve showing the obtained values of \u03bby as\na function of the concentration is depicted in Figure 4.\n\nFigure 4 illustrates that the values of \u03bby depend on the position of the circular\nholes centers while the values of \u03bbe are the same for both cases since it depends only\non the concentration of the circular holes and not on their positions. We notice a\nbetter agreement between \u03bby and \u03bbe in Cases II when comparing to Case I.\n\nExample 2 We consider m = 30 circular holes with centers xk + 0.25i, xk + 0.5i,\nand xk + 0.75i, where xk = \u22120.9 + 0.2(k \u2212 1) for k = 1, 2, . . . , 10, and with radius r\nfor 0 < r < 0.1. The contour plot of T and |q| for r = 0.099 are shown in Figure 5.\nThe approximate value of the effective thermal conductivity for r = 0.099 is\n\n\u03bby = 0.1519156.\n\nWhen r is close to 0.1, the circular holes become adjacent to each other. We\ncompute the values of \u03bby for several values of r, 0.00001 \u2264 r \u2264 0.09999. The\nobtained results are depicted in Figure 6 (left) where, by (31), the concentration of\nthese 30 holes is c = c(30, r) = 30r2\u03c0/2 \u2248 47.124r2. Note that 0 < c < 3\u03c0/20 for\n0 < r < 0.1.\n\nExample 3 We take up here the case of m = 50 circular holes with centers xk+0.1i,\nxk + 0.3i, xk + 0.5i, xk + 0.7i, and xk + 0.9i, where xk = \u22120.9 + 0.2(k \u2212 1) for\nk = 1, 2, . . . , 10, and with radius r for 0 < r < 0.1. On the basis of (31), the\nconcentration of these 50 holes is c = c(50, r) = 50r2\u03c0/2 \u2248 78.54r2. For 0 < r < 0.1,\n\n\n\n12\n\nFigure 3: A contour plot of the temperature distribution T and the heat flux |q| for\nthe domain \u2126 with m = 5 circular holes (Example 1 for r = 0.1). First row for Case\nI and second row for Case II.\n\nFigure 4: The effective thermal conductivity \u03bby and the estimated effective con-\nductivity \u03bbe in (32) vs. the concentration c(m, r) = 5r\n\n2\u03c0/2 for the domain \u2126 with\nm = 5 circular holes for 0.00001 \u2264 r \u2264 0.19999. The vertical dotted line is c = \u03c0/10.\n\nwe have 0 < c < \u03c0/4. When r is close to 0.1, the circular holes become adjacent\nto each other, and the concentration is almost equal to \u03c0/4. The obtained results\nshowing the behavior of \u03bby as a function of the radius r, for 0.001 \u2264 r \u2264 0.099, are\npresented in Figure 6 (right).\n\n6.2 The domain \u2126 with only CNTs\n\nIn this subsection, we consider the domain \u2126 with m non-overlapping elliptic CNTs\nwithout any circular holes (i.e., m = `). We assume that all CNTs have equal sizes\nand are of elliptic shape where the ellipses have the parametrization\n\n\u03b7j(t) = zj + a cos t\u2212 ib sin t, 0 \u2264 t \u2264 2\u03c0, j = 1, 2, . . . ,m, (33)\n\nwhere zj is the center of the ellipse, 2a and 2b are the length of the ellipses axes\nin the x and y-directions, respectively. If a/b > 1, the major axis of the ellipses is\n\n\n\n13\n\nFigure 5: A contour plot of the temperature distribution T and the heat flux |q| for\nthe domain \u2126 with 30 circular holes (r = 0.099).\n\nFigure 6: The effective thermal conductivity \u03bby vs. the concentration c(m, r) =\nmr2\u03c0/2. On the left, the domain \u2126 with m = 30 circular holes (Example 2) and\n0.00001 \u2264 r \u2264 0.09999. The vertical dotted line is c = 3\u03c0/20. On the right, the\ndomain \u2126 with m = 50 circular holes (Example 3) and 0.001 \u2264 r \u2264 0.099. The\nvertical dotted line is c = \u03c0/4.\n\nhorizontal, if a/b < 1, the major axis of the ellipses is vertical, and if a/b = 1, the\nellipses reduced to circles. Here, we choose a and b such that their ratio satisfies\n0.1 \u2264 a/b \u2264 10. These elliptic shape CNTs are chosen in the part of the domain\n\u2126 between x = \u22121 and x = 1. So, we define the concentration c(m, a, b) of these\nCNTs to be\n\nc(m, a, b) =\nmab\u03c0\n\n2\n. (34)\n\nIf a\nb\n\ufffd 1, instead of (34) the plane slits density is considered in the theory of\n\ncomposites and porous media\n\n\u03c6 =\nmb2\n\n|\u2126|\n=\nmb2\n\n2\n, (35)\n\nFor a macroscopically isotropic media with only perfectly conducting identical\ncircular inclusions (CNTs), an approximation of the effective conductivity \u03bbe is given\nby the inverse to (32) value (see [17])\n\n\u03bbe =\n1 + c\n\n1\u2212 c\n+O(c3). (36)\n\nExample 4 We consider m = 5 elliptic CNTs with centers \u22120.8 + 0.5i, \u22120.4 + 0.5i,\n0.5i, 0.4 + 0.5i, and 0.8 + 0.5i, and where 0 < a < 0.2 and 0 < b < 0.5. Figure 7\n\n\n\n14\n\n(first row) presents the contour plot of T and |q| for a = 0.19 and b = 0.019 (the\nellipses are horizontal). For these values of a and b, the approximate value of the\neffective thermal conductivity is\n\n\u03bby = 1.0272480.\n\nFor a = 0.019 and b = 0.19, the contour plot of T and |q| are shown in Figure 7\n(second row). The approximate value of the effective thermal conductivity for these\nvalues of a and b is\n\n\u03bby = 1.2804116.\n\nThe CNTs in Figure 7 have the same concentration. However, the value of \u03bby is\nlarger for the vertical ellipses case.\n\nWhen a approaches 0.2, the ellipses get adjacent to each other. On the other\nhand, they come close to the upper and lower walls when b approaches 0.5. We\ncompute the values of \u03bby for several values of a, 0.0001 \u2264 a \u2264 0.1999, with b =\n0.1a. The obtained results are presented in Figure 8 (left) where, by (34), the\nconcentration of these 5 ellipses is c = c(5, a, b) = 5ab\u03c0/2 = a2\u03c0/4 \u2248 0.7854a2.\nNote that, for 0 < a < 0.2 and b = 0.1a, we have 0 < c < \u03c0/100.\n\nThe values of \u03bby are also computed for several values of b for 0.001 \u2264 b \u2264 0.499\nwith a = 0.1b. Since a/b = 0.1 is small, the obtained values of \u03bby are plotted versus\nthe values of \u03c6 = 2.5b2, given by (35), where 0 < \u03c6 < 5/8 for 0 < b < 0.5. The\nobtained results are presented in Figure 8 (right).\n\nFigure 7: A contour plot of the temperature distribution T and the heat flux |q| for\nthe domain \u2126 with m = 5 elliptic CNTs (Example 4), where a = 0.19 and b = 0.019\nfor the first row and a = 0.019 and b = 0.19 for the second row.\n\nExample 5 We consider m = 200 elliptic CNTs with centers xk + iyj for k =\n1, 2, . . . , 20 and j = 1, 2, . . . , 10 where xk = \u22120.95 + (k\u2212 1)/10 and yj = 0.05 + (j\u2212\n1)/10, and with 0 < a < 0.05 and 0 < b < 0.05.\n\nWe compute the values of \u03bby for several values of a, 0.0002 \u2264 a \u2264 0.0498,\nand a/b = 10 (i.e., the ellipses are horizontal) where the ellipses become close to\neach other when a approaches 0.05. The obtained results are presented in Figure 9\n\n\n\n15\n\nFigure 8: The effective thermal conductivity \u03bby for the domain \u2126 with m = 5\nelliptic CNTs (Example 4). On the left, the effective thermal conductivity \u03bby vs.\nthe concentration c = a2\u03c0/4 for 0.0001 \u2264 a \u2264 0.1999 and a/b = 10. The vertical\ndotted line is c = \u03c0/100. On the right, the effective thermal conductivity \u03bby vs.\nthe plane slits density \u03c6 = 2.5b2 for 0.001 \u2264 b \u2264 0.499 and a/b = 0.1. The vertical\ndotted line is \u03c6 = 0.625.\n\n(left) where the concentration of these 200 ellipses is c = 10a2\u03c0 \u2248 31.416a2. For\n0 < a < 0.05 and a/b = 10, we have 0 < c < \u03c0/40. Then, we compute the values\nof \u03bby for several values of b, 0.0002 \u2264 b \u2264 0.0498, and a/b = 0.1. The obtained\nresults for \u03bby versus the the plane slits density \u03c6 = mb\n\n2/2 = 100b2 are presented in\nFigure 9 (right) where 0 < \u03c6 < 1/4 for 0 < b < 0.05 and a/b = 0.1.\n\nWhen a/b = 1, the ellipses reduce to circles. We compute the values of \u03bby for\nseveral values of a, 0.0002 \u2264 a \u2264 0.0498. The obtained results are presented in\nFigure 10 where the concentration of these 200 ellipses is c = 100a2\u03c0 \u2248 314.16a2.\nFor 0 < a < 0.05 and a/b = 1, we have 0 < c < \u03c0/4. Figure 10 presents also the\nvalues of the estimated effective conductivity \u03bbe given by (36).\n\nFigure 9: The effective thermal conductivity \u03bby for the domain \u2126 with m = 200\nelliptic CNTs (Example 5). On the left, the effective thermal conductivity \u03bby vs.\nthe concentration c = 10a2\u03c0 for 0.0002 \u2264 a \u2264 0.0498 with a/b = 10. The vertical\ndotted line is c = \u03c0/40. On the right, the effective thermal conductivity \u03bby vs. the\nplane slits density \u03c6 = 100b2 for 0.0002 \u2264 b \u2264 0.0498, with a/b = 0.1. The vertical\ndotted line is \u03c6 = 1/4. The vertical dotted line is c = 1/4.\n\n\n\n16\n\nFigure 10: The effective thermal conductivity \u03bby (for the domain \u2126 with m = 200\ncircular CNTs obtained by setting b = a in Example 5) and the estimated effective\nconductivity \u03bbe in (36) vs. the concentration c = 100a\n\n2\u03c0 for 0.0002 \u2264 a \u2264 0.0498.\nThe vertical dotted line is c = \u03c0/4.\n\n6.3 The domain \u2126 with 2000 CNTs and/or circular voids\n\nWe are concerned in this section with the study of a large number of perfect con-\nductors and/or insulators. We consider two example where in the first both perfect\nconductors and insulators have the same circular shape, while in the second, con-\nductors have an elliptic shape and insulators have a circular shape. The present\ninvestigation is useful when studying the impact of geometric shapes on the macro-\nscopic properties of three-phases high contrast media.\n\nExample 6 We take m = 2000 circular holes of equal size with radius r = 0.0075.\nIn this example, the concentration c = c(m, r) = 1000r2\u03c0 \u2248 0.1767 is constant\nand the locations of these holes are chosen randomly. In this case, the following\nextension of CMA may be used\n\n\u03bbe =\n1 + c1 \u2212 c2\n1\u2212 c1 + c2\n\n+O(c3), (37)\n\nwhere c1 denotes the conductor concentration, c2 the insulator concentration, and\nc = c1 + c2. Three cases are considered:\n\nCase I: We assume that half of the holes are CNTs and the other half are voids (see\nFigure 11). For this case, c1 and c2 are given by c1 = c2 = 500r\n\n2\u03c0 \u2248 0.0884.\n\nCase II: All holes are voids, and hence c1 = 0 while c2 = 1000r\n2\u03c0 \u2248 0.1767.\n\nCase III: All holes are CNTs, and hence c1 = 1000r\n2\u03c0 \u2248 0.1767 while c2 = 0.\n\nFor each case, we run the code for 20 times, so that to get 20 different locations\nfor these circular holes. In each of these 20 experiments, we compute the value of\nthe effective thermal conductivity \u03bby by the presented method and the values of the\nestimated effective conductivity \u03bbe by (32). As we can see from Figure 12, \u03bbe is a\nconstant and the values of \u03bby depend on the locations of the holes.\n\n\n\n17\n\nFigure 11: The domain \u2126 with m = 2000 circular holes. Case I: We have p = 1000\nvoids (blue circles) and ` = 1000 CNTs (red circles).\n\nExample 7 We consider m = 2000 elliptic and circular holes with ` = 1000 elliptic\nperfect conductors and p = 1000 circular insulators of equal area \u03c0r2 (see Figure 13).\nThe radius r is chosen to be the same as in the previous example, i.e., r = 0.0075.\nThe locations of both elliptic and circular holes are chosen randomly. For the ellipses,\nwe assume that the ratio between the length of the major axis and the minor axis\nis 4, and the angles between the major axis and the x-axis are chosen randomly.\nAs in the previous example, we run the code for 20 times. In each of these 20\nexperiments, we compute the value of the effective thermal conductivity \u03bby by the\npresented method. The computed values are shown in Figure 13 (left).\n\nSince we have the same number of elliptic perfect conductors and circular insu-\nlators of equal area \u03c0r2, the conductor concentration c1 and the insulator concen-\ntration c2 are equal and given by c1 = c2 = 500r\n\n2\u03c0 \u2248 0.0884. Although c1 and c2\nhere are the same as in Case I of the previous example, it is clear from Figures 12\n(first row) and 13 (right) that the values \u03bby in this example (elliptic conductors) are\nlarger than those in the previous example (circular conductors).\n\n\n\n18\n\nFigure 12: The values of the effective thermal conductivity \u03bby and the estimated\neffective conductivity \u03bbe in (37) (for the domain \u2126 with m = 2000 circular holes in\nExample 6) vs. the number of the experiment for Case I (first row), Case II (second\nrow, left), and Case III (second row, right).\n\nFigure 13: On the left, the domain \u2126 in Example 7 with m = p + ` = 2000 holes,\np = 1000 circular voids (blue) and ` = 1000 elliptic CNTs (red). On the right, the\nvalues of the effective thermal conductivity \u03bby vs. the number of the experiment.\n\n\n\n19\n\n6.4 The dependence of \u03bby on \u03c6 and c\n\nWe consider now a domain domain \u2126 with m = ` = 276 non-overlapping elliptic\nCNTs without any void. We assume that all CNTs are of equal size and elliptic\nshape. The ellipses are parametrized by (33) with a < b, which means they are\ntaken to be vertical.\n\nFirst we assume that the concentration c = c(m, a, b) is constant and we choose\nthe values of the parameters a and b such that the plane slits density \u03c6 = \u03c6(m, a, b) \u2208\n[0.4, 1.3]. The domain \u2126 for c = 0.5 and \u03c6 = 1.3 is shown in Figure 14. We consider\nas well five values of the concentration, c = 0.1, 0.2, 0.3, 0.4, 0.5. Then, for each of\nthese values, we compute and show in Figure 15 (left) the values of \u03bby = \u03bby(\u03c6).\n\nAfterwards, we take up the plane slits density \u03c6 = \u03c6(m, a, b) to be constant\nand we choose the values of the parameters a and b such that the concentration\nc = c(m, a, b) \u2208 [0.1, 0.5]. We consider four values of \u03c6, \u03c6 = 0.4, 0.7, 1, 1.3, and\ncompute again \u03bby = \u03bby(c) for each case. The obtained results are presented in\nFigure 15 (right).\n\nFigure 14: The domain \u2126 with m = 276 elliptic CNTs for c = 0.5 and \u03c6 = 1.3.\n\nFigure 15: The effective thermal conductivity \u03bby for the domain \u2126 with m = 276\nelliptic CNTs. On the left, the values of \u03bby = \u03bby(\u03c6) for \u03c6 \u2208 [0.4, 1.3] and for several\nvalues of c. On the right, the values of \u03bby = \u03bby(c) for c \u2208 [0.1, 0.5] and for several\nvalues of \u03c6.\n\n\n\n20\n\n7 Conclusion\n\nA systematic estimation of the local fields and the effective conductivity properties\nof 2D composites reinforced by uniformly and randomly distributed CNTs is car-\nried out. It is assumed that the medium may contains voids as well. The CNTs\nare considered as perfectly conducting elliptic inclusions and the voids as circular\ninsulators. For definiteness, a composite strip is considered with the given constant\nexternal field passing through the strip. The local field is governed by the Laplace\nequation in the multiply connected domain formed by the strip without two types\nof holes, CNTs and voids. The Dirichlet boundary condition is imposed on the\nCNTs boundary and the Neumann boundary condition governs the void boundary.\nA numerical method is developed to solve the mixed problem for a large number\nof CNTs and voids. The method is based on using the boundary integral equation\nwith the generalized Neumann kernel [11, 12]. One key feature of this method is\nthat it can be employed for domains with complex geometry as it provides accurate\nresults even when the boundaries are close together. To solve the integral equation,\nthe Fast Multipole Method has been employed, which enables to treat the case of\nthousands of CNTs and voids. With the help of conformal mappings, the presented\nmethod can be extended to include the case when CNTs and voids are rectilinear\nslits as done in [16] for example.\n\nThe computational study has shown a dependence of the local fields and the\neffective conductivity \u03bby on the concentration of voids c given by (34) as well as\non the density \u03c6 of CNTs given by (35). Besides the opposite conductive proper-\nties, voids and CNTs have also different types of the geometric parameters c and\n\u03c6 that are not reduced to each other. Hence, the present study is concerned ad-\nditionally with the case of three-phase composites of high contrast conductivity.\nIt is demonstrated that our simulations are covered with the classical lower order\napproximations (Clausius-Mossotti, Maxwell) for dilute composites. The high or-\nder concentrations and densities led to different results from the classical ones. It\nis worth noting that the huge number of numerical experiments for uniformly dis-\ntributed inclusions yield the graphical dependencies of \u03bby on c and \u03c6, which can be\nused in practical applications.\n\nAcknowledgments\n\nReferences\n\n[1] Mostafizur Rahaman, Dipak Khastgir, and Ali Kanakhir Aldalbahi. Carbon-\ncontaining polymer composites. Springer, 2019.\n\n[2] Lichao Feng, Ning Xie, and Jing Zhong. Carbon nanofibers and their compos-\nites: a review of synthesizing, properties and applications. Materials, 7(5):3919\u2013\n3945, 2014.\n\n[3] Marcio Loos. Carbon nanotube reinforced composites: CNT Polymer Science\nand Technology. Elsevier, 2014.\n\n[4] Ronald L Poveda and Nikhil Gupta. Carbon nanofiber reinforced polymer com-\nposites. Springer, 2016.\n\n\n\n21\n\n[5] Pierre M Adler, Jean-Franc\u0327ois Thovert, and Valeri V Mourzenko. Fractured\nporous media. Oxford University Press, 2013.\n\n[6] S. Gluzman, V. Mityushev, and W. Nawalaniec. Computational analysis of\nstructured Media. Academic Press, London, 2017.\n\n[7] Mohamed Nasser, Ali HM Murid, and Samer AA Al-Hatemi. A boundary\nintegral equation with the generalized neumann kernel for a certain class of\nmixed boundary value problem. J. Appl. Math., 2012, 2012.\n\n[8] Mohamed MS Nasser and Matti Vuorinen. Numerical computation of the ca-\npacity of generalized condensers. J. Comput. Appl. Math., 377:112865, 2020.\n\n[9] S Malekie and F Ziaie. Study on a novel dosimeter based on polyethylene\u2013\ncarbon nanotube composite. Nuclear Instruments and Methods in Physics Re-\nsearch Section A: Accelerators, Spectrometers, Detectors and Associated Equip-\nment, 791:1\u20135, 2015.\n\n[10] Jianming Zhang, Masataka Tanaka, and Toshiro Matsumoto. A simplified ap-\nproach for heat conduction analysis of cnt-based nano-composites. Computer\nmethods in applied mechanics and engineering, 193(52):5597\u20135609, 2004.\n\n[11] R. Wegmann and M.M.S. Nasser. The Riemann-Hilbert problem and the gener-\nalized Neumann kernel on multiply connected regions. J. Comput. Appl. Math.,\n214:36\u201357, 2008.\n\n[12] M.M.S. Nasser. Fast solution of boundary integral equations with the general-\nized Neumann kernel. Electron. Trans. Numer. Anal., 44:189\u2013229, 2015.\n\n[13] M.M.S. Nasser. Numerical conformal mapping of multiply connected regions\nonto the second, third and fourth categories of Koebe canonical slit domains.\nJ. Math. Anal. Appl., 382:47\u201356, 2011.\n\n[14] K.E. Atkinson. The Numerical Solution of Integral Equations of the Second\nKind. Cambridge University Press, Cambridge, 1997.\n\n[15] L. Greengard and Z. Gimbutas. FMMLIB2D: A MATLAB toolbox for fast\nmultipole method in two dimensions, version 1.2. edition, 2012. http://www.\ncims.nyu.edu/cmcl/fmm2dlib/fmm2dlib.html. Accessed 1 Jan 2018.\n\n[16] M.M.S. Nasser and E. Kalmoun. Application of integral equations to simulat-\ning local fields in carbon nanotube reinforced composites. In R. Mcphedran,\nS. Gluzman, V. Mityushev, and N. Rylko, editors, 2D and Quasi-2D Compos-\nite and Nanocomposite Materials: Properties and Photonic Applications, pages\n233\u2013248. Elsevier, Amsterdam, 2020.\n\n[17] V Mityushev and N Rylko. Maxwell\u2019s approach to effective conductivity and\nits limitations. Quarterly Journal of Mechanics and Applied Mathematics,\n66(2):241\u2013251, 2013.\n\nhttp://www.cims.nyu.edu/cmcl/fmm2dlib/fmm2dlib.html\nhttp://www.cims.nyu.edu/cmcl/fmm2dlib/fmm2dlib.html\n\n\t1 Introduction\n\t2 Problem formulation\n\t3 The integral equation method\n\t4 Computing the temperature distribution and the heat flux\n\t5 Computing the effective thermal conductivity\n\t6 Numerical results\n\t6.1 The domain  with only circular voids\n\t6.2 The domain  with only CNTs\n\t6.3 The domain  with 2000 CNTs and/or circular voids\n\t6.4 The dependence of y on  and c\n\n\t7 Conclusion\n\n"}
{"Title": "Robust reliability-based topology optimization under random-field material model", "Authors": "Trung Pham, Christopher Hoyle", "Abstract": "  This paper proposes an algorithm to find robust reliability-based topology optimized designs under a random-field material model. The initial design domain is made of linear elastic material whose property, i.e., Young's modulus, is modeled by a random field. To facilitate computation, the Karhunen-Lo\u00e8ve expansion discretizes the modeling random field into a small number of random variables. Robustness is achieved by optimizing a weighted sum of mean and standard deviation of a quantity of interest, while reliability is employed through a probabilistic constraint. The Smolyak-type sparse grid and the stochastic response surface method are applied to reduce computational cost. Furthermore, an efficient inverse-reliability algorithm is utilized to decouple the double-loop structure of reliability analysis. The proposed algorithm is tested on two common benchmark problems in literature. Finally, Monte Carlo simulation is used to validate the claimed robustness and reliability of optimized designs.      ", "Subject": "Optimization and Control (math.OC)", "ID": "arXiv:2201.00004", "Text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobust reliability-based topology optimization\nunder random-field material model\n\nTrung Pham\u2217\nDepartment of Aerospace Engineering\n\nUniversity of Michigan\nAnn Arbor, Michigan 48109, USA\n\nEmail: trungp@umich.edu\n\nChristopher Hoyle\nSchool of Mechanical, Industrial & Manufacturing Engineering\n\nOregon State University\nCorvallis, Oregon 97331\u20136001, USA\nEmail: chris.hoyle@oregonstate.edu\n\nThis paper proposes an algorithm to find robust reliability-\nbased topology optimized designs under a random-field ma-\nterial model. The initial design domain is made of linear\nelastic material whose property, i.e., Young\u2019s modulus, is\nmodeled by a random field. To facilitate computation, the\nKarhunen\u2212\u2013Loe\u0300ve expansion discretizes the modeling ran-\ndom field into a small number of random variables. Robust-\nness is achieved by optimizing a weighted sum of mean and\nstandard deviation of a quantity of interest, while reliability\nis employed through a probabilistic constraint. The Smolyak-\ntype sparse grid and the stochastic response surface method\nare applied to reduce computational cost. Furthermore, an\nefficient inverse-reliability algorithm is utilized to decouple\nthe double-loop structure of reliability analysis. The pro-\nposed algorithm is tested on two common benchmark prob-\nlems in literature. Finally, Monte Carlo simulation is used to\nvalidate the claimed robustness and reliability of optimized\ndesigns.\n\n1 Introduction\nA mechanical structure is characterized by its boundary\n\nand loading conditions, its material properties, and its topol-\nogy. Finding an appropriate topology is often a major task\nin structural design, which has fueled the rise of topology\noptimization (TO) in the last two decades. Without consid-\nering a designer\u2019s experience, TO is a mathematical tool to\nidentify the optimal size, shape, and connectivity of a de-\nsign [1], resulting in improved performance while using the\nleast amount of material. However, research in TO often\nonly concerns with deterministic inputs, while uncertainty\nis inherent in nature, which manifests itself in the stochastic-\nity of random parameters of engineered systems. The anal-\n\n\u2217Address all correspondence related to this paper to this author.\n\nysis and design of engineered systems are affected heavily\nby uncertainty; for example, modern design codes, such as\nACI 318 [2] and AISC 360 [3], have comprehensive recom-\nmendations of safety factors for loading, material property,\nconstruction conditions, etc., which obviously are intended\nto take into account uncertainty. Among different sources\nof uncertainty, material property is intrinsically random in\nspace, which has been modeled by random field in the de-\nsign of composite structures [4, 5, 6]. Such a modeling tech-\nnique has been especially popular in the vast literature of the\nStochastic Finite Element Method [7, 8, 9], which certainly\nproves its validity and the need to be considered in TO. So\nfar, uncertainty in TO has been treated separately by robust\noptimization or reliability-based optimization, while both ro-\nbustness and reliability are desired properties of design under\nuncertainty. Therefore, this paper presents an algorithm to\nfind robust reliability-based topology optimized design un-\nder a random-field material model.\n\nThere are a number of steps in our proposed algo-\nrithm, which are addressed in depth in the subsequent\nsections. Here we provide a brief overview of them.\nFirst, from a known covariance function, the modeling ran-\ndom field is estimated by a random polynomial using the\nKarhunen\u2212\u2013Loe\u0300ve expansion. To make the design robust,\na weighted sum of mean and standard deviation of a quantity\nof interest, which are computed by a Smolyak-type sparse\ngrid, is considered as the objective function. Reliability of\nthe design is reflected in the probabilistic constraint, which\nis handled by the Sequential Optimization and Reliability\nAssessment (SORA) method [10]\u2212a single-loop inverse-\nreliability algorithm\u2212coupled with the performance measure\napproach [11] to reduce the computational cost of reliability\nanalysis. The stochastic response surface method approxi-\nmates the random output, which is required to solve the in-\n\n1 Copyright \u00a9 by ASME\n\nar\nX\n\niv\n:2\n\n20\n1.\n\n00\n00\n\n4v\n1 \n\n [\nm\n\nat\nh.\n\nO\nC\n\n] \n 2\n\n9 \nD\n\nec\n 2\n\n02\n1\n\n\n\nverse reliability analysis problem.\nThe layout of this paper is as follows. Section 2 is\n\na literature review of uncertainty propagation, robust and\nreliability-based optimization, and reliability analysis, with\nfocus on TO under uncertainty. Section 3 derives the math-\nematical formulation of the deterministic TO problem and\nthe robust reliability-based topology optimization (RRBTO)\nproblem. This section also exposes the details of our pro-\nposed solution for the RRBTO problem. Two numerical ex-\namples show how our proposed algorithm works, and are\nverified by Monte Carlo simulation in section 4 followed by\ndiscussions of the results. Finally, the paper is summarized\nwith major findings, and then suggests future work.\n\n2 Background\nThe earliest idea of topology optimization (TO) can be\n\ntraced back to Michell\u2019s paper [12] in 1904. Since then TO\nhas been mature enough to have its own treatise [1]. As a\nmathematical optimization problem, TO requires specifica-\ntion of objective function(s) and constraint(s), which do not\ninvolve any probabilistic quantities when using deterministic\ninputs. Thus, changes are needed to deal with uncertainty\nin the forms of robust optimization (RO) and reliability-\nbased optimization (RBO). A number of papers, which are\nreviewed below, have tried to integrate uncertainty into a TO\nproblem using RO and RBO separately.\n\nRO primarily aims to minimize the variability of an\noutput of interest [13], due to uncertainty, around its mean\nvalue. Therefore, this goal can be formulated by minimizing\na weighted sum of mean and standard deviation of the output\nof interest. This approach was chosen in several papers cov-\nering various sources of uncertainty and solution methods:\nspatial variation of manufacturing error with Monte Carlo\nsimulations [14]; random-field truss material with a multi-\nobjective approach [15]; random loading field and random\nmaterial field with the level set method [16]; material and\ngeometric uncertainties with stochastic collocation methods\nand perturbation techniques [17, 18]; misplacement of ma-\nterial and imperfect geometry [19, 20]; Young\u2019s modulus of\ntruss members with a perturbation method [21]; random-field\nmaterial properties with a polynomial chaos expansion [22];\ngeometric and material property uncertainties with a stochas-\ntic perturbation method for frame structures [23]; material\nuncertainty with known second-order statistics [24]; random\nspatial distribution of Young\u2019s modulus and loading uncer-\ntainty in a stress-based problem [25,26]; and random loading\nfield with stochastic collocation methods [27]. The robust\ntopology optimization (RTO) problem is solved by a unified\nframework based on polynomial chaos expansion in [28],\nwhile [29] tackled the problem exploiting the linear elas-\nticity of structure. The seemingly arbitrary factors in the\nweighted sum are often cited as one major weakness of this\nRO methodology [30, 15]; however, they are well-defined in\ndecision-based design reflecting risk-taking attitude of de-\nsigners [31, 32, 33].\n\nInstead of modifying the objective function, RBO makes\nsome of the constraints probabilistic\u2212probability of failure\n\nevent is used in place of the event itself. This change requires\nspecialized methods to handle, because the probabilistic con-\nstraints are expressed by multiple integrals of the joint prob-\nability density function (PDF) of random variables, both of\nwhich are either practically impossible to obtain or very dif-\nficult to evaluate [34]. Many methods have been devised to\novercome such difficulties, which were surveyed thoroughly\nin [35]. Within the scope of this paper, we only briefly review\nthe first-order reliability methods (FORM), the second-order\nreliability methods (SORM), and the Sequential Optimiza-\ntion and Reliability Assessment (SORA) method. FORM\nappeared early [36] together with the concept of reliability\nindex [37] to solve RBO problems. SORM [38] followed\nto improve accuracy of the FORM in case of highly nonlin-\near limit state functions and/or slow decay of the joint PDF.\nThe main idea of FORM and SORM is to approximate the\nlimit state functions using first-order and second-order Tay-\nlor series, respectively, at appropriate values (i.e., means) of\nrandom variables. This results in a double-loop optimiza-\ntion problem to find the most probable point (MPP). In the\ncontext of reliability-based topology optimization (RBTO),\ndirectly solving the double-loop optimization problem has\nbeen shown in [39] for MEMS mechanisms with stochastic\nloading, boundary conditions as well as material properties;\nin [40] for shape uncertainty; in [41] for geometric imperfec-\ntions; in [42] for frame structures using system reliability un-\nder random-variable inputs; in [43] for electromagnetic sys-\ntems; in [44] for geometrically nonlinear structures; in [45]\nfor local failure constraints; and in [46] for continuum struc-\ntures subject to local stress constraints. In [47], FORM was\nreplaced by a mean-value, second-order saddlepoint approx-\nimation method, which was asserted to be more accurate.\nThe double-loop approach is prohibitively expensive and\nlacks robustness when a large number of random variables\npresents [48]. For this reason, single-loop approaches have\nbeen developed, in which, i.e., the Karush\u2212Kuhn\u2212Tucker\n(KKT) optimality conditions are utilized to avoid the inner\nloop. Both [49] and [50] used variants of the single-loop\nmethod in [51] for component and system reliability-based\nTO. In [52], it is somewhat unique when the authors used\ntheir own single-loop method [53]. Kogiso et al. [54] applied\nthe single-loop-single-vector method [55] for frame struc-\ntures under random-variable loads and nonstructural mass.\nAnother way to bypass the double-loop problem is the de-\ncoupling approaches [35], in which reliability analysis re-\nsults are used to facilitate the optimization loops. Among\nthem, the SORA method is known for its simple implemen-\ntation compared to the above single-loop methods, and its ef-\nficiency with FORM [10,56]. This method was employed for\nRBTO under random-variable inputs in [57] and [58]. Meta\nmodeling or surrogate modeling used together with simula-\ntion techniques to solve RBO problems has received consid-\nerable attention [59], but still remained relatively unexplored\nin TO literature. In [60], reliability was assessed using a\nprobabilistic neural network classifier for truss structures un-\nder random Young\u2019s modulus.\n\nBoth robustness and reliability are desired properties of\ndesign under uncertainty; however, to the best of our knowl-\n\n2 Copyright \u00a9 by ASME\n\n\n\nedge, this paper is the first one considering both criteria in\nTO. Therefore, a literature review of robust reliability-based\noptimization (RRBO) has to be drawn from other fields. The\nRRBO problem was investigated in [61] using an inverse re-\nliability strategy; in [62] using a performance moment inte-\ngration method to estimate the product quality loss; in [63]\nusing a preference aggregation method to produce a single-\nobjective RBO problem; and in [64] under epistemic un-\ncertainty. Both [65] and [66] used a genetic algorithm to\nsolve the problem. The dimension reduction method and its\nderivatives were introduced in [67, 68, 69] as an alternative\napproach to the RRBO problem.\n\nWith respect to RBTO, the approaches in [57], [30],\nand [70] are closest to ours. Still, random-field modeling was\nnot used for material property in [57] and [70]. Furthermore,\nseveral concerns can be identified from [70]. One of the most\nimportant stages in their method is the approximation of both\nfailure probability and its sensitivity, which were calculated\nby Monte Carlo sampling. Direct Monte Carlo sampling is\nwell-known to have variability [71], meaning two indepen-\ndent runs are very likely to get different values of failure\nprobability and its sensitivity which would obviously affect\nthe optimization results. Another concern is that the value\nof the parameter \u03b5 needed to replace the Heaviside func-\ntion with a smooth approximation [72] and was chosen by a\n\u201crecommendation\u201d backed by observation only. In [30], the\nmodeling random field was assumed with a known marginal\ndistribution, and in order to apply a perturbation technique,\nrandom variability of Young\u2019s modulus had to be small. Both\nof these assumptions clearly restrict the general applicability\nof their method. Lastly, robustness against uncertainty was\nnot studied in the three papers. As described in the following\nsections, our proposed method considers random field un-\ncertainty with the Karhunen\u2212\u2013Loe\u0300ve (KL) expansion used\nto reduce the dimension of the random field. The KL ex-\npansion covers a large class of random field without any re-\nstrictions on random variability. In this way, we are able to\nuse the FORM-based inverse reliability method within the\nSORA framework coupled with the stochastic response sur-\nface method to avoid the aforementioned weaknesses of di-\nrect Monte Carlo sampling.\n\n3 Topology Optimization under Uncertainty\n3.1 Deterministic Topology Optimization\n\nA standard notation is adopted throughout this\npaper\u2212bold upper and lower case letters denote matrices\nand vectors, respectively. The below formulation shows a\ndensity-based deterministic topology optimization (DTO):\n\nmin\n\u03c1\u03c1\u03c1\n\nC(\u03c1\u03c1\u03c1) = uT Ku\n\nsubject to K(\u03c1\u03c1\u03c1)u(\u03c1\u03c1\u03c1) = f,\nV (\u03c1\u03c1\u03c1)\n\nV0\n= \u03b3,\n\n0 < \u03c1min \u2264 \u03c1\u03c1\u03c1\u2264 1,\n\n(1)\n\nwhere \u03c1\u03c1\u03c1 is the vector of design variables of the TO prob-\nlem, which also are the deterministic finite-element densi-\nties; V (\u03c1\u03c1\u03c1) and V0 are the total and the initial volume of the\nfinite-element mesh, respectively; \u03b3 is the predetermined vol-\nume fraction; K(\u03c1\u03c1\u03c1), u(\u03c1\u03c1\u03c1), and f are the stiffness matrix, the\ndisplacement vector, and the external load vector, respec-\ntively; and C(\u03c1\u03c1\u03c1) is the structural compliance. In the opti-\nmization problem (1), there are three constraints: the first ex-\npresses the equilibrium of the structure; the second requires\nthe optimized design to have a prescribed volume; and the\nthird is a component-wise inequality, in which each density\n(design variable) must be between 1 and a lower limit (i.e.,\n\u03c1min = 0.001).\n\nTo ensure manufacturability, the optimized design must\nhave a well-defined boundary, which is not guaranteed if\nsolving (1) directly because there is nothing to prevent\nintermediate values of densities from dominating the de-\nsign. Hence, the Solid Isotropic Material with Penalization\n(SIMP) method [1] is used to make intermediate densities\nunfavorable compared to \u03c1min or 1. According to SIMP,\neach finite element has a Young\u2019s modulus Ei specified by\nEi = \u03c1\n\np\ni E\n\n0\ni , where p is the penalization factor and E\n\n0\ni is the\n\ninitial value of the Young\u2019s modulus corresponding to unit\ndensity. The interpretation and possible values of p were\nelaborated in [73]. Any established gradient-based algo-\nrithms can solve the problem (1) after it is converted into\na nonlinear optimization problem using the SIMP method.\nThis paper follows common practice in the literature, select-\ning the Method of Moving Asymptotes (MMA) [74, 75] as\nthe optimizer of the DTO problem. The MMA has proved its\nreliability and competitive performance in various settings\nof TO. However, SIMP alone is plagued with checkerboard-\ning, mesh dependence, and local minima [76]. Many mesh-\nindependent filtering methods [77] have been designed to\npreclude checkerboarding and mesh dependence, while lo-\ncal minima remain an open question. This paper uses the\ndensity filtering [78, 79] as implemented in [80]. The next\nsections describe how uncertainty shapes our problem for-\nmulation and the solution algorithm.\n\n3.2 Robust Reliability-based Topology Optimization\n3.2.1 Problem Formulation\n\nConsidering input uncertainty modeled by a random\nfield y(\u03c9,x), a robust reliability-based topology optimization\n(RRBTO) problem is formulated as follows:\n\nmin\n\u03c1\u03c1\u03c1\n\n\u03ba1\u00b5 [C(\u03c1\u03c1\u03c1,y)]+\u03ba2\u03c3 [C(\u03c1\u03c1\u03c1,y)]\n\ns.t. K(\u03c1\u03c1\u03c1,y)u(\u03c1\u03c1\u03c1,y) = f,\nV (\u03c1\u03c1\u03c1)\n\nV0\n= \u03b3,\n\nPi [gi(\u03c1\u03c1\u03c1,y)< 0]\u2264 P0i , i = 1,2 . . . ,m,\n0 < \u03c1min \u2264 \u03c1\u03c1\u03c1\u2264 1,\n\n(2)\n\nwhere x \u2208 D \u2282 Rd is coordinates of a point in a d-\ndimensional physical domain D; \u03c9 \u2208 \u2126 is an element of the\n\n3 Copyright \u00a9 by ASME\n\n\n\nsample space \u2126; \u00b5 [C(\u03c1\u03c1\u03c1,y)] and \u03c3 [C(\u03c1\u03c1\u03c1,y)] are the mean and\nstandard deviation of the compliance C(\u03c1\u03c1\u03c1,y), respectively;\n\u03ba1 and \u03ba2 are the real non-negative weighting factors. The\nlimit state function gi(\u03c1\u03c1\u03c1,y) is defined so that gi(\u03c1\u03c1\u03c1,y) < 0\nmeans failure of the design, and Pi[gi(\u03c1\u03c1\u03c1,y) < 0] shows the\nprobability of the ith failure event. The target probability P0i\nis the upper bound of the failure probability Pi and often de-\nfined as P0i = \u03a6(\u2212\u03b2i), where \u03b2i is the reliability index and\n\u03a6(\u00b7) is the standard normal cumulative distribution function.\nIn this paper the random field y(\u03c9,x) is taken to be the mate-\nrial Young\u2019s modulus, which must be physically meaningful\n(i.e., taking only positive values) and is modeled as in [17]:\n\nE(x) = F\u22121 \u25e6\u03a6 [y(\u03c9,x)] , (3)\n\nwhere \u03a6[\u00b7] is the standard normal cumulative distribution\nfunction (CDF), and F\u22121 is the inverse of a prescribed CDF.\nThe uniform distribution is chosen for the two numerical ex-\namples resulting in\n\nE(x) = a+(b\u2212a)\u03a6 [y(\u03c9,x)] , (4)\n\nwhere a and b are the two bounds of the distribution. The\nlog-normal and the beta distribution are also capable of mod-\neling non-negative, bounded physical quantities, which can\nsupersede the uniform distribution in (3) with minimal effort.\n\nA number of challenges need to be cleared before we\nare able to solve (2). The modeling random field, which tries\nto capture spatial variability of material property, needs to\nbe cast into an explicit, computable form because a defined\nvalue of material property is required to perform finite el-\nement analysis. The Karhunen\u2212\u2013Loe\u0300ve (KL) expansion in\nSection 3.2.3 is able to turn a random field into a series of\nrandom variables. The mean and standard deviation of the\ncompliance, and the probabilistic constraints are often very\nhard or expensive to evaluate because of complex geome-\ntry of their domains. A Smolyak-type sparse grid in Sec-\ntion 3.2.2 is an efficient method to calculate the mean and\nstandard deviation, while Inverse Reliability Analysis (IRA)\nand SORA in Section 3.2.4, coupled with the Stochastic Re-\nsponse Surface Method (SRSM) in Section 3.2.5, handle the\nprobabilistic constraints effectively by avoiding the demand-\ning double-loop problem. Combining the above methods, a\ndetailed description of our proposed algorithm is laid out in\nSection 3.2.6.\n\n3.2.2 Smolyak-type Sparse Grid\nIn practice, it is very difficult or even impossible to\n\ncalculate the mean and standard deviation of the compli-\nance in (2) analytically through multidimensional integrals.\nSuch difficulties have motivated the development of various\nnumerical methods such as simulation-based methods (i.e.,\nMonte Carlo (MC), important sampling, adaptive sampling,\netc.), and the stochastic collocation methods (SCM) [81].\nSimulation-based methods are usually more straightforward\nto implement and embarrassingly parallel, and their cost does\n\nnot depend on the number of dimensions; however, even\nwith better sampling techniques, they still require a lot more\nsampling points than the SCM. Depending on the smooth-\nness of the target function, the convergence rate of the SCM\ncan be orders of magnitude faster than MC-based methods\n[82, 83]. The SCM approximates the quantity of interest by\na weighted sum, whose weighing factors and terms are com-\nputed at specific collocation points. Locating such points is\none of the central topics in SCM. The popular approach is\nto pick a known one-dimensional quadrature rule and then\nbuild up the multidimensional grid from the one-dimension\nrule. Interested readers can find in [84] a list of popular\nquadrature rules. The Gauss-Hermite quadrature, which is\nparticularly suitable for approximating the mean of a normal\ndistribution, is selected in this paper. The multidimensional\ngrid can be constructed using a tensor product; nevertheless,\ndue to the well-known curse of dimensionality, the cost of\nSCM on a full tensor-product grid is still excessively high\nfor a large number of dimensions. The Smolyak-type sparse\ngrid (SSG), which may be traced back to the Smolyak algo-\nrithm [85], can significantly reduce the cost by using only a\nsubset of the full tensor grid. In [86] numerical experiments\nwith random input, whose dimension was up to 50, showed\nthat the SCM on sparse grids was more efficient than MC.\n\nFor the sake of completeness the SSG is reviewed here.\nThe construction presented below follows [87]. Consider a\nd-dimensional function f (x), the difference \u2206(1)l f is defined\nas\n\n\u2206\n(1)\nl f =\n\n(\nQ(1)l \u2212Q\n\n(1)\nl\u22121\n\n)\nf , (5)\n\nwhere\n\nQ(1)l f =\nn\n(1)\nl\n\n\u2211\ni=1\n\nf\n(\n\nx(i)l\n)\n\nw(i)l ,\n\nQ(1)0 f = 0,\n\n(6)\n\nand n(1)l is the number of nodes for the l-level quadrature\n\nformula. The collocation points x(i)l and the corresponding\nweights w(i)l are calculated using the Gauss-Hermite quadra-\nture rule, which is a natural choice for the class of integrals\ninvolving an exponential function over an infinite interval\nsuch as mean and standard deviation [88]. The sparse in-\ntegration formula at level l is expressed as\n\nQ(d)l f = \u2211\n|`|\u2264l+d\u22121\n\n(\n\u2206\n(1)\nl1\n\u2297\u2206(1)l2 \u2297\u00b7\u00b7 \u00b7\u2297\u2206\n\n(1)\nld\n\n)\nf , (7)\n\nwhere |`| = l1 + l2 + . . .+ ld . The mean and standard devia-\ntion of f (x) can be approximated using (7).\n\n3.2.3 Karhunen\u2212\u2013Loe\u0300ve Expansion\nIn a physical system, the quantity of interest can be mea-\n\nsured at spatial points over the system domain. If a random\n\n4 Copyright \u00a9 by ASME\n\n\n\nfield is considered an appropriate model for such a quan-\ntity, it is then required to construct the random field from\nmeasurements. Several methods, including the Expansion\nOptimal Linear Estimator [89] and polynomial chaos expan-\nsion [82, 8], have been adopted. Compared to others, the\nKarhunen\u2212\u2013Loe\u0300ve (KL) expansion [90] is \u201cthe most effi-\ncient in terms of the number of random variables required\nfor a given accuracy\u201d [7]. The KL expansion of a random\nfield y(\u03c9,x) is given as\n\ny(\u03c9,x) = E[x]+\n\u221e\n\n\u2211\ni=1\n\n\u221a\n\u03bbi\u03bei(\u03c9)ei(x) (8)\n\nwhere E[x] is the mean of the random field. The orthogo-\nnal eigenfunctions ei(x) and the corresponding eigenvalues\n\u03bbi are solutions of the following eigenvalue problem:\n\n\u222b\nD\n\nK(x1,x2)ei(x)dx = \u03bbiei(x) x,x1,x2 \u2208 D (9)\n\nwhere K(x1,x2) is the covariance function of the random\nfield\n\nK(x1,x2) = E [y(x1)y(x2)] x1,x2 \u2208 D (10)\n\nThe random variables \u03bei(\u03c9) are uncorrelated and satisfy:\n\nE[\u03bei] = 0,E[\u03bei\u03be j] = \u03b4i j,\n\n\u03bei(\u03c9) =\n1\n\u221a\n\n\u03bbi\n\n\u222b\nD\n(y(\u03c9,x)\u2212E[x])ei(x)dx,\n\n(11)\n\nwhere \u03b4i j is the Kronecker delta. The infinite series in (8) has\nto be truncated to use in practice. Because the influence of\nhigher order terms decays rapidly, satisfactory precision can\nbe achieved using only the first few terms of the expansion.\n\nThe KL expansion requires the solution of the eigen-\nvalue problem (9), which is pretty straightforward in the case\nof a random process (1-dimensional random field) [91]. For\nthe purpose of demonstration and without loss of generality,\nthis paper in Section 4 assumes the separability of the covari-\nance function of a 2-dimensional random field:\n\nK(s, t) = exp\n(\n\u2212|s1\u2212 t1|\n\nl1\n\u00d7\n\u2212|s2\u2212 t2|\n\nl2\n\n)\n= exp\n\n(\n\u2212\n|s1\u2212 t1|\n\nl1\n\n)\nexp\n(\n\u2212\n|s2\u2212 t2|\n\nl2\n\n)\n,\n\ns, t \u2208 D\u2282R2,\n\n(12)\n\nwhere l1 and l2 are the correlation lengths in the two coordi-\nnate directions. The separability of the covariance function\nleads to separable eigenvalues and eigenfunctions, which are\nthe product of their univariate counterparts [92].\n\n3.2.4 Inverse Reliability Analysis and SORA\nThe probabilistic constraints formulated as in (2) are\n\ncalled the reliability index approach (RIA); however, as [11]\nreported, the performance measure approach (PMA) pro-\nvides better numerical stability and higher rate of conver-\ngence. Using the PMA, (2) is transformed as follows:\n\nmin\n\u03c1\u03c1\u03c1\n\n\u03ba1\u00b5 [C(\u03c1\u03c1\u03c1,y)]+\u03ba2\u03c3 [C(\u03c1\u03c1\u03c1,y)]\n\ns.t. K(\u03c1\u03c1\u03c1,y)u(\u03c1\u03c1\u03c1,y) = f,\nV (\u03c1\u03c1\u03c1)\n\nV0\n= \u03b3,\n\ngi(\u03c1\u03c1\u03c1,y)\u2265 0, i = 1,2 . . . ,m,\n0 < \u03c1min \u2264 \u03c1\u03c1\u03c1\u2264 1.\n\n(13)\n\nThe most notable change is that the probabilistic constraints\nare replaced by inequalities of the limit state functions. Solv-\ning the new problem requires a truncated KL expansion\ny(\u03c9,x)\u2248 y(\u03be\u03be\u03be(\u03c9),x), FORM, and inverse reliability analysis\n(IRA). In order to apply FORM, the random vector \u039e\u039e\u039e = {\u03bei}\nis first transformed into a vector of standard normal random\nvariables \u03a8\u03a8\u03a8 = {\u03c8i} using the Rosenblatt or the Nataf trans-\nformation \u03a8\u03a8\u03a8 = T (\u039e\u039e\u039e) or \u039e\u039e\u039e = T\u22121(\u03a8\u03a8\u03a8). Then, the most prob-\nable point (MPP) \u03be\u03be\u03be\u2217i in physical space or \u03c8\u03c8\u03c8\n\n\u2217\ni in transformed\n\nspace is obtained by solving the following IRA problem:\n\nmin\n\u03c8\u03c8\u03c8\n\ngi(\u03c8\u03c8\u03c8)\n\ns.t. \u2016 \u03c8\u03c8\u03c8 \u2016= \u03b2i,\n(14)\n\nwhere gi(\u03c8\u03c8\u03c8) is the ith limit state function in transformed\nspace. In this paper, the Matlab CODES toolbox [93] is\nchosen to solve (14). Furthermore, the SORA framework\nis adopted to decouple the double-loop structure of (13).\nIn SORA, instead of nesting the optimization problem (14)\nwithin (13), it serializes (13) into a chain of loops of DTO\nand IRA (Fig. 1). Each kth loop starts with DTO followed by\nIRA:\n\nmin\n\u03c1\u03c1\u03c1k\n\n\u03ba1\u00b5\n[\nC(\u03c1\u03c1\u03c1k,y)\n\n]\n+\u03ba2\u03c3\n\n[\nC(\u03c1\u03c1\u03c1k,y)\n\n]\n\ns.t. K(\u03c1\u03c1\u03c1k,y)u(\u03c1\u03c1\u03c1k,y) = f,\n\nV (\u03c1\u03c1\u03c1k)\nV0\n\n= \u03b3,\n\ngi\n(\n\n\u03c1\u03c1\u03c1\nk,y(\u03be\u03be\u03be\u2217(k\u22121)i ,x)\n\n)\n\u2265 0, i = 1,2 . . . ,m,\n\n0 < \u03c1min \u2264 \u03c1\u03c1\u03c1k \u2264 1.\n\n(15)\n\nwhere \u03be\u03be\u03be\u2217(k\u22121)i denotes the MPP in physical space of i\nth limit\n\nstate function in the (k\u22121)th loop. Solving (15) gives \u03c1\u03c1\u03c1\u2217(k),\nwhich is substituted into (14) to find the next MPP \u03be\u03be\u03be\u2217(k)i in\n\n5 Copyright \u00a9 by ASME\n\n\n\nthe form of \u03c8\u03c8\u03c8\u2217(k)i :\n\nmin\n\u03c8\u03c8\u03c8\n\ngi(\u03c1\u03c1\u03c1\n\u2217(k),\u03c8\u03c8\u03c8)\n\ns.t. \u2016 \u03c8\u03c8\u03c8 \u2016= \u03b2i.\n(16)\n\nStart\n\nInitialization:\n- Initial values\n- KL expansion\n- Collocation and sample points\n\nFinite Element Analysis\n\nSensitivity and filtering\n\nUpdate densities\n\nConverge\nNo\n\nSIMP\nDTO\n\nFinite Element Analysis\n\nConstruct limit state \nfunction\n\nFind MPP\n\nConverge\n\nInverse \nReliability \nAnalysis\n\nSRSM\n\nYes\n\nNo\n\nEnd\n\nYes\n\nUpdate Young\u2019s \nmodulus using \n\nMPP\n\nCalculate robust objective and \nits derivarives using the SCM\n\nFig. 1. SORA-based RRBTO flowchart [57].\n\nThe SORA framework coupled with the PMA can save\ncomputational cost significantly by reducing the number of\nreliability analyses performed to reach convergence of both\nDTO and IRA. However, as a heuristic method, the optimiza-\ntion solution \u03c1\u03c1\u03c1\u2217 acquired by SORA may not be the one found\nin the double-loop problem, which is corrected in each loop\nby shifting the random design variables using information\nfrom the previous loop [94]. Such modification is not neces-\nsary in this paper because there are only deterministic design\nvariables (only the material property is random).\n\n3.2.5 Stochastic Response Surface Method\nAs uncertainty is propagated from random input to out-\n\nput through complex computations such as finite element\nanalysis, it is almost unlikely to derive the output directly\nas an explicit expression of input. However, such an expres-\nsion is needed to run the gradient-based algorithm in IRA.\nThe SRSM deals with this difficulty by approximating the\noutput by a polynomial chaos expansion [82]. The below\nformulations follows [95]. The multidimensional Hermite\npolynomials of degree p are used in the SRSM and defined\nas:\n\nHp(\u03b1i1 ,\u03b1i2 , . . . ,\u03b1ip)\n\n= (\u22121)pe\n1\n2 \u03b1\u03b1\u03b1\n\nT\n\u03b1\u03b1\u03b1\n\n\u2202\np\n\n\u2202\u03b1i1 ,\u2202\u03b1i2 , . . . ,\u2202\u03b1ip\ne\u2212\n\n1\n2 \u03b1\u03b1\u03b1\n\nT\n\u03b1\u03b1\u03b1\n\n(17)\n\nwhere \u03b1\u03b1\u03b1 = {\u03b1ik}\np\nk=1 is a vector of standard normal random\n\nvariables. The output of interest z is estimated as follows:\n\nz = a0 +\nn\n\n\u2211\ni1=1\n\nai1H1(\u03b1i1)+\nn\n\n\u2211\ni1=1\n\ni1\n\n\u2211\ni2=1\n\nai1i2H2(\u03b1i1 ,\u03b1i2)\n\n+\nn\n\n\u2211\ni1=1\n\ni1\n\n\u2211\ni2=1\n\ni2\n\n\u2211\ni3=1\n\nai1i2i3H3(\u03b1i1 ,\u03b1i2 ,\u03b1i3)+ . . .\n\n(18)\n\nwhere n is the number of standard normal random vari-\nables used in the expansion, and a0,ai1 ,ai1i2 ,ai1i2i3 , . . . are\nunknown coefficients. If n = 2 and p = 3, then the expan-\nsion (18) will become:\n\nz(\u03b1i1 ,\u03b1i2) = a0 +a1\u03b1i1 +a2\u03b1i2 +a3(\u03b1\n2\ni1 \u22121)+a4(\u03b1\n\n2\ni2 \u22121)\n\n+a5\u03b1i1\u03b1i2 +a6(\u03b1\n3\ni1 \u22123\u03b1i1)+a7(\u03b1\n\n3\ni2 \u22123\u03b1i2)\n\n+a8(\u03b1i1\u03b1\n2\ni2 \u2212\u03b1i1)+a9(\u03b1\n\n2\ni1\u03b1i2 \u2212\u03b1i2)\n\n= a0 +\n9\n\n\u2211\nk=1\n\nakhik\n\n(19)\nwhere 1,hi1 ,hi2 , . . . ,hi9 are Hermite polynomials. The ten\nunknown coefficients a0,a1, . . . ,a9 are found by solving a\nsystem of linear equations using at least ten different realiza-\ntions of (\u03b1i1 ,\u03b1i2). Such realizations can be chosen at colloca-\ntion points according to the SSG in Section 3.2.2, or a much\nsimpler heuristic rule in [96], which is selected in this paper.\nFor the approximation in (19) the rule generates 17 colloca-\ntion points to form a stochastic response surface, which was\nvery close to the target output [95].\n\n3.2.6 Solution Algorithm\nThe optimization algorithm (Fig. 1) to solve the RRBTO\n\nproblem (2) is expounded below:\n\n1. Initialize the problem: size of the finite element mesh;\ninitial values of design variables, SIMP and optimization\nparameters; KL expansion of random field; collocation\npoints and weights for the SSG, and the SRSM; etc.\n\n6 Copyright \u00a9 by ASME\n\n\n\n2. Until convergence do:\n\n\u2022 Solve Kiui = fi and compute\n\u2202Ci(\u03c1\u03c1\u03c1)\n\n\u2202\u03c1\u03c1\u03c1\nfor i =\n\n1,2, . . . ,n(d)l .\n\u2022 Calculate mean, variance and their derivatives:\n\nE [C] =\nn\n(d)\nl\n\n\u2211\ni=1\n\nwiCi,\n\n\u03c3\n2 [C] =\n\nn\n(d)\nl\n\n\u2211\ni=1\n\nwiC\n2\ni \u2212E\n\n2 [C] ,\n\n\u2202E [C]\n\u2202\u03c1\u03c1\u03c1\n\n=\n\nn\n(d)\nl\n\n\u2211\ni=1\n\nwi\n\u2202Ci(\u03c1\u03c1\u03c1)\n\n\u2202\u03c1\u03c1\u03c1\n,\n\n\u2202\u03c3\n2 [C]\n\u2202\u03c1\u03c1\u03c1\n\n=\n\nn\n(d)\nl\n\n\u2211\ni=1\n\n2Ci(\u03c1\u03c1\u03c1)wi\n\u2202Ci(\u03c1\u03c1\u03c1)\n\n\u2202\u03c1\u03c1\u03c1\n\u22122E [C]\n\n\u2202E [C]\n\u2202\u03c1\u03c1\u03c1\n\n.\n\n(20)\n\n\u2022 Compute derivative of the robust objective:\n\n\u03ba1\n\u2202E [C]\n\n\u2202\u03c1\u03c1\u03c1\n+\u03ba2\n\n1\n\n2\n\u221a\n\n\u03c32[C]\n\n\u2202\u03c3\n2 [C]\n\u2202\u03c1\u03c1\u03c1\n\n. (21)\n\n\u2022 Deterministic topology optimization (DTO): the most\nprobable point (MPP) \u03be\u03be\u03be\u2217(k\u22121)i found in the previous loop\n(or some initial values for the first loop) is used in place\nof random parameters \u03be\u03be\u03be(\u03c9), making (15) a regular TO\nproblem. The SIMP and the MMA method are em-\nployed to solve it.\n\n\u2022 Inverse reliability analysis (IRA): the optimum values\nof design variables from the DTO step and the colloca-\ntion points given in Section 3.2.5 are used to construct\nstochastic response surfaces, which in turn are utilized\nin (16) to find the next MPP. Based on convergence con-\nditions, the algorithm may stop, or a new loop is re-\nquested with updated Young\u2019s modulus using the new\nMPP.\n\n4 Results\nIn this section our proposed algorithm is run on two\n\ncommon benchmark problems (the cantilever and the L-\nshaped beam) with three target reliability levels, six weight-\ning factors, and one parameter tuple of the uniform distribu-\ntion in (4). The correctness and accuracy of our algorithm\nis then verified on the optimization results by Monte Carlo\nsimulations. All quantities given below are dimensionless\nfor simplicity.\n\nTo ensure the generality of our approach, we intention-\nally do not specify the limit state functions gi(\u03c1\u03c1\u03c1,y) in the\nprevious sections, which is essential for the two numerical\n\nexamples. The RRBTO problem now becomes:\n\nmin\n\u03c1\u03c1\u03c1\n\n\u03ba1\u00b5 [C(\u03c1\u03c1\u03c1,y)]+\u03ba2\u03c3 [C(\u03c1\u03c1\u03c1,y)]\n\ns.t. K(\u03c1\u03c1\u03c1,y)u(\u03c1\u03c1\u03c1,y) = f,\nV (\u03c1\u03c1\u03c1)\n\nV0\n= \u03b3,\n\nP [u(\u03c1\u03c1\u03c1,y)\u2212u0 < 0]\u2264 P0,\n0 < \u03c1min \u2264 \u03c1\u03c1\u03c1\u2264 1,\n\n(22)\n\nwhere u(\u03c1\u03c1\u03c1,y) and u0 are the actual displacement and the\nminimum allowable displacement at a selected point, respec-\ntively. The above formulation is inspired by design of com-\npliant mechanisms [97], in which both flexibility and stiff-\nness are required. Flexibility allows the mechanisms to reach\ndesigned deformation, implied by the limit state function\ng(\u03c1\u03c1\u03c1,y) = u(\u03c1\u03c1\u03c1,y)\u2212 u0, while maximizing stiffness, or min-\nimizing compliance, helps them withstand loads. Further-\nmore, the weighting factors \u03ba1 and \u03ba2 need substantial at-\ntention. Our preliminary numerical results showed that the\nmean and standard deviation of the compliance are differ-\nent by about two orders of magnitude, which may have crit-\nical impact on the solution. This was examined carefully\nin [98] and normalization transforming them into the same\nscale has been recommended. \u03ba1 and \u03ba2 are identified ac-\ncording to [21]:\n\n\u03ba1 =\n\u03b5\n\n\u00b5\u2217\n,\u03ba2 =\n\n1\u2212 \u03b5\n\u03c3\u2217\n\n, (23)\n\nwhere \u03b5 \u2208 [0,1], \u00b5\u2217 is the mean of the compliance when\n(\u03b5,1\u2212 \u03b5) = (0,1), and \u03c3\u2217 is the standard deviation of the\ncompliance when (\u03b5,1\u2212\u03b5)= (1,0). \u00b5\u2217 and \u03c3\u2217 have to be cal-\nculated on the same set of input parameters except \u03b5, under\nwhich they are the maximum values of the mean and stan-\ndard deviation of the compliance resulting in 0 <\n\n\u00b5\n\u00b5\u2217\n\n,\n\u03c3\n\n\u03c3\u2217\n\u2264 1\n\nin (22).\nCoding the algorithm demands concrete values of ev-\n\nery parameter, many of which are shared between the two\nexamples. The finite element mesh in both examples is as-\nsembled from square, linear, plane stress elements, whose\nside length and thickness are unit dimension. Those ele-\nments are made of an isotropic, linear elastic material with\nPoisson\u2019s ratio \u03bd = 0.3. The material Young\u2019s modulus\nis assumed to be a centered, mean-square Gaussian ran-\ndom field with known covariance function as in (12), ex-\npanded into a series of independent, standard normal ran-\ndom variables [99]. The correlation lengths are chosen as\nl1 = l2 = 0.6, and only the first two eigenvalues and eigen-\nfunctions are picked for the truncated KL expansion. A wide\nrange of optimization algorithms for reliability analysis is\navailable in the CODES toolbox [93], including the Hybrid\nMean Value method selected for the examples due to its ef-\nficiency [100]. Other parameters of the optimization prob-\nlem (22) are the target reliability levels \u03b2 = {1.0,2.0,3.0},\n\n7 Copyright \u00a9 by ASME\n\n\n\nweighting factors \u03b5 = {1,0.9,0.8,0.5,0.2,0}, and the ma-\nterial property limits (a,b) = (1,1.5). Due to iterative na-\nture of MMA and SORA, several convergence criteria are\nenforced. The MMA optimizer stops when the maximum\ndifference of design variables of two consecutive iterations\nis smaller than a prescribed value (dMMAmax \u2264 0.001), or the\nnumber of iterations is more than nMMA = 200. The sim-\nilar conditions are applied in the SORA loops, but for the\nMPPs (dMPPmax \u2264 0.001) and a different maximum allowable\nnumber of iterations (nSORA = 20). The level of sparse grid\napproximation is 4 using Gauss-Hermite quadrature as the\nbase one-dimensional rule. In SIMP, the minimum length\nscale rmin = 1.5 and penalization factor p = 3 are used. Each\nexample is then validated by 50000 Monte Carlo simulations\n(MCS), which compute failure probabilities of the limit state\nfunction, and the mean and standard deviation of the compli-\nance. The examples are implemented in Matlab using Latin\nhypercube sampling for MCS with seed 0. Those probabil-\nities are compared with values calculated from the three re-\nliability levels, while statistical moments of the compliance\nprove robustness of the optimization results against uncer-\ntainty.\n\n4.1 The Cantilever Beam\n\n/ \n\nA B A B\n\nFig. 2. The cantilever beam\n\nFig. 2 displays the cantilever beam, a two-dimensional\ndomain used in this example. The beam is fixed on its left\nside, meshed into 60 \u00d7 20 elements, and subject to two unit\nvertical loads, which are placed on its bottom edge at equal\ndistances (points A and B). The optimization problem is set\nup as in (22), in which the minimum allowable displacement\nat the load application point B is given as u0 = 220. Then our\nproposed algorithm is tested on the example with different\nvalues of target reliability, material parameters and weight-\ning factors. The 18 optimized designs are presented in Tables\n1 and 2. The MCS and SRSM utilize those designs to calcu-\nlate the mean and standard deviation of vertical displacement\nof point B (\u00b5B and \u03c3B in Table 3), as well as the probabili-\nties of failure event Pf = P [g(\u03c1\u03c1\u03c1,y)< 0] to show the reliabil-\nity levels achieved by the designs. Moreover, the mean and\nstandard deviation of the compliance are also obtained from\nthe MCS (\u00b5[C] and \u03c3[C] in Table 3) to examine how they\nvary with respect to the weighting factors \u03b5. All of theses\nvalues are gathered in Table 3, whose second column shows\n\nexpected failure probabilities Pf = \u03a6(\u2212\u03b2). Some conclu-\nsions from this example results can be found in Section 5.\n\n4.2 The L-shaped Beam\n\n/ \n\nA B A B\n\nFig. 3. The L-shaped beam\n\nThe second numerical example to illustrate our proposed\nalgorithm is the L-shaped beam as shown in Fig. 3. The\nbeam is fixed on its topmost edge and subject to two unit\nvertical loads on its bottom edge\u2212one at the right endpoint\nB and the other at point A located one quarter of the bot-\ntom edge length from point B. As in the previous example,\nthe weighted sum of the two statistical moments of the com-\npliance is minimized, while a probabilistic constraint is im-\nposed on the vertical displacement of one load application\npoint (point B in Fig. 3) to design for flexibility. The mini-\nmum allowable vertical displacement of point B is chosen as\nu0 = 130. The design domain is discretized using a 60\u00d760\nmesh of finite elements, and then one quarter of the mesh is\nremoved to make the domain L-shaped by forcing the ele-\nment densities in this region to be 0.001 before proceeding\nto the next operation. The larger mesh used in this exam-\nples results in much longer running time based on our ex-\nperiments on the same computer. The results also exhibit the\nsame trends as in the previous example. Therefore, instead of\nrunning the complete set of 18 combinations, only 12 cases,\nwhich are the combinations of \u03b2 = {1,3}, six weighting fac-\ntors, and (a,b) = (1,1.5), are considered. Table 6 shows the\nprobabilities of displacement constraint violation Pf at point\nB, the statistical moments of that point\u2019s vertical displace-\nment (\u00b5B and \u03c3B) calculated from both the MCS and SRSM,\nand the mean and standard deviation of the compliance (\u00b5[C]\nand \u03c3[C]). The L-shaped optimized designs are analyzed for\ninsights in the next section.\n\n5 Discussions\nIn this section we closely scrutinize the two numerical\n\nexamples for comparison, verification, and insights.\nVisual inspection and analysis of topology optimized de-\n\nsigns (i.e., identifying and comparing their differences) is\n\n8 Copyright \u00a9 by ASME\n\n\n\nTa\nbl\n\ne\n1.\n\nT\nhe\n\nca\nnt\n\nile\nve\n\nr\nbe\n\nam\n:\n\nR\nR\n\nB\nT\n\nO\nre\n\nsu\nlts\n\n(\u03b5\n,1\n\u2212\n\n\u03b5\n)\n\n(1\n,0\n)\n\n(0\n.9\n,0\n.1\n)\n\n(0\n.8\n,0\n.2\n)\n\n\u03b2\n=\n\n1\n\n\u03b2\n=\n\n2\n\n\u03b2\n=\n\n3\n\nTa\nbl\n\ne\n2.\n\nT\nhe\n\nca\nnt\n\nile\nve\n\nr\nbe\n\nam\n:\n\nR\nR\n\nB\nT\n\nO\nre\n\nsu\nlts\n\n(\u03b5\n,1\n\u2212\n\n\u03b5\n)\n\n(0\n.5\n,0\n.5\n)\n\n(0\n.2\n,0\n.8\n)\n\n(0\n,1\n)\n\n\u03b2\n=\n\n1\n\n\u03b2\n=\n\n2\n\n\u03b2\n=\n\n3\n\n9 Copyright \u00a9 by ASME\n\n\n\nTable 3. The cantilever beam: Numerical results\n\nMCS SRSM\n\n\u03b2 Expected Pf (\u03b5,1\u2212 \u03b5) \u00b5[C] \u03c3[C] \u00b5B \u03c3B Pf \u00b5B \u03c3B\n\n1 0.15865\n\n(1,0) 162.9505 0.9263 -220.6120 0.6071 0.15674 -220.6120 0.6071\n\n(0.9,0.1) 162.9992 0.9188 -220.6070 0.6017 0.15642 -220.6070 0.6017\n\n(0.8,0.2) 163.2204 0.8953 -220.5980 0.5853 0.15326 -220.5980 0.5853\n\n(0.5,0.5) 166.5160 0.9032 -225.0110 0.5906 0.00000 -225.0110 0.5906\n\n(0.2,0.8) 177.7632 0.8588 -240.6990 0.5572 0.00000 -240.6990 0.5572\n\n(0,1) 206.2317 0.8559 -278.9100 0.5527 0.00000 -278.9100 0.5527\n\n2 0.02275\n\n(1,0) 163.4230 0.9150 -221.1960 0.5995 0.02252 -221.1960 0.5995\n\n(0.9,0.1) 163.6040 0.9106 -221.1880 0.5961 0.02250 -221.1880 0.5961\n\n(0.8,0.2) 163.7847 0.9118 -221.1890 0.5967 0.02266 -221.1890 0.5967\n\n(0.5,0.5) 164.7008 0.8953 -222.9560 0.5866 0.00000 -222.9560 0.5866\n\n(0.2,0.8) 178.0176 0.8586 -241.0670 0.5570 0.00000 -241.0670 0.5570\n\n(0,1) 206.2320 0.8559 -278.9110 0.5527 0.00000 -278.9110 0.5527\n\n3 0.001349\n\n(1,0) 163.7490 0.9094 -221.7690 0.5962 0.001340 -221.7690 0.5962\n\n(0.9,0.1) 164.0019 0.9078 -221.7630 0.5944 0.001340 -221.7630 0.5944\n\n(0.8,0.2) 164.0458 0.9015 -221.7520 0.5907 0.001320 -221.7520 0.5907\n\n(0.5,0.5) 165.0556 0.8938 -223.2560 0.5855 0.000000 -223.2560 0.5855\n\n(0.2,0.8) 177.8977 0.8585 -240.8960 0.5570 0.000000 -240.8960 0.5570\n\n(0,1) 206.2320 0.8559 -278.9110 0.5527 0.000000 -278.9110 0.5527\n\nlargely an untouched topic in TO research, which, in our\nopinion, is curious because topology is all about geometry\nand \u201cappearance\u201d of structures. Without such tools, the best\neffort is to compare those results qualitatively. To make their\ndifferences more pronounced, readers may render them into\nshort animations, which would reveal much more than hu-\nman eyes can perceive using only static images. There would\nbe subtle material re-distributions (i.e., among results under\nthe same tuple (\u03b5,1\u2212\u03b5)), as well as thickening or thinning of\ncertain features, which could be almost undetectable by com-\nparing static images. The removal or addition of features is\neasier to spot among results. The most distinct results corre-\nspond to \u03b5 = 1 and \u03b5 = 0, which is understandable because\nthey are the extreme cases. The results between them are sort\nof transitions from one bounding value to the other.\n\nA number of trends can be observed from both the opti-\nmized designs and their corresponding numerical results:\n\n1. The MCS-based Pf is always smaller than the expected\nPf , which confirms that the designs have achieved the\ndesired reliability level.\n\n2. Decreasing \u03b5, or increasing the weight on standard de-\nviation in the robust objective, makes the MCS-based\nPf smaller until reaching 0. We hypothesize that there\nare two classes of solutions depending on the weight:\none on the constraint boundary and the other inside the\nfeasible region of the optimization problem. In the first\nclass, the MCS-based Pf is close to the expected Pf :\ndecreasing rate of the MCS-based Pf is very slow for\ncertain range of \u03b5 (i.e., \u03b5 = {1,0.9,0.8} in Table 3, and\n\u03b5 = {1,0.9} in Table 6). In the second class, the so-\n\n10 Copyright \u00a9 by ASME\n\n\n\nTable 4. The L-shaped beam: RRBTO results\n\n(\u03b5,1\u2212 \u03b5) (1,0) (0.9,0.1) (0.8,0.2)\n\n\u03b2 = 1\n\n\u03b2 = 3\n\nTable 5. The L-shaped beam: RRBTO results\n\n(\u03b5,1\u2212 \u03b5) (0.5,0.5) (0.2,0.8) (0,1)\n\n\u03b2 = 1\n\n\u03b2 = 3\n\n11 Copyright \u00a9 by ASME\n\n\n\nTable 6. The L-shaped beam: Numerical results\n\nMCS SRSM\n\n\u03b2 Expected Pf (\u03b5,1\u2212 \u03b5) \u00b5[C] \u03c3[C] \u00b5B \u03c3B Pf \u00b5B \u03c3B\n\n1 0.15865\n\n(1,0) 96.4893 0.5352 -130.3580 0.3577 0.15808 -130.3580 0.3577\n\n(0.9,0.1) 96.3536 0.5321 -130.3560 0.3553 0.15790 -130.3560 0.3553\n\n(0.8,0.2) 96.8678 0.5241 -131.0290 0.3498 0.00144 -131.0290 0.3498\n\n(0.5,0.5) 99.5821 0.5045 -135.7740 0.3396 0.00000 -135.7740 0.3396\n\n(0.2,0.8) 108.7352 0.4917 -149.3090 0.3332 0.00000 -149.3090 0.3332\n\n(0,1) 133.3052 0.4762 -183.8160 0.3238 0.00000 -183.8160 0.3238\n\n3 0.001349\n\n(1,0) 96.8866 0.5300 -131.0510 0.3544 0.00130 -131.0510 0.3544\n\n(0.9,0.1) 96.7519 0.5298 -131.0510 0.3545 0.00126 -131.0510 0.3545\n\n(0.8,0.2) 96.8852 0.5240 -131.0560 0.3497 0.00106 -131.0560 0.3497\n\n(0.5,0.5) 99.6087 0.5049 -135.9450 0.3399 0.00000 -135.9450 0.3399\n\n(0.2,0.8) 108.7883 0.4916 -149.3880 0.3331 0.00000 -149.3880 0.3331\n\n(0,1) 133.9731 0.4755 -184.5450 0.3229 0.00000 -184.5450 0.3229\n\nlutions are away from the constraint boundary but still\ninside the feasible region: the decreasing rate acceler-\nates and the MCS-based Pf eventually becomes 0 (i.e.,\n\u03b5\u2265 0.5 in Table 3, and \u03b5\u2265 0.8 in Table 6).\n\n3. From our understanding of robust optimization, increas-\ning the weight on standard deviation in a minimization\nproblem will decrease its value and have the opposite ef-\nfect on the mean, which is actually the case as observed\nin Tables 3 and 6. However, there are a couple of out-\nliers to this trend (i.e., (\u03b5,\u03b2) = (0.8,2) in Table 3, and\n\u03b5 = 0.9 in Table 6). An explanation can be made by in-\nvestigating where the solution converges in the design\nspace. When the solutions are away from the constraint\nboundary, the robust objective is more dominant in the\nsolution: a marked increase and decrease of the mean\nand standard deviation, respectively, is observed. In so-\nlutions at the constraint limit, the robust objective has\nless in the solution.\nThis trend explains the decreasing tendency of the MCS-\nbased Pf . Increasing the weight on standard deviation\nleads to decreased influence of the mean compliance,\nwhose major proportion is contributed by the displace-\nment of point B (Fig. 2 and 3) which is constrained prob-\nabilistically in (22) to be larger than the minimum al-\nlowable value u0. This results in increasing the mean of\npoint B displacement, whose changing rate is slow for\nsolutions close to constraint boundary and much more\n\nrapid in the remaining cases (i.e., the sixth column of\nTables 3 and 6). The bigger the displacement of point B\nbecomes, the smaller the MCS-based Pf is. It is ob-\nvious that under a specific set of inputs (i.e., loading\nand boundary conditions, material property) mechani-\ncal capabilities of a structure (i.e., stress, strain, dis-\nplacement), even if heavily optimized, are always finite.\nThus, the MCS-based Pf approaches 0 when the dis-\nplacement of point B continues to increase.\n\n4. The Smolyak-type sparse grid and the SRSM are both\nvery good methods for their respective approximating\ntargets. As shown in Tables 3 and 6, approximately\nsix significant digits are required to see the differences\nbetween the MCS-based and the SRSM results. The\ncumulative distribution functions of point B displace-\nment from the two methods are also almost identi-\ncal, so they are not included in the paper. This also\nmakes us confident in choosing only two terms in the\nKL expansion\u2212adding more terms would only increase\ncomputational cost without clear benefits. The same ob-\nservation is applied to the Smolyak-type sparse grid and\nMCS-based results of the mean and standard deviation\nof the compliance. Because of such agreement, we de-\ncide to use only one level of the sparse grid. Multiple\nlevels were tested in [17, 27].\n\n12 Copyright \u00a9 by ASME\n\n\n\n6 Conclusions\nIn this paper, we have presented an efficient robust\n\nreliability-based design approach for topology optimized de-\nsigns, considering random field uncertainty in material prop-\nerty. Our approach avoids a double-loop approach to reliabil-\nity evaluation, and utilizes a number of techniques to reduce\ncomputational cost without sacrificing solution accuracy.\nThe two case studies have demonstrated the methodology,\nand have shown the impact of using the robust reliability-\nbased design approach. As shown in the case studies, the\ntopology can converge to a (local) solution where either the\nreliability-based constraint or the robust objective has more\ninfluence upon the resulting topology. This difference in\nconverged solution is determined by the relative weight of\nthe mean versus the variance in the robust objective. This\nfinding is significant to designers because it shows that us-\ning either exclusively a robust design approach or reliability-\nbased design approach will not identify the optimal topology\nconsidering material property uncertainty. Only the robust\nreliability-based design approach is capable of identifying\nthe optimal topology considering the influence of material\nproperty uncertainty in both the objective and the constraint.\n\nExplicitly considering material property uncertainty will\nbe significant in design for additive manufacturing, where\nthe additive process can lead to significant material property\nuncertainty due to temperature gradients or other sources of\nvariation. In terms of future work, the following are rec-\nommended. Further case studies should be considered to\nbetter understand the interplay between the reliability con-\nstraint and the robust objective. Different objectives and con-\nstraints can be considered as well. Considering topology op-\ntimization as a tool for design for additive manufacturing,\nanother significant characteristic of the material property is\nnon-linearity. Polymeric materials used in additive manu-\nfacturing will be better modeled as hyper-elastic or visco-\nelastic, as opposed to linear elastic. Another need is visu-\nalization of the results. As noted in the text, it can be dif-\nficult to understand the differences in topology based upon\nvisual inspection. A more systematic comparison of differ-\nent converged results will help researchers better understand\nthe differences in designs resulting from different problem\nformulations, and will lead to a better understanding of the\ndesign principles leading to robust, reliable designs.\n\nAcknowledgements\nThe first author would like to thank the Vietnam Edu-\n\ncation Foundation (VEF) for their financial support through\nthe VEF fellowship, and Prof. Krister Svanberg for his assis-\ntance on the MMA code.\n\nReferences\n[1] Bends\u00f8e, M. P., and Sigmund, O., 2003. Topol-\n\nogy optimization: theory, methods, and applications.\nSpringer, Berlin ; New York.\n\n[2] American Concrete Institute. Building Code Require-\nments for Structural Concrete (ACI 318-14).\n\n[3] American Institute of Steel Construction. Specifica-\ntion for Structural Steel Buildings (ANSI/AISC 360-\n16).\n\n[4] Sriramula, S., and Chryssanthopoulos, M. K., 2013.\n\u201cAn experimental characterisation of spatial variabil-\nity in GFRP composite panels\u201d. Structural Safety, 42,\npp. 1\u201311.\n\n[5] Nader, J. W., Dagher, H. J., Lopez-Anido, R., El Chiti,\nF., Fayad, G. N., and Thomson, L., 2008. \u201cProba-\nbilistic Finite Element Analysis of Modified ASTM\nD3039 Tension Test for Marine Grade Polymer Ma-\ntrix Composites\u201d. Journal of Reinforced Plastics and\nComposites, 27(6), pp. 583\u2013597.\n\n[6] Sriramula, S., and Chryssanthopoulos, M. K., 2009.\n\u201cQuantification of uncertainty modelling in stochastic\nanalysis of FRP composites\u201d. Composites Part A: Ap-\nplied Science and Manufacturing, 40(11), pp. 1673\u2013\n1684.\n\n[7] Sudret, B., and Kiureghian, A. D., 2000. Stochastic\nFinite Element Methods and Reliability: A State-of-\nthe-Art Report (Report No. UCB/SEMM-2000/08).\nDepartment of Civil and Environmental Engineering,\nUniversity of California, Berkeley.\n\n[8] Ghanem, R., and Spanos, P., 2003. Stochastic Finite\nElements: A Spectral Approach. Civil, Mechanical\nand Other Engineering Series. Dover Publications.\n\n[9] Charmpis, D., Schue\u0308ller, G., and Pellissetti, M., 2007.\n\u201cThe need for linking micromechanics of materials\nwith stochastic finite elements: A challenge for ma-\nterials science\u201d. Computational Materials Science,\n41(1), pp. 27\u201337.\n\n[10] Du, X., and Chen, W., 2004. \u201cSequential Optimiza-\ntion and Reliability Assessment Method for Efficient\nProbabilistic Design\u201d. Journal of Mechanical Design,\n126(2), p. 225.\n\n[11] Tu, J., Choi, K. K., and Park, Y. H., 1999. \u201cA\nNew Study on Reliability-Based Design Optimiza-\ntion\u201d. Journal of Mechanical Design, 121(4), p. 557.\n\n[12] Michell, A., 1904. \u201cThe limits of economy of ma-\nterial in frame-structures\u201d. The London, Edinburgh,\nand Dublin Philosophical Magazine and Journal of\nScience, 8(47), pp. 589\u2013597.\n\n[13] Taguchi, G., 1986. Introduction to quality engineer-\ning: designing quality into products and processes.\nDistributed by the American Supplier Institute, Inc.,\nDearborn, MI.\n\n[14] Schevenels, M., Lazarov, B., and Sigmund, O., 2011.\n\u201cRobust topology optimization accounting for spa-\ntially varying manufacturing errors\u201d. Computer Meth-\nods in Applied Mechanics and Engineering, 200(49-\n52), Dec., pp. 3613\u20133627.\n\n[15] Richardson, J. N., Filomeno Coelho, R., and Adri-\naenssens, S., 2015. \u201cRobust topology optimization\nof truss structures with random loading and material\nproperties: A multiobjective perspective\u201d. Computers\n& Structures, 154, July, pp. 41\u201347.\n\n[16] Chen, S., Chen, W., and Lee, S., 2010. \u201cLevel set\nbased robust shape and topology optimization under\n\n13 Copyright \u00a9 by ASME\n\n\n\nrandom field uncertainties\u201d. Structural and Multidis-\nciplinary Optimization, 41(4), Apr., pp. 507\u2013524.\n\n[17] Lazarov, B. S., Schevenels, M., and Sigmund, O.,\n2012. \u201cTopology optimization considering material\nand geometric uncertainties using stochastic colloca-\ntion methods\u201d. Structural and Multidisciplinary Opti-\nmization, 46(4), Oct., pp. 597\u2013612.\n\n[18] Lazarov, B. S., Schevenels, M., and Sigmund, O.,\n2012. \u201cTopology optimization with geometric un-\ncertainties by perturbation techniques\u201d. Interna-\ntional Journal for Numerical Methods in Engineering,\n90(11), June, pp. 1321\u20131336.\n\n[19] Jansen, M., Lombaert, G., Diehl, M., Lazarov, B. S.,\nSigmund, O., and Schevenels, M., 2013. \u201cRobust\ntopology optimization accounting for misplacement\nof material\u201d. Structural and Multidisciplinary Opti-\nmization, 47(3), Mar., pp. 317\u2013333.\n\n[20] Jansen, M., Lombaert, G., and Schevenels, M., 2015.\n\u201cRobust topology optimization of structures with im-\nperfect geometry based on geometric nonlinear anal-\nysis\u201d. Computer Methods in Applied Mechanics and\nEngineering, 285, Mar., pp. 452\u2013467.\n\n[21] Asadpoure, A., Tootkaboni, M., and Guest, J. K.,\n2011. \u201cRobust topology optimization of structures\nwith uncertainties in stiffness \u2212 Application to truss\nstructures\u201d. Computers & Structures, 89(11-12),\nJune, pp. 1131\u20131141.\n\n[22] Tootkaboni, M., Asadpoure, A., and Guest, J. K.,\n2012. \u201cTopology optimization of continuum struc-\ntures under uncertainty \u2212 A Polynomial Chaos ap-\nproach\u201d. Computer Methods in Applied Mechanics\nand Engineering, 201-204, Jan., pp. 263\u2013275.\n\n[23] Changizi, N., and Jalalpour, M., 2017. \u201cRobust topol-\nogy optimization of frame structures under geometric\nor material properties uncertainties\u201d. Structural and\nMultidisciplinary Optimization, 56(4), Oct., pp. 791\u2013\n807.\n\n[24] da Silva, G. A., and Cardoso, E. L., 2016. \u201cTopology\noptimization of continuum structures subjected to un-\ncertainties in material properties\u201d. International Jour-\nnal for Numerical Methods in Engineering, 106(3),\nApr., pp. 192\u2013212.\n\n[25] da Silva, G., and Cardoso, E., 2017. \u201cStress-based\ntopology optimization of continuum structures under\nuncertainties\u201d. Computer Methods in Applied Me-\nchanics and Engineering, 313, Jan., pp. 647\u2013672.\n\n[26] da Silva, G. A., Beck, A. T., and Cardoso, E. L., 2018.\n\u201cTopology optimization of continuum structures with\nstress constraints and uncertainties in loading\u201d. Inter-\nnational Journal for Numerical Methods in Engineer-\ning, 113(1), Jan., pp. 153\u2013178.\n\n[27] Zhao, Q., Chen, X., Ma, Z.-D., and Lin, Y., 2015.\n\u201cRobust Topology Optimization Based on Stochastic\nCollocation Methods under Loading Uncertainties\u201d.\nMathematical Problems in Engineering, 2015, pp. 1\u2013\n14.\n\n[28] Richardson, J., Coelho, R. F., and Adriaenssens, S.,\n2016. \u201cA unified stochastic framework for robust\n\ntopology optimization of continuum and truss-like\nstructures\u201d. Engineering Optimization, 48(2), Feb.,\npp. 334\u2013350.\n\n[29] Zhao, J., and Wang, C., 2014. \u201cRobust topology op-\ntimization under loading uncertainty based on linear\nelastic theory and orthogonal diagonalization of sym-\nmetric matrices\u201d. Computer Methods in Applied Me-\nchanics and Engineering, 273, May, pp. 204\u2013218.\n\n[30] Jalalpour, M., and Tootkaboni, M., 2016. \u201cAn effi-\ncient approach to reliability-based topology optimiza-\ntion for continua under material uncertainty\u201d. Struc-\ntural and Multidisciplinary Optimization, 53(4), Apr.,\npp. 759\u2013772.\n\n[31] McIntire, M. G., Vasylkivska, V., Hoyle, C., and Gib-\nson, N., 2014. \u201cApplying robust design optimiza-\ntion to large systems\u201d. In ASME 2014 International\nDesign Engineering Technical Conferences and Com-\nputers and Information in Engineering Conference,\npp. V02BT03A054\u2013V02BT03A054.\n\n[32] Lewis, K., Chen, W., and Schmidt, L., 2006. Decision\nMaking in Engineering Design. ASME Press, New\nYork.\n\n[33] Beck, A. T., Gomes, W. J. S., Lopez, R. H., and\nMiguel, L. F. F., 2015. \u201cA comparison between ro-\nbust and risk-based optimization under uncertainty\u201d.\nStructural and Multidisciplinary Optimization, 52(3),\nSept., pp. 479\u2013492.\n\n[34] Haldar, A., and Mahadevan, S., 2000. Probability,\nreliability, and statistical methods in engineering de-\nsign. John Wiley.\n\n[35] Valdebenito, M. A., and Schue\u0308ller, G. I., 2010. \u201cA sur-\nvey on approaches for reliability-based optimization\u201d.\nStructural and Multidisciplinary Optimization, 42(5),\nNov., pp. 645\u2013663.\n\n[36] Cornell, C., 1969. \u201cA probability-based structural\ncode\u201d. ACI Journal Proceedings, 66(12).\n\n[37] Hasofer, A. M., and Lind, N. C., 1974. \u201cExact\nand invariant second moment code format\u201d. Journal\nof Engineering Mechanics Division, 100(EM1), 01,\npp. 111\u2013121.\n\n[38] Fiessler, B., Neumann, H., and Rackwitz, R., 1979.\n\u201cQuadratic limit states in structural reliability\u201d. Jour-\nnal of Engineering Mechanics Division, 105(4), 08,\npp. 661\u2013676.\n\n[39] Maute, K., and Frangopol, D. M., 2003. \u201cReliability-\nbased design of MEMS mechanisms by topology op-\ntimization\u201d. Computers & Structures, 81(8-11), May,\npp. 813\u2013824.\n\n[40] Sato, Y., Izui, K., Yamada, T., Nishiwaki, S., Ito, M.,\nand Kogiso, N., 2018. \u201cReliability-based topology\noptimization under shape uncertainty modeled in Eu-\nlerian description\u201d. Structural and Multidisciplinary\nOptimization, Sept.\n\n[41] Kang, Z., and Liu, P., 2018. \u201cReliability-based topol-\nogy optimization against geometric imperfections\nwith random threshold model\u201d. International Journal\nfor Numerical Methods in Engineering, 115(1), July,\npp. 99\u2013116.\n\n14 Copyright \u00a9 by ASME\n\n\n\n[42] Mogami, K., Nishiwaki, S., Izui, K., Yoshimura,\nM., and Kogiso, N., 2006. \u201cReliability-based struc-\ntural optimization of frame structures for multiple fail-\nure criteria using topology optimization techniques\u201d.\nStructural and Multidisciplinary Optimization, 32(4),\nOct., pp. 299\u2013311.\n\n[43] Kang, J., Kim, C., and Wang, S., 2004. \u201cReliability-\nbased topology optimization for electromagnetic sys-\ntems\u201d. COMPEL - The international journal for com-\nputation and mathematics in electrical and electronic\nengineering, 23(3), Sept., pp. 715\u2013723.\n\n[44] Jung, H.-S., and Cho, S., 2004. \u201cReliability-\nbased topology optimization of geometrically nonlin-\near structures with loading and material uncertain-\nties\u201d. Finite Elements in Analysis and Design, 41(3),\nDec., pp. 311\u2013331.\n\n[45] Luo, Y., Zhou, M., Wang, M. Y., and Deng, Z., 2014.\n\u201cReliability based topology optimization for contin-\nuum structures with local failure constraints\u201d. Com-\nputers & Structures, 143, Sept., pp. 73\u201384.\n\n[46] da Silva, G. A., and Beck, A. T., 2018. \u201cReliability-\nbased topology optimization of continuum structures\nsubject to local stress constraints\u201d. Structural and\nMultidisciplinary Optimization, 57(6), Jun, pp. 2339\u2013\n2355.\n\n[47] Papadimitriou, D. I., and Mourelatos, Z. P., 2018.\n\u201cReliability-Based Topology Optimization Using\nMean-Value Second-Order Saddlepoint Approxima-\ntion\u201d. Journal of Mechanical Design, 140(3), Jan.,\np. 031403.\n\n[48] Schue\u0308ller, G., Pradlwarter, H., and Koutsourelakis, P.,\n2004. \u201cA critical appraisal of reliability estimation\nprocedures for high dimensions\u201d. Probabilistic En-\ngineering Mechanics, 19(4), Oct., pp. 463\u2013474.\n\n[49] Nguyen, T. H., Song, J., and Paulino, G. H., 2011.\n\u201cSingle-loop system reliability-based topology opti-\nmization considering statistical dependence between\nlimit-states\u201d. Structural and Multidisciplinary Opti-\nmization, 44(5), Nov., pp. 593\u2013611.\n\n[50] Silva, M., Tortorelli, D. A., Norato, J. A., Ha, C.,\nand Bae, H.-R., 2010. \u201cComponent and system\nreliability-based topology optimization using a single-\nloop method\u201d. Structural and Multidisciplinary Opti-\nmization, 41(1), Feb., pp. 87\u2013106.\n\n[51] Liang, J., Mourelatos, Z. P., and Tu, J., 2004. \u201cA\nSingle-Loop Method for Reliability-Based Design\nOptimization\u201d. In Volume 1: 30th Design Automa-\ntion Conference, Vol. 2004, ASME, pp. 419\u2013430.\n\n[52] Kharmanda, G., Olhoff, N., Mohamed, A., and\nLemaire, M., 2004. \u201cReliability-based topology op-\ntimization\u201d. Structural and Multidisciplinary Opti-\nmization, 26(5), Mar., pp. 295\u2013307.\n\n[53] Kharmanda, G., Mohamed, A., and Lemaire, M.,\n2002. \u201cEfficient reliability-based design optimization\nusing a hybrid space with application to finite element\nanalysis\u201d. Structural and Multidisciplinary Optimiza-\ntion, 24(3), Sept., pp. 233\u2013245.\n\n[54] Kogiso, N., Hirano, Y., Nishiwaki, S., Izui, K.,\n\nYoshimura, M., and Min, S., 2010. \u201cReliability-Based\nTopology Optimization of Frame Structures for Multi-\nple Criteria Using SLSV Method\u201d. Journal of Compu-\ntational Science and Technology, 4(3), pp. 172\u2013184.\n\n[55] Chen, X., Hasselman, T., Neill, D., Chen, X., Hassel-\nman, T., and Neill, D., 1997. \u201cReliability based struc-\ntural design optimization for practical applications\u201d.\nIn 38th Structures, Structural Dynamics, and Materi-\nals Conference, American Institute of Aeronautics and\nAstronautics.\n\n[56] Lopez, R. H., and Beck, A. T., 2012. \u201cReliability-\nbased design optimization strategies based on FORM:\na review\u201d. Journal of the Brazilian Society of Mechan-\nical Sciences and Engineering, 34(4), Dec., pp. 506\u2013\n514.\n\n[57] Zhao, Q., Chen, X., Ma, Z.-D., and Lin, Y.,\n2015. \u201cReliability-Based Topology Optimization Us-\ning Stochastic Response Surface Method with Sparse\nGrid Design\u201d. Mathematical Problems in Engineer-\ning, 2015, pp. 1\u201313.\n\n[58] Zhao, Q., Chen, X., Ma, Z., and Lin, Y., 2016.\n\u201cA Comparison of Deterministic, Reliability-Based\nTopology Optimization under Uncertainties\u201d. Acta\nMechanica Solida Sinica, 29(1), Feb., pp. 31\u201345.\n\n[59] Wang, G. G., and Shan, S., 2007. \u201cReview of Meta-\nmodeling Techniques in Support of Engineering De-\nsign Optimization\u201d. Journal of Mechanical Design,\n129(4), p. 370.\n\n[60] Patel, J., and Choi, S.-K., 2012. \u201cClassification ap-\nproach for reliability-based topology optimization us-\ning probabilistic neural networks\u201d. Structural and\nMultidisciplinary Optimization, 45(4), Apr., pp. 529\u2013\n543.\n\n[61] Du, X., Sudjianto, A., and Chen, W., 2004. \u201cAn In-\ntegrated Framework for Optimization Under Uncer-\ntainty Using Inverse Reliability Strategy\u201d. Journal of\nMechanical Design, 126(4), p. 562.\n\n[62] Youn, B. D., Choi, K. K., and Yi, K., 2005. \u201cPerfor-\nmance Moment Integration (PMI) Method for Quality\nAssessment in Reliability-Based Robust Design Opti-\nmization\u201d. Mechanics Based Design of Structures and\nMachines, 33(2), Apr., pp. 185\u2013213.\n\n[63] Mourelatos, Z. P., and Liang, J., 2006. \u201cA Method-\nology for Trading-Off Performance and Robustness\nUnder Uncertainty\u201d. Journal of Mechanical Design,\n128(4), p. 856.\n\n[64] Tang, Y., Chen, J., and Wei, J., 2012. \u201cA Sequential\nAlgorithm for Reliability-Based Robust Design Opti-\nmization Under Epistemic Uncertainty\u201d. Journal of\nMechanical Design, 134(1), p. 014502.\n\n[65] Forouzandeh Shahraki, A., and Noorossana, R., 2014.\n\u201cReliability-based robust design optimization: A gen-\neral methodology using genetic algorithm\u201d. Comput-\ners & Industrial Engineering, 74, Aug., pp. 199\u2013207.\n\n[66] Rathod, V., Yadav, O. P., Rathore, A., and Jain,\nR., 2013. \u201cOptimizing reliability-based robust de-\nsign model using multi-objective genetic algorithm\u201d.\nComputers & Industrial Engineering, 66(2), Oct.,\n\n15 Copyright \u00a9 by ASME\n\n\n\npp. 301\u2013310.\n[67] Lee, I., Choi, K., Du, L., and Gorsich, D., 2008.\n\n\u201cDimension reduction method for reliability-based ro-\nbust design optimization\u201d. Computers & Structures,\n86(13-14), July, pp. 1550\u20131562.\n\n[68] Youn, B. D., Xi, Z., Wells, L. J., and Lamb,\nD. A., 2006. \u201cStochastic Response Surface Using the\nEnhanced Dimension-Reduction (eDR) Method for\nReliability-Based Robust Design Optimization\u201d. In III\nEuropean Conference on Computational Mechanics,\nC. A. Motasoares, J. A. C. Martins, H. C. Rodrigues,\nJ. A. C. Ambro\u0301sio, C. A. B. Pina, C. M. Motasoares,\nE. B. R. Pereira, and J. Folgado, eds. Springer Nether-\nlands, Dordrecht, pp. 388\u2013388.\n\n[69] Youn, B. D., and Xi, Z., 2009. \u201cReliability-based ro-\nbust design optimization using the eigenvector dimen-\nsion reduction (EDR) method\u201d. Structural and Multi-\ndisciplinary Optimization, 37(5), Feb., pp. 475\u2013492.\n\n[70] Keshavarzzadeh, V., Fernandez, F., and Tortorelli,\nD. A., 2017. \u201cTopology optimization under uncer-\ntainty via non-intrusive polynomial chaos expansion\u201d.\nComputer Methods in Applied Mechanics and Engi-\nneering, 318, May, pp. 120\u2013147.\n\n[71] Taflanidis, A. A., and Beck, J. L., 2008. \u201cAn effi-\ncient framework for optimal robust stochastic system\ndesign using stochastic simulation\u201d. Computer Meth-\nods in Applied Mechanics and Engineering, 198(1),\nNov., pp. 88\u2013101.\n\n[72] Keshavarzzadeh, V., Meidani, H., and Tortorelli,\nD. A., 2016. \u201cGradient based design optimization\nunder uncertainty via stochastic expansion methods\u201d.\nComputer Methods in Applied Mechanics and Engi-\nneering, 306, July, pp. 47\u201376.\n\n[73] Bends\u00f8e, M. P., and Sigmund, O., 1999. \u201cMate-\nrial interpolation schemes in topology optimization\u201d.\nArchive of Applied Mechanics (Ingenieur Archiv),\n69(9-10), Nov., pp. 635\u2013654.\n\n[74] Svanberg, K., 1987. \u201cThe method of moving asymp-\ntotes\u2014a new method for structural optimization\u201d. In-\nternational Journal for Numerical Methods in Engi-\nneering, 24(2), Feb., pp. 359\u2013373.\n\n[75] Svanberg, K., 2002. \u201cA Class of Globally Convergent\nOptimization Methods Based on Conservative Convex\nSeparable Approximations\u201d. SIAM Journal on Opti-\nmization, 12(2), Jan., pp. 555\u2013573.\n\n[76] Sigmund, O., and Petersson, J., 1998. \u201cNumer-\nical instabilities in topology optimization: A sur-\nvey on procedures dealing with checkerboards, mesh-\ndependencies and local minima\u201d. Structural Opti-\nmization, 16(1), Aug., pp. 68\u201375.\n\n[77] Sigmund, O., 2007. \u201cMorphology-based black and\nwhite filters for topology optimization\u201d. Structural\nand Multidisciplinary Optimization, 33(4-5), Feb.,\npp. 401\u2013424.\n\n[78] Bruns, T. E., and Tortorelli, D. A., 2001. \u201cTopol-\nogy optimization of non-linear elastic structures and\ncompliant mechanisms\u201d. Computer Methods in Ap-\nplied Mechanics and Engineering, 190(26-27), Mar.,\n\npp. 3443\u20133459.\n[79] Bourdin, B., 2001. \u201cFilters in topology optimization\u201d.\n\nInternational Journal for Numerical Methods in En-\ngineering, 50(9), Mar., pp. 2143\u20132158.\n\n[80] Andreassen, E., Clausen, A., Schevenels, M.,\nLazarov, B. S., and Sigmund, O., 2011. \u201cEfficient\ntopology optimization in MATLAB using 88 lines of\ncode\u201d. Structural and Multidisciplinary Optimization,\n43(1), Jan., pp. 1\u201316.\n\n[81] Lee, S. H., and Chen, W., 2009. \u201cA comparative\nstudy of uncertainty propagation methods for black-\nbox-type problems\u201d. Structural and Multidisciplinary\nOptimization, 37(3), Jan., pp. 239\u2013253.\n\n[82] Xiu, D., 2010. Numerical Methods for Stochastic\nComputations: A Spectral Method Approach. Prince-\nton University Press.\n\n[83] Xiong, F., Greene, S., Chen, W., Xiong, Y., and Yang,\nS., 2010. \u201cA new sparse grid based method for uncer-\ntainty propagation\u201d. Structural and Multidisciplinary\nOptimization, 41(3), Apr., pp. 335\u2013349.\n\n[84] Gerstner, T., and Griebel, M., 1998. \u201cNumerical in-\ntegration using sparse grids\u201d. Numerical Algorithms,\n18, Jan., pp. 209\u2013232.\n\n[85] Smolyak, S. A., 1963. \u201cQuadrature and interpolation\nformulas for tensor products of certain classes of func-\ntions\u201d. Dokl. Akad. Nauk SSSR, 148(5), pp. 1042\u2013\n1045.\n\n[86] Xiu, D., and Hesthaven, J. S., 2005. \u201cHigh-Order Col-\nlocation Methods for Differential Equations with Ran-\ndom Inputs\u201d. SIAM Journal on Scientific Computing,\n27(3), Jan., pp. 1118\u20131139.\n\n[87] Maitre, O., and Knio, O., 2010. Spectral Methods\nfor Uncertainty Quantification: With Applications to\nComputational Fluid Dynamics. Scientific Computa-\ntion. Springer Netherlands.\n\n[88] Davis, P., and Rabinowitz, P., 2007. Methods of Nu-\nmerical Integration. Dover Books on Mathematics Se-\nries. Dover Publications.\n\n[89] Li, C., and Der Kiureghian, A., 1993. \u201cOptimal Dis-\ncretization of Random Fields\u201d. Journal of Engineer-\ning Mechanics, 119(6), June, pp. 1136\u20131154.\n\n[90] Loe\u0300ve, M., 2017. Probability Theory: Third Edition.\nDover Books on Mathematics. Dover Publications.\n\n[91] Ray, S. S., and Sahu, P. K., 2013. \u201cNumerical Meth-\nods for Solving Fredholm Integral Equations of Sec-\nond Kind\u201d. Abstract and Applied Analysis, 2013,\npp. 1\u201317.\n\n[92] Wang, L., 2008. \u201cKarhunen\u2212\u2013Loe\u0300ve Expansions and\ntheir Applications\u201d. PhD thesis, London School of\nEconomics and Political Science.\n\n[93] Missoum, S., Lacaze, S., Boroson, E., and Jiang, P.,\n2015. CODES Toolbox. Computational Optimal De-\nsign of Engineering Systems Laboratory. University\nof Arizona.\n\n[94] Yin, X., and Chen, W., 2006. \u201cEnhanced sequen-\ntial optimization and reliability assessment method for\nprobabilistic optimization with varying design vari-\nance\u201d. Structure and Infrastructure Engineering, 2(3-\n\n16 Copyright \u00a9 by ASME\n\n\n\n4), Sept., pp. 261\u2013275.\n[95] Huang, S., and Kou, X., 2007. \u201cAn extended stochas-\n\ntic response surface method for random field prob-\nlems\u201d. Acta Mechanica Sinica, 23(4), Aug., pp. 445\u2013\n450.\n\n[96] Isukapalli, S., Balakrishnan, S., and Georgopoulos, P.,\n2004. \u201cComputationally efficient uncertainty propa-\ngation and reduction using the stochastic response sur-\nface method\u201d. IEEE, pp. 2237\u20132243 Vol.2.\n\n[97] Frecker, M. I., Ananthasuresh, G. K., Nishiwaki,\nS., Kikuchi, N., and Kota, S., 1997. \u201cTopologi-\ncal Synthesis of Compliant Mechanisms Using Multi-\nCriteria Optimization\u201d. Journal of Mechanical De-\nsign, 119(2), p. 238.\n\n[98] Marler, R. T., and Arora, J. S., 2010. \u201cThe weighted\nsum method for multi-objective optimization: new in-\nsights\u201d. Structural and Multidisciplinary Optimiza-\ntion, 41(6), June, pp. 853\u2013862.\n\n[99] Alexanderian, A., 2015. A brief note on the\nKarhunen-Loe\u0300ve expansion.\n\n[100] Youn, B. D., Choi, K. K., and Park, Y. H., 2003.\n\u201cHybrid Analysis Method for Reliability-Based De-\nsign Optimization\u201d. Journal of Mechanical Design,\n125(2), p. 221.\n\n17 Copyright \u00a9 by ASME\n\n\n\t1 Introduction\n\t2 Background\n\t3 Topology Optimization under Uncertainty\n\t3.1 Deterministic Topology Optimization\n\t3.2 Robust Reliability-based Topology Optimization\n\t3.2.1 Problem Formulation\n\t3.2.2 Smolyak-type Sparse Grid\n\t3.2.3 Karhunen-\u2013Lo\u00e8ve Expansion\n\t3.2.4 Inverse Reliability Analysis and SORA\n\t3.2.5 Stochastic Response Surface Method\n\t3.2.6 Solution Algorithm\n\n\n\t4 Results\n\t4.1 The Cantilever Beam\n\t4.2 The L-shaped Beam\n\n\t5 Discussions\n\t6 Conclusions\n\n"}
{"Title": "A Literature Review on Length of Stay Prediction for Stroke Patients using Machine Learning and Statistical Approaches", "Authors": "Ola Alkhatib, Ayman Alahmar", "Abstract": "  Hospital length of stay (LOS) is one of the most essential healthcare metrics that reflects the hospital quality of service and helps improve hospital scheduling and management. LOS prediction helps in cost management because patients who remain in hospitals usually do so in hospital units where resources are severely limited. In this study, we reviewed papers on LOS prediction using machine learning and statistical approaches. Our literature review considers research studies that focus on LOS prediction for stroke patients. Some of the surveyed studies revealed that authors reached contradicting conclusions. For example, the age of the patient was considered an important predictor of LOS for stroke patients in some studies, while other studies concluded that age was not a significant factor. Therefore, additional research is required in this domain to further understand the predictors of LOS for stroke patients.      ", "Subject": "Machine Learning (cs.LG)", "ID": "arXiv:2201.00005", "Text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Title (use style: paper title)\n\n\nA Literature Review on Length of Stay Prediction for Stroke \n\nPatients using Machine Learning and Statistical Approaches \n\nOla Alkhatib1 and Ayman Alahmar2 \n\n1Department of Computer Science, Lakehead University, Thunder Bay, Ontario, Canada \n2Department of Software Engineering, Lakehead University, Thunder Bay, Ontario, Canada \n\nAbstract Hospital length of stay (LOS) is one of the most essential healthcare metrics that \n\nreflects the hospital quality of service and helps improve hospital scheduling and management. \n\nLOS prediction helps in cost management because patients who remain in hospitals usually do \n\nso in hospital units where resources are severely limited. In this study, we reviewed papers on \n\nLOS prediction using machine learning and statistical approaches. Our literature review \n\nconsiders research studies that focus on LOS prediction for stroke patients. Some of the \n\nsurveyed studies revealed that authors reached contradicting conclusions. For example, the age \n\nof the patient was considered an important predictor of LOS for stroke patients in some studies, \n\nwhile other studies concluded that age was not a significant factor. Therefore, additional \n\nresearch is required in this domain to further understand the predictors of LOS for stroke \n\npatients. \n\nKeywords: Length of Stay, Stroke, Machine Learning, Data Mining, Statistical Analysis. \n\n1. INTRODUCTION\n\nEALTHCARE sectors show increasing costs in most regions around the world. Healthcare\n\nexpenditure constitutes a significant share of the gross domestic product for many \n\ncountries. There are many challenges associated with growth in the healthcare sector, including \n\nincreased pressure on the limited resources of hospitals. This issue has motivated researchers \n\nto conduct further research related to hospital resource optimization. Since hospitalization \n\nconstitutes a significant cost of patient care, many researchers have been investigating the \n\nproblem of patient Length of Stay (LOS) prediction. LOS is defined as the duration of a patient \n\nhospitalization, and it is determined as the difference between the timestamp of a patient \n\nhospital discharge and the timestamp of their hospital admission [1], [2]. LOS prediction is an \n\nimportant topic for many reasons, such as: \n\n\u2022 Knowledge of LOS allows hospitals to manage their bed and room capacities so that\n\nthey can know how long a patient is expected to occupy hospital space.\n\n\u2022 Information about LOS allows hospitals to determine the number of staff that must be\n\nscheduled over the day/night shifts to properly accommodate the patients.\n\n\u2022 Patients and their families can estimate the cost of a stay in paid hospitalizations.\n\nBy investigating the existing literature, we found that LOS prediction papers used machine \n\nlearning/statistical approaches and can be divided into the type of disease under consideration. \n\nSome authors studied LOS prediction in general (i.e. without specifying a specific disease), \n\nwhile other researchers focused on LOS prediction pertinent to a specific disease (e.g., stroke, \n\nH \n\n1 \n\n\n\n2 \n\ndiabetes). Fig. 1 depicts this categorization and includes example references to research articles. \n\nFig. 1. Classification of LOS prediction research articles. \n\nIn this survey, we review papers that predict the patients\u2019 LOS in general and then we focus \n\non LOS prediction for stroke patients. We focused on stroke patients because they face many \n\nchallenges with definite need for hospitalization, and also because strokes have an enormous \n\ncost on healthcare systems around the world. Using popular research search engines (e.g., IEEE \n\nXplore, Springer, Science Direct, etc.), we searched phrases such as \u201clength of stay\u201d, \u201chospital \n\nlength of stay\u201d, \u201cprediction\u201d, \u201cmachine learning\u201d, \u201cstroke\u201d, \u201cischemic stroke\u201d, \u201cdata mining\u201d, \n\nand \u201cstatistical analysis\u201d in order to find existing work on this topic (up to mid 2020). \n\nStroke is a disease that affects the arteries leading to and within the brain. It can be a \n\nsignificant financial and health burden for patients, medical staff, and healthcare systems. \n\nStroke is associated with prolonged LOS in hospitals and rehabilitation facilities [1] and is a \n\nleading cause of death and disability worldwide. According to Statistics Canada, in 2018, stroke \n\nwas the third largest cause of death in Canada after cancer and heart disease \n\n(https://www.statcan.gc.ca). Stroke, also known as cerebrovascular accident (or CVA), is a \n\nsudden and devastating illness that is characterized by the rapid loss of the functions of the brain \n\ndue to a disruption of blood flow to the brain (see Fig. 2). This disruption is caused by: lack of \n\nblood flow (ischemic strokes), which account for more than 80% of all strokes; blockage of \n\nblood flow; or hemorrhage [3]. \n\nFig. 2. Illustration of stroke (Source: Mayo Clinic (mayoclinic.org)). \n\nThe remainder of the paper is organized as follows. Below we present our literature review on \n\n\n\n \n\n \n\n \n\n \n\n \n\n3 \n\n \n\nLOS prediction for general patients. Next we consider stroke patients. Finally, the paper ends \n\nwith a discussion and conclusions section. \n\n2. LOS PREDICTION FOR GENERAL PATIENTS \n\nKabir et al. [4] proposed a non-linear feature selection method using artificial neural networks \n\n(ANNs) to determine the most essential features for LOS prediction. The study evaluates the \n\nperformance of (ANNs), support vector machines (SVM), and logistic regression (LR) on \n\nselected subsets of features to predict the LOS class and identify the best subset of features. The \n\nstudy used a dataset from the National Surgical Quality Improvement Program database. The \n\ndataset, based on data from 2015, included 273 features of more than 880,000 surgical patients \n\nadmitted to hospital for various surgical procedures to address different diseases and medical \n\nconditions. The authors reduced the features from 273 to 40 based on consultation with domain \n\nexperts (anesthesiologists). After preprocessing the dataset, 715,143 patient records were \n\nselected for further analysis. The patients were categorized using their medical group into the \n\nfollowing nine surgical categories: (1) general surgery, (2) vascular, (3) urology, (4) plastics, \n\n(5) otolaryngology, (6) orthopedics, (7) gynecology, (8) neurosurgery, and (9) other surgical \n\nconditions (including thoracic, cardiac surgery, and interventional radiology patients). An \n\nadditional category that includes all patients was added as group (10) for comparison purposes. \n\nThe authors presented the normalized importance score of features for each patient category. In \n\ngeneral, features 16, 20, and 21 showed the most significant contribution to the prediction of \n\nLOS. Feature 16 indicates the period from admission to surgery, feature 20 denotes whether the \n\npatient is an outpatient or inpatient, and feature 21 represents the estimated probability of \n\nmorbidity computed by the hospital using LR. These features had a strong correlation with LOS \n\nwhich validates the performance of the non-linear approach of this study in obtaining \n\nconsiderable features. Another important factor presented in this research is the specific \n\ncorrelation of features with their categories. For example, feature 38, which represents the \n\nsituation of a patient\u2019s wound, was important in predicting the LOS for otolaryngology patients, \n\nwhile it had less importance in LOS predicting for other patient categories. By comparing the \n\nimportance of features computed for all patients (group 10) with other categories, the authors \n\nshowed that grouping patients based on their disease can improve the accuracy of LOS \n\npredictive models. The final results revealed that ANNs, as non-linear classifiers, beat SVMs \n\nand LR in the achieved accuracy for LOS prediction. This proves that the relationship between \n\nLOS and its predictors is highly non-linear. The ANNs model improves accuracy and eliminates \n\nthe number of required features. \n\nAzari et al. [5] used a multi-tiered data mining approach for predicting hospital LOS to \n\ndecrease the uncertainty related with the LOS for inpatients. Their prediction approach was \n\nbased on clustering (i.e., k-means clustering) to create the training sets that train various \n\nclassification algorithms. The number of clusters was determined based on the disease \n\nconditions or by using the Charlson index which provides the general categories of the diseases. \n\nFig. 3 describes the approach used in this study. Several classifiers were used to predict the \n\nLOS such as: K-nearest neighbors, LR, naive Bayes, SVMs, Bayesian networks (Bnets), J48 \n\ndecision tree, classification rules (JRip), bagging, random forest, and boosting. The paper \n\nconsidered various performance metrics such as accuracy, Kappa statistic, precision, recall, and \n\narea under the curve (AUC). In order to rank the classifiers, researchers used the Friedman test \n\nto determine the classifier with the best outcome for a certain level of clustering. The dataset \n\n\n\n \n\n \n\n \n\n \n\n \n\n4 \n\n \n\nused the Heritage Health prize data. The dataset contains 1,048,576 records of hospital claims \n\nwithin a 3-year period. The results showed that using clustering as a precursor to form the \n\ntraining set provides better results compared to non-clustering based training sets. The results \n\nalso showed that Bnets, SVMs, JRip, Bagging, and J48 had better overall performance than the \n\nother classifiers. The outcomes of the paper were validated by a domain expert from Emergency \n\nMedicine. \n\n \n\n \n \n\nFig. 3. LOS prediction approach of Azari et al. [5]. \n\n \n\nNouaouri et al. [6] proposed the application of data mining techniques to predict the LOS for \n\npatients without considering a specific disease. They introduced the Evidential LOS prediction \n\nAlgorithm (ELOSA) that allows the prediction of the LOS of a new patient. Their approach \n\nhandles the imprecision, uncertainty, and missing data within the dataset of patients. The \n\nELOSA algorithm is based on the precise support and association rule confidence measures [6]. \n\nThe LOS experiments were conducted on a real hospital dataset that contains the data for 270 \n\npatients. To predict the inpatient LOS, the authors considered age, sex, physiological conditions \n\n(emergency degree), and operation length. The emergency degree column in the dataset was a \n\ncategorical attribute that contained values from the following set {A, R, D}. A indicates an \n\nabsolute emergency, R is for relative emergency, and D represents delayed emergency. The \n\nlength of stay was classified as Short (S), approximately 3 days, Medium (M), approximately \n\n10 days, and Long (L), approximately 20 days, with a small overlap between the ranges as \n\nshown in Fig. 4. They compared their results with other algorithms in similar studies and \n\nconcluded that their approach showed better results. \n\nRathor et al. [7] used a clustering algorithm (i.e., Density Based Spatial Clustering of \n\nApplications with Noise (DB- SCAN)) and K-Apriori, which is a combination of Apriori and \n\nK-means algorithms. The algorithms were applied to a dataset of 9,052 patients (the source of \n\nthe dataset was not disclosed). The execution time of the algorithms were compared, showing \n\nDBSCAN to be faster than the K-Apriori; although, DBSCAN took exponentially longer as the \n\nnumber of inputs increased. The prediction was based on the current symptoms and medical \n\nhistory of the patients, which was provided by the patient at the time of admission. For \n\nprediction of LOS, the medical data underwent a pre-processing phase, which had three steps:  \n\n \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n5 \n\n \n\n \n \n\nFig. 4. LOS classes in terms of days [6]. \n\n \n\n \n\ndata cleaning, data integration and transformation, and data reduction. Then, using the \n\nprocessed data, symptoms for a particular disease were grouped together and used for LOS \n\nprediction. The study determined the times of execution of K-apriori and DBSCAN \n\nindependently and subsequently compared them. Both algorithms were treated with the same \n\nnumber of inputs and with the same values. The authors concluded that the execution time of \n\nDBSCAN was comparatively much shorter than K-Apriori, but as the number of inputs increase \n\nto high values, the execution time of DBSCAN increased exponentially whereas there was no \n\nchange in K-Apriori. \n\n \n\n3. LOS PREDICTION FOR STROKE PATIENTS \n\nIn stroke, the brain is prevented from getting oxygen and nutrients from the blood. Without \n\noxygen and nutrients, brain cells begin to die within minutes. Sudden bleeding in the brain can \n\nalso cause a stroke if it damages brain cells. A stroke is a medical emergency that can cause \n\nlasting brain damage, long-term disability, or even death. Signs of a stroke can range from mild \n\nweakness to paralysis or numbness on one side of the face or body. Other signs include a sudden \n\nand severe headache, sudden weakness, trouble seeing, and trouble speaking or understanding \n\nspeech (https://www.nhlbi.nih.gov/health-topics/stroke). \n\nAl Taleb et al. [3] introduced a machine learning method for early prediction of LOS of stroke \n\npatients. They tested their approach at the Stroke Unit of King Fahad Bin Abdul- Aziz Hospital \n\nin Saudi Arabia. The study was based on 866 stroke patients, whose data was retrieved from \n\nthe Neurology Department database. For data cleaning, each set of patient data was manually \n\nexamined for invalid or erroneous inputs. Records with missing values in more than 50% of the \n\nattributes were deleted. For the records with missing values in less than 50% of attributes, \n\nmissing values were replaced with the average value of the respective attributes for numeric \n\nattributes, and with the mode value for the categorical attributes. The approach involved a \n\nfeature selection step based on Information Gain (IG) followed by a prediction model \n\ndevelopment step using different machine learning algorithms as explained below. \n\n \n\nThe original dataset contained 105 attributes, out of which 54 attributes were manually \n\neliminated due to being irrelevant or redundant, such as time of arrival, date of MRI, and cause \n\nof death. The remaining 51 attributes were ranked based on their IG with respect to LOS, and \n\nthen an iterative process of elimination was applied where the researchers began processing all \n\n\n\n \n\n \n\n \n\n \n\n \n\n6 \n\n \n\nof the features. Then features were eliminated one at a time, starting with the least ranked one, \n\nand the IG was recalculated. The repetitive process stopped when there was no further \n\nimprovement in IG. Finally, 16 remaining attributes (including LOS) were selected for the \n\nprediction steps. The selected attributes and their IG values are listed in Fig. 5. \n\n \n\n \n\n \n \n\nFig. 5. Selected attributes and their IG values with respect to the class attribute (LOS) [3].  \n\n \n\nPrediction results were compared to identify the algorithm with the best performance. Several \n\nexperiments were performed in various settings. The authors found that the most accurate model \n\nin their study was the Bnet model with accuracy of 81.28%. \n\nIn another study, Neto et al. [8] proposed a neural network LOS prediction method based on \n\nthe information available on the stroke neurological events, the patient\u2019s health status, and \n\nsurgery details. The neural network was trained to test with three attribute subsets of different \n\nsizes. The first subset contained 33 attributes, the second 14, and the third subset consisted of \n\nonly 7 attributes. By testing the three subsets, it was possible to define an optimal neural \n\nnetwork configuration where the lowest error values were registered as Root Mean Squared \n\nError, 5.9451, and Mean Absolute Error, 4.6354. They concluded that the third use case (the \n\none with fewer variables) obtained better results than the other attribute sets. \n\nZhang et al. [1] aimed to develop a risk prediction model of prolonged LOS in stroke patients \n\nfor 50 inpatient rehabilitation centers in 20 provinces across mainland China, based on the \n\nInternational Classification of Functioning, Disability, and Health Generic Set case mix on \n\nadmission. The study was conducted on 383 stroke patients. The independent predictors of \n\nprolonged LOS were identified using Multivariate Logistic Regression (MLR) analysis. A \n\nprediction model was established and then evaluated by receiver operating characteristic curve \n\nanalysis, and the Hosmer-Lemeshow test. The results showed that the type of medical insurance \n\nand the performance of daily activities were associated with prolonged LOS. Age and mobility \n\nlevel demonstrated no significant predictive value. The prediction model revealed acceptable \n\ndiscrimination shown by an AUC of 0.699. The researchers concluded that the scores for the \n\ntype of medical insurance and the performance of daily activities on admission were \n\n\n\n \n\n \n\n \n\n \n\n \n\n7 \n\n \n\nindependent predictors of prolonged LOS for stroke patients. Their study proved that prediction \n\nmodels allow stakeholders to quantitatively estimate the risk of prolonged LOS upon admission, \n\nand to facilitate financial planning. They can also determine any required treatment regimens \n\nduring hospitalization, the need for referral after discharge, and reimbursement of costs. \n\nMinaeian et al. [9] sought to determine whether a longer emergency LOS was associated with \n\na poor 90-day outcome following an ischemic stroke. Their method was based on a \n\nretrospective analysis of a single-center cohort of consecutive ischemic stroke patients. There \n\nwere 325 patients in the study. They constructed multivariable linear and LR models to \n\ndetermine factors independently associated with emergency LOS as well as a poor 90-day \n\noutcome. The results revealed that the median LOS in the cohort was 5.8 hours of time spent in \n\nemergency. For patients admitted to the inpatient stroke ward (160 patients) versus \n\nneurointensive care unit (NICU) (165 patients), the median LOS was 8.2 hours versus 3.7 hours, \n\nrespectively. On multivariable linear regression, NICU admission, endovascular stroke therapy, \n\nand thrombolysis were inversely associated with the LOS. Evening shift presentation was \n\nassociated with a longer LOS. On MLR, a greater admission stroke severity, worse pre-\n\nadmission modified Rankin scale, hemorrhagic conversion, and a shorter LOS were associated \n\nwith a poor 90-day outcome. Early initiation of statin therapy, endovascular stroke therapy, \n\nNICU admission, and evening shift presentation were associated with a good 90-day outcome. \n\nThe authors stressed in their conclusion that in contrast to prior studies, a shorter emergency \n\nLOS was associated with a worse 90-day functional outcome, possibly reflecting prioritized \n\nadmission of more severely affected stroke patients who were at high risk for a poor functional \n\noutcome. \n\nChang et al. [10] aimed to determine the clinical and demographic predictors of LOS of acute \n\ncare hospital stay for patients with first-ever ischemic stroke. In the study, a group of 330 \n\npatients who had their first-ever ischemic stroke and were admitted to a medical center in \n\nsouthern Taiwan were followed prospectively. The researchers evaluated only the factors that \n\ncould be known at the time of admission. Univariate analysis and multiple regression analysis \n\nwere used to identify the LOS main predictors. \n\nIn the reported results, the median LOS was 7 days, average LOS was 11 days, and the LOS \n\nrange was 1 to 122 days. Among the prespecified demographic and clinical characteristics, the \n\nNational Institutes of Health Stroke Scale (NIHSS) score at admission, the quadratic term of \n\nthe initial NIHSS score, the modified Barthel Index score at admission, small-vessel occlusion \n\nstroke, smoking, and sex were the main predictors for LOS. In particular, for each 1-point \n\nincrease in the score of NIHSS, LOS increased by approximately 1 day for patients with mild \n\nor moderate neurological impairments (score 0 to 15 points), while LOS decreased \n\napproximately 1 day for patients with severe neurological impairments (score 15 points). The \n\nauthors concluded that the severity of acute stroke, as scored by the total score on NIHSS, was \n\nan important factor influencing LOS after acute stroke hospitalization.  \n\nAppelros et al. [11] examined the factors that influence acute and total LOS for stroke \n\npatients. The basis of their investigation was a population-based cohort of first-ever stroke \n\npatients (388 patients). Patient data included age, sex, risk factors, social factors, dementia, \n\nstroke type, and stroke severity, measured using the NIHSS. The results showed a mean acute \n\nLOS of 12 days and mean total LOS of 29 days. Independent predictors of acute LOS were \n\nstroke severity, lacunar stroke, pre-stroke dementia, and smoking. Independent predictors of \n\ntotal LOS were stroke severity and pre-stroke activities of daily living dependency. The NIHSS \n\n\n\n \n\n \n\n \n\n \n\n \n\n8 \n\n \n\nelements that best correlated with LOS included paresis, unilateral neglect, and level of \n\nconsciousness. The conclusion was that stroke severity is a strong and reliable predictor of LOS. \n\nThe results can be used as a baseline for evaluating cost-effectiveness of stroke care changes, \n\nsuch as assessment of new drugs and organizational modifications. \n\nThe study conducted by Okere et al. [12] was designed to evaluate predictors of hospital LOS \n\nand re-admissions among non-surgical ischemic stroke patients. The patients in this study were \n\nadult patients (\u2265 18 years) with a diagnosis of non-surgical ischemic stroke, who were \n\nhospitalized between November 2007 and March 2013. The results of the statistical analyses \n\n(multivariate and bivariate analyses), revealed that insurance type was a significant predictor of \n\nLOS, with Medicare patients having a longer LOS compared to patients with private insurance. \n\nSeverity of illness was also a predictor of LOS, whereby patients prescribed statins and patients \n\naged less than 80 years old had a lower 30-day hospital re-admission rate compared to patients \n\nwho were not prescribed statins and who were older than 80 years of age, respectively. \n\nChoi et al. [13] considered LOS prediction for acute stroke patients and extracted their dataset \n\nfrom 2013 and 2014 discharge injured patient data. The data was classified as 60% for training \n\nand 40% for evaluation. In their model, they used the multiple regression analysis method \n\ncombined with machine learning techniques (such as decision tree and neural network) to create \n\nan ensemble technique that integrates all methods. They evaluated their model using root \n\nabsolute error index. Considering the used methods, the error index was 23.7 for multiple \n\nregression, 23.7 for decision tree, 22.7 for neural network, and 22.7 for the ensemble technique. \n\nThey concluded that the neural network technique was found to be superior (even reaching the \n\nlevel of ensemble methods). \n\nIn the study carried out by Svendsen et al. [14], the author\u2019s objective was to determine \n\nwhether healthcare quality was associated with LOS among stroke patients. They performed a \n\npopulation-based study that included 2,636 stroke patients between 2003 and 2005 from a \n\nstroke unit in Denmark. In this study, quality of care was measured as fulfillment of twelve (12) \n\ncriteria: \u201cearly admission to a stroke unit, early antiplatelet therapy, early anticoagulant therapy, \n\nearly computed tomography/magnetic resonance imaging scan, early water swallowing test, \n\nearly mobilization, early intermittent catheterization, early deep venous thromboembolism \n\nprophylaxis, early assessment by a Physiotherapist and an Occupational Therapist, and early \n\nassessment of nutritional and constipation risk\u201d [14]. The authors\u2019 analyzed the patients\u2019 data \n\nusing linear regression clustered at the stroke units by multilevel modeling. The results showed \n\nthat the median length of stay was 13 days. Fulfilling each quality of care criterion was \n\nassociated with shorter LOS. The authors found that \u201cthe association between meeting more \n\nquality of care criteria and LOS followed a dose-response effect, that is, patients who fulfilled \n\nbetween 75% and 100% of the quality of care criteria were hospitalized only one-half as long \n\nas patients who fulfilled between 0% and 24% of the criteria\u201d. The study concluded that the \n\ncare in the early phase of stroke is very important as a high initial quality of care was associated \n\nwith shorter length of stay among stroke patients. \n\nGarza-Ulloa [15] used neural network algorithms to predict rehabilitation LOS for stroke \n\npatients along with other stroke metrics (i.e., the need for surgery and rehabilitation need). The \n\nstudy objective was to find an optimal neural network configuration using three different \n\navailable software: one manual (with no automatic stepwise functions and limited diagnostic \n\ncapability), another semi-automatic (allows step- wise function with good diagnostics), and \n\nneuro-intelligence (uses genetic algorithm to find the best neural network (NN) configuration). \n\n\n\n \n\n \n\n \n\n \n\n \n\n9 \n\n \n\nBased on the 14 stroke input variables and the 3 output target stroke values, the paper suggested \n\nthat the forecasting of: surgery, rehab and days of rehabilitation were possible using neural \n\nnetwork tools. Fig. 6 (from [15]) outlines the 14-group variables for the proposed neural \n\nnetwork . \n\n \n\n \nFig. 6. 14-Group variables for the proposed NN [15]. \n\n \n\nThe study of Ng et al. [16] aimed to investigate LOS characteristics and identify the predictors \n\nof post-stroke acute, rehabilitation and total LOS. The study divided the stroke patients (1,277 \n\npatients) into two subgroups of short LOS and long LOS, and compared the two subgroups \n\nregarding complication rates and functional outcomes. The authors considered stroke patients \n\nwithin a 5-year period from 2004 to 2009 in a dedicated rehabilitation unit within a tertiary \n\nacademic acute hospital in Singapore. The primary outcome measure considered in the \n\nrehabilitation phase was the functional independence measure (FIM). Short acute LOS patients \n\nwere defined as patients who stayed less than 7 days. Most patients in the study were ischemic \n\nstroke patients (1,019 patients (80%)), while the remaining patients were haemorrhagic stroke \n\npatients (20%). The results of the study showed that the average acute and rehabilitation LOS \n\nwere 9-7 days and 18-10 days, respectively. \u201cHaemorrhagic strokes and anterior circulation \n\ninfarcts had significantly longer acute, rehabilitation and total LOS compared to posterior \n\ncirculation and lacunar infarcts\u201d [16]. Patients that were admitted after 2007 had significantly \n\nshorter acute, rehabilitation and total LOS. The authors found poor correlation between the \n\nacute and rehabilitation LOS (r = 0.12). In multivariate analysis, considering rehabilitation \n\nLOS, admission FIM scores were significantly associated with LOS, while, in acute LOS, \n\nstroke type was strongly associated with LOS. \u201cPatients in the short acute LOS group had fewer \n\nmedical complications and similar FIM efficacies compared to the longer acute LOS group.\u201d \n\n[16]. The authors concluded that it is very important to transfer appropriate patients as early as \n\npossible to rehabilitation units as this ensures that the development of clinical complications is \n\nminimized, while rehabilitation efficacy is maintained. \n\nBindawas et al. [17] aimed to investigate the association between LOS and functional \n\noutcomes among patients with stroke discharged from a rehabilitation facility in Saudi Arabia. \n\nThere were 409 adult patients in the study (age 18) admitted between 2008 and 2014, with no \n\ndeaths during the study period. Patients were divided into 4 different groups based on the days \n\n\n\n \n\n \n\n \n\n \n\n \n\n10 \n\n \n\nof rehabilitation: \u2264 30 days (n=114), 31\u201360 days (n=199), 61\u201390 days (n=72), and > 90 days \n\n(n=24). Multivariate regression analyses were used to evaluate functional outcomes using the \n\nFIM. The results of the study showed that higher FIM scores were significantly associated with \n\na LOS \u2264 30 days and 31\u201360 days, compared to > 90 days. The authors concluded that \u201ca short \n\nor intermediate LOS is not necessarily associated with worse outcomes, assuming adequate care \n\nis provided.\u201d [17]. \n\nArboix et al. [18] considered the identification of clinical predictors of prolonged hospital \n\nstay after acute stroke. They considered a long study period of 17 years for patients in Spain \n\nwho have had their first-ever ischemic stroke and primary intracerebral hemorrhage. Prolonged \n\nLOS stay was defined as LOS longer than 12 days after admission. The attributes considered \n\nincluded demographic data, cardiovascular risk factors, neuroimaging findings, clinical factors, \n\nand outcome. LR analysis was used to evaluate the independent influence of statistically \n\nsignificant variables in the duration of hospitalization. The results of 3,112 acute stroke patients \n\nshowed that prolonged hospital stay was recorded in 1,536 (49.4%) cases. Furthermore, males, \n\nlimb weakness, vascular complications, urinary complications, and infectious complications \n\nwere independently associated with longer LOS, whereas being symptom free at hospital \n\ndischarge and lacunar infarction were inversely associated with prolonged LOS. The authors \n\nconcluded that \u201cin-hospital medical complications (vascular, urinary, and infectious) are \n\nrelevant factors influencing duration of hospitalization after acute stroke. Therefore, prevention \n\nof potentially modifiable risk factors for medical complications is an important aspect of the \n\nearly management of patients who experienced stroke\u201d [18]. \n\nThe objective of the study conducted by Koton et al. [19] was to derive a simple score for the \n\nassessment of the risk of prolonged length of stay for acute stroke patients. Prolonged LOS was \n\ndefined as LOS \u2265 7 days. The results showed that the severity of stroke was the strongest \n\nmultivariable predictor of prolonged LOS. The study concluded that a simple prolonged LOS \n\nscore, based on available baseline information (stroke severity), may be useful for developing \n\npolicies aimed at better use of resources and optimal discharge planning of acute stroke patients \n\n[19]. \n\nHung et al. [20] aimed to consider the factors that influence LOS for stroke patients in \n\nTaiwan. The researchers explored how intravenous thrombolysis (IVT) affects LOS in an acute \n\ncare hospital setting. The study considered adult patients with ischemic stroke who presented \n\nwithin 48 hours of stroke onset. The relationship between IVT and prolonged length of stay \n\n(LOS \u2265 7 days) was studied by both classification and regression tree, as well as MLR analyses. \n\nFig. 7 illustrates the risk stratification for prolonged LOS by means of the classification and \n\nregression tree analysis. Among the study population of 3,054 patients, 1,110 presented within \n\n4.5 hours. The median LOS was 7 days (ranging from 4 to 11 days), and 1,619 patients had \n\nprolonged LOS. MLR revealed that IVT was an independent factor that reduced the risk of \n\nprolonged LOS, whereas age, NIHSS score, diabetes mellitus, and leukocytosis at admission \n\npredicted prolonged LOS. Decision tree analysis identified four variables (NIHSS score, IVT, \n\nleukocytosis at admission, and age) as important factors and they were used to partition the \n\npatients into six subgroups (see Fig. 7). The patient subgroup that had an NIHSS score of 5 to \n\n7 and received IVT had the lowest probability (19%) of prolonged LOS [20]. The authors \n\nconcluded that IVT minimized the risk of prolonged length of stay in patients with acute \n\nischemic stroke. They recommended that measures to increase the rate of IVT be encouraged. \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n11 \n\n \n\n \nFig. 7. Risk stratification for prolonged LOS by means of the classification \n\nand regression tree analysis [20]. \n\n 4. DISCUSSION AND CONCLUSIONS \n\nThe topic of hospital LOS is an important topic for hospital resource utilization and \n\noptimization [21][22]. Although many researchers have conducted research on LOS prediction, \n\nmore research is needed to further enrich this domain of study. We noticed from our literature \n\nreview that researchers occasionally reached contradicting conclusions about LOS for stroke \n\npatients. For example, some researchers (e.g., [1]) found that the age of the patient was not a \n\nsignificant predictor of prolonged LOS, whereas, others (e.g., [20]) concluded that a patient\u2019s \n\nage was an important predictor for LOS for stroke patients. This shows how LOS prediction is \n\na complex phenomenon that requires more studies and careful investigation. On the other hand, \n\nmost researchers (implicitly or explicitly) agreed that stroke severity (e.g., NIHSS) is a major \n\npredictor of LOS in stroke patients. \n\nAnother observation from our literature review is that not all researchers teamed up with \n\ndomain experts while conducting machine learning studies on LOS prediction. We view this as \n\na limitation in such studies because LOS prediction is a topic that is highly related to the medical \n\nfield as well as to patients\u2019 medical data and characteristics. This implies that in future research, \n\ndomain experts should be consulted before finalizing and publishing such LOS prediction \n\nstudies. Domain experts enrich the studies and make them more realistic. \n\nAnother point is that some researchers did not specifically mention the attributes that were \n\neffective in LOS prediction at the end of their studies. However, this particular information is \n\nhighly important for the readers of such research articles so that new research can build on \n\nprevious studies. We also recommend more future cooperation between machine learning \n\nresearchers in this field because of its importance in hospital resource utilization, decreasing \n\nhealthcare costs, and achieving healthier people and an overall healthier society.  \n\n\n\n \n\n \n\n \n\n \n\n \n\n12 \n\n \n\n \n\nREFERENCES \n\n[1] X. Zhang, H. Qiu, S. Liu, J. Li, and M. Zhou, \u201cPrediction of prolonged length of stay for stroke \n\npatients on admission for inpatient rehabilitation based on the international classification of \n\nfunctioning, disability, and health (ICF) generic set: A study from 50 centers in china,\u201d Medical \n\nScience Monitor: International Medical Journal of Experimental and Clinical Research, vol. 26, \n\npp. e918 811\u20131, 2020. \n\n[2] I. E. Livieris, I. F. Dimopoulos, T. Kotsilieris, and P. Pintelas, \u201cPredicting length of stay in \n\nhospitalized patients using ssl algorithms,\u201d in Proceed- ings of the 8th International Conference on \n\nSoftware Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion, \n\n2018, pp. 16\u201322. \n\n[3] A. R. Al Taleb, M. Hoque, A. Hasanat, and M. B. Khan, \u201cApplication of data mining techniques \n\nto predict length of stay of stroke patients,\u201d in 2017 International Conference on Informatics, \n\nHealth & Technology (ICIHT). IEEE, 2017, pp. 1\u20135. \n\n[4] S. Kabir and L. Farrokhvar, \u201cNon-linear feature selection for prediction of hospital length of stay,\u201d \n\nin 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA). \n\nIEEE, 2019, pp. 945\u2013950. \n\n[5] A. Azari, V. P. Janeja, and A. Mohseni, \u201cPredicting hospital length of stay (PHLOS): A multi-\n\ntiered data mining approach,\u201d in 2012 IEEE 12th International Conference on Data Mining \n\nWorkshops. IEEE, 2012, pp. 17\u201324. \n\n[6] I. Nouaouri, A. Samet, and H. Allaoui, \u201cEvidential data mining for length of stay (LOS) prediction \n\nproblem,\u201d in 2015 IEEE International Conference on Automation Science and Engineering \n\n(CASE). IEEE, 2015, pp. 1415\u20131420. \n\n[7]    R. Rathor and P. Agarkar, \u201cLosh prediction using data mining,\u201d Inter- national Journal of \n\nComputer Applications, vol. 119, no. 2, 2015. \n\n[8]    C. Neto, M. Brito, H. Peixoto, V. Lopes, A. Abelha, and J. Machado, \u201cPrediction of length of \n\nstay for stroke patients using artificial neural networks,\u201d in World Conference on Information \n\nSystems and Technolo- gies. Springer, 2020, pp. 212\u2013221. \n\n[9]    A. Minaeian, A. Patel, B. Essa, R. P. Goddeau Jr, M. Moonis, and N. Henninger, \u201cEmergency \n\ndepartment length of stay and outcome after ischemic stroke,\u201d Journal of Stroke and \n\nCerebrovascular Diseases, vol. 26, no. 10, pp. 2167\u20132173, 2017. \n\n[10] K.-C. Chang, M.-C. Tseng, H.-H. Weng, Y.-H. Lin, C.-W. Liou, and T.-Y. Tan, \u201cPrediction of \n\nlength of stay of first-ever ischemic stroke,\u201d Stroke, vol. 33, no. 11, pp. 2670\u20132674, 2002. \n\n\n\n \n\n \n\n \n\n \n\n \n\n13 \n\n \n\n[11] P. Appelros, \u201cPrediction of length of stay for stroke patients,\u201d Acta Neurologica Scandinavica, vol. \n\n116, no. 1, pp. 15\u201319, 2007. \n\n[12] A. N. Okere, C. M. Renier, and A. Frye, \u201cPredictors of hospital length of stay and readmissions \n\nin ischemic stroke patients and the impact of inpatient medication management,\u201d Journal of Stroke \n\nand Cerebrovascular Diseases, vol. 25, no. 8, pp. 1939\u20131951, 2016. \n\n[13] B. K. Choi, S. W. Ham, C. H. Kim, J. S. Seo, M. H. Park, and S. H. Kang, \u201cDevelopment of \n\npredictive model for length of stay (los) in acute stroke patients using artificial intelligence,\u201d \n\nJournal of Digital Convergence, vol. 16, no. 1, pp. 231\u2013242, 2018. \n\n[14] M. L. Svendsen, L. H. Ehlers, G. Andersen, and S. P. Johnsen, \u201cQuality of care and length of \n\nhospital stay among patients with stroke,\u201d Medical care, pp. 575\u2013582, 2009. \n\n[15] J. Garza-Ulloa, \u201cArtificial intelligence analysis using neural network to predict three stroke \n\nparameters: Surgery needed, treatment, and length of stay for rehabilitation.\u201d Unpublished. \n\n[16] Y. S. Ng, K. H. Tan, C. Chen, G. C. Senolos, E. Chew, and G. C. Koh, \u201cPredictors of acute, \n\nrehabilitation and total length of stay in acute stroke: a prospective cohort study,\u201d Ann Acad Med \n\nSingapore, vol. 45, no. 9, pp. 394\u2013403, 2016. \n\n[17] S. M. Bindawas, V. Vennu, H. Mawajdeh, H. M. Alhaidary, and E. Mof- tah, \u201cLength of stay and \n\nfunctional outcomes among patients with stroke discharged from an inpatient rehabilitation facility \n\nin saudi arabia,\u201d Medical science monitor: international medical journal of experimental and \n\nclinical research, vol. 24, p. 207, 2018. \n\n[18] A. Arboix, J. Massons, L. Garc\u00b4\u0131a-Eroles, C. Targa, M. Oliveres, and E. Comes, \u201cClinical predictors \n\nof prolonged hospital stay after acute stroke: relevance of medical complications,\u201d 2012. \n\n[19] S. Koton, N. Bornstein, R. Tsabari, D. Tanne et al., \u201cDerivation and validation of the prolonged \n\nlength of stay score in acute stroke patients,\u201d Neurology, vol. 74, no. 19, pp. 1511\u20131516, 2010. \n\n[20] L.-C. Hung, Y.-H. Hu, and S.-F. Sung, \u201cExploring the impact of intravenous thrombolysis on length \n\nof stay for acute ischemic stroke: a retrospective cohort study,\u201d BMC health services research, vol. \n\n15, no. 1, p. 404, 2015. \n\n[21] A. Alahmar, E. Mohammed, and R. Benlamri. \"Application of data mining techniques to predict \n\nthe length of stay of hospitalized patients with diabetes.\" In 2018 4th International Conference on \n\nBig Data Innovations and Applications (Innovate-Data), pp. 38-43. IEEE, 2018. \n\n[22] A. Alahmar and R. Benlamri, Optimizing Hospital Resources using Big Data Analytics with \n\nStandardized e-Clinical Pathways. In 2020 IEEE Intl Conf on Cloud and Big Data Computing, \n\n(CBDCom), pp. 650-657. IEEE, August 2020. \n\n \n\n \n\n\n"}
{"Title": "Knowledge intensive state design for traffic signal control", "Authors": "Liang Zhang, Qiang Wu, Jianming Deng", "Abstract": "  There is a general trend of applying reinforcement learning (RL) techniques for traffic signal control (TSC). Recently, most studies pay attention to the neural network design and rarely concentrate on the state representation. Does the design of state representation has a good impact on TSC? In this paper, we (1) propose an effective state representation as queue length of vehicles with intensive knowledge; (2) present a TSC method called MaxQueue based on our state representation approach; (3) develop a general RL-based TSC template called QL-XLight with queue length as state and reward and generate QL-FRAP, QL-CoLight, and QL-DQN by our QL-XLight template based on traditional and latest RL models.Through comprehensive experiments on multiple real-world datasets, we demonstrate that: (1) our MaxQueue method outperforms the latest RL based methods; (2) QL-FRAP and QL-CoLight achieves a new state-of-the-art (SOTA). In general, state representation with intensive knowledge is also essential for TSC methods. Our code is released on Github.      ", "Subject": "Machine Learning (cs.LG)", "ID": "arXiv:2201.00006", "Text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge intensive state design for traffic signal control\n\nLiang Zhang1, Qiang Wu2, Jianming Deng1*\n1 School of Life Sciences, Lanzhou University, Lanzhou 730000, China\n\n2 Institute of Fundamental and Frontier Sciences, University of Electronic Science and Technology of China, Chengdu\n611731, China\n\nAbstract\n\nThere is a general trend of applying reinforcement learning\n(RL) techniques for traffic signal control (TSC). Recently,\nmost studies pay attention to the neural network design and\nrarely concentrate on the state representation. Does the design\nof state representation has a good impact on TSC? In this pa-\nper, we (1) propose an effective state representation as queue\nlength of vehicles with intensive knowledge; (2) present a\nTSC method called MaxQueue based on our state representa-\ntion approach; (3) develop a general RL-based TSC template\ncalled QL-XLight with queue length as state and reward and\ngenerate QL-FRAP, QL-CoLight, and QL-DQN by our QL-\nXLight template based on traditional and latest RL models.\nThrough comprehensive experiments on multiple real-world\ndatasets, we demonstrate that: (1) our MaxQueue method out-\nperforms the latest RL based methods; (2) QL-FRAP and QL-\nCoLight achieves a new state-of-the-art (SOTA). In general,\nstate representation with intensive knowledge is also essential\nfor TSC methods. Our code is released on Github1.\nKeywords: intensive knowledge, traffic signal control, rein-\nforcement learning, state design, effective state\n\n1 Introduction\nWith population and economic growth, automobiles increase\nrapidly, and traffic congestion has become an emergent prob-\nlem. Traffic congestion causes fuel waste, environmental\npollution, economic losses, and waste of time. Mitigating\ntraffic congestion and improving transportation efficiency is\nof great urgency.\n\nIn many modern cities, FixedTime[5], GreenWave[12],\nSCOOT[4], and SCATS[7] are the most common traffic\nsignal control systems, which relys on pre-designed traffic\nsignal plans. These methods can\u2019t adapt to dynamic traf-\nfic flows. In addition, some traditional traffic signal control\n(TSC) methods such as MaxPressure[14] and SOTL[2] have\ngood performance but takes much efforts to deploy.\n\nRecently, reinforcement learning (RL) has drawn increas-\ning attention, and people have begun to use RL to solve TSC\nproblems. RL models can directly learn from the environ-\nment through trial-and-error without requiring assumptions\nlike traditional TSC methods. Furthermore, RL models can\n\n*Jianming Deng is the corresponding author. Email:\ndengjm@lzu.edu.cn\n\n1https:github.com/LiangZhang1996/QL XLight\n\nhandle complex and dynamic environments with a deep neu-\nral network[8]. RL-based TSC methods[16, 1, 16] become\na promising solution for realizing intelligent transportation\nsystems. PressLight[16] can realize large-scale traffic sig-\nnal control. Furthermore, MPLight[1] and CoLight[17] have\ndemonstrate the ability to handle city-level TSC. In addi-\ntion, MPLight uses a decentralized RL paradigm and is eas-\nier for large-scale deployment. PressLight[16] and MPLight\nall demonstrate the essential role of the state and reward de-\nsign.\n\nIn RL-based approach, the state representations vary in\nterms of queue length[9, 18], number of vehicles[18, 23, 20,\n16, 22, 20, 17, 19], traffic image[13, 18]; the reward rep-\nresentations vary in terms of queue length[22, 16, 17, 19] ,\npressure[1], total wait time[18, 9, 13, 19], and delay[18, 13,\n19]. Some methods can perform better with simple state and\nreward. However, some methods with complex state and re-\nward designs get limited results. However, most studies con-\ncentrate on developing novel network structures to improve\nTSC performance. Few of the studies have deeply explored\nwhy some methods have great performance with simple state\nand reward design. More attention should be paid to the state\ndesign for TSC.\n\nIn summary, the main contribution of this article as fol-\nlows:\n1. Propose an effective state representation as queue length\n\nwith intensive-knowledge;\n2. Propose one transportation method, namely MaxQueue,\n\nwhich has superior performance than previous state-of-\nthe-art RL methods;\n\n3. With effective state design, we develop an RL-based TSC\ntemplate: QL-XLight with queue length as state and re-\nward;\n\n4. Based on QL-XLight, we generate three RL-based meth-\nods: QL-DQN, QL-FRAP, QL-MPLight, which all have\nsuperior performance than the latest methods;\n\n5. Demonstrate that state design with intensive knowledge\nis as essential as network structure design.\n\nThe remainder of this paper is organized as follows:\nSection 2 introduces the related works, including typi-\ncal transprotation and RL-based approaches for TSC; Sec-\ntion 3 depicts the definitions of TSC; Section 4 system-\natically analyzes the typical used state representation with\n\nar\nX\n\niv\n:2\n\n20\n1.\n\n00\n00\n\n6v\n1 \n\n [\ncs\n\n.L\nG\n\n] \n 3\n\n0 \nD\n\nec\n 2\n\n02\n1\n\nhttps:github.com/LiangZhang1996/QL_XLight\n\n\nintensive-knowledge, and develop the MaxQueue algorithm\nand QL-XLight template. Section 5 conducts experiments\nand demonstrates the results. Section 6 concludes the paper\nand discusses future work.\n\n2 Related work\n2.1 Conventional transportation methods\nConventional transportation methods can be mainly catego-\nrized into the four categories: fixed-time control[5, 12], actu-\nated control[3, 2], adaptive control [4, 7], and optimization-\nbased control[14, 6, 10]. All the methods mentioned above\nhighly rely on expert knowledge. Fixed-time and adaptive\ncontrol rely on predefined signal plans (i.e., cycle length,\nphase split, and offset). Actuated control relies on the prede-\nfined threshold, which highly influences the control perfor-\nmance; optimization-based control also relies on the prede-\nfined signal plan (i.e., cycle length), turn ratios, and satura-\ntion flow rates. Therefore, these conventional transportation\nmethods have limited capacity to adapt to dynamic traffic.\n\n2.2 RL based methods\nRL models can learn their policy directly from environments\nthrough trial-and-error, and deep neural networks make\nthem adapt to various conditions. The RL-based method is a\npromising solution for traffic signal control. PressLight[16]\ncan achieve multi-intersection traffic signal control. MP-\nLight and CoLight can realize city-level TSC. HiLight also\nachieves superior performance than MPLight and CoLight\nwith hierarchical RL models. The state, reward, and neu-\nral network design play an essential role in RL models. We\nsummarize some typical RL-based methods and category\nthem into the following classes:\n\n\u2022 The methods that introduce the novel neural network\nstructure. GCN[9] adopt graph convolution neural net-\nwork for TSC control; IntelliLight[18] develops a com-\nplex neural network with phase selector; FRAP[22] uses\na modified network structure to capture the phase com-\npetition relation between different traffic movements;\nCoLight[17] uses graph attention networks to learn inter-\nsection cooperation; HiLight[19] adopts hierarchical RL\napproach for TSC.\n\n\u2022 The methods that introduce effective state and reward de-\nsign. PressLight[16] incorporates pressure into state and\nreward design; MPLight[1] uses traffic movement pres-\nsure as state, intersection pressure as reward.\n\nWe can know that most people try to develop an effective\nnetwork structure from the above. The concentration on state\ndesign is rare.\n\n2.3 State design\nIn other RL fields, the state representation is clear, and peo-\nple can directly use images as a state. However, in the TSC\nfield, the state is dynamic and complex, and the state rep-\nresentation should be dealt carefully. The state design for\nTSC has not been deeply studied. LIT[23] finds that: as a\nreward representation, queue length is better than delay; as a\n\nstate representation, number of vehicles is better than wait-\ning time and traffic image. PressLight finds pressure is better\nthan queue length as a reward representation. MPLight intro-\nduces pressure into state design and finds the improvement\nin the model. However, none of them systematically explain\nwhy some state and reward is better. The state representation\nfor TSC needs to be further discussed.\n\n3 Preliminary\nIn this section, we summarize the definition for recent TSC\nmethods[1, 17].\n\nFigure 1: The illustration of an intersection with four phases.\nIn this case, phase #2 is activated.\n\nDefinition 1 (Traffic network). Each traffic network is de-\nscribed as a directed graph, in which each node represents\nthe intersection, and each edge represents the road. Each\nroad consists of several lanes. An incoming lane for an inter-\nsection is where the vehicles enter the intersection. An out-\ngoing lane for an intersection is where the vehicles leave the\nintersection. We denote the set of incoming lanes and outgo-\ning lanes of intersection i as Lini and L\n\nout\ni respectively. We\n\nuse l,m, k to denote the lanes.\nDefinition 2 (Traffic movement). A traffic movement is\n\ndefined as the traffic traveling across an intersection towards\na certain direction, i.e., left turn, go straight, and right turn.\nFollowing the traffic rules in most cities, right turn traffic can\npass regardless of the signal, but it needs to yield on a red\nlight. In Figure 1 (a), there are 12 traffic movements.\n\nDefinition 3(Signal phase). Each signal phase is a set of\npermissible traffic movements, denoted by d, andDi denotes\nthe set of all the phases at intersection i. As shown in Figure\n1, the intersection has 4 phases with phase #2 activated.\n\nDefinition 4 (Phase queue length). The queue length of\neach phase is the sum queue length of the incoming lanes of\nthe phase, denoted by\n\nq(d) =\n\u2211\n\nq(l), l \u2208 d (1)\n\nin which q(l) is the queue length of lane l.\nDefinition 5 (Intersection queue length). The queue\n\nlength of each intersection is defined as the total queue\n\n\n\nlength of the incoming lanes of the intersection, denoted by\n\nQi =\n\u2211\n\nq(l), l \u2208 Lini (2)\n\nin which q(l) represents the queue length of lane l.\nDefinition 6 (Phase duration). The minimum duration for\n\neach phase is denoted by tduration. It can also represent the\naction interval of RL-based models.\n\nProblem (Multi-intersection traffic signal control). Each\nintersection is controlled by a RL agent. At time step t,\nagent i views the environment as its observation oti. Every\ntduration, the action ati is taken to control the signal of inter-\nsection i. The goal of the agent is to take an optimal action\nati (i.e. which phase to set) to maximize the throughput of\nthe systems and minimize the average travel time.\n\nTable 1: Summary of notation.\n\nNotation Meaning\n\nLini set of incoming lanes of intersection i\nLouti set of outgoing lanes of intersection i\nl,m, k lanes\nq(l) queue length of lane l\nd signal phase which is set of traffic movements\nDi set of all phases at intersection i\nQi total queue length of intersection i\n\ntduration minimal phase duration or said action interval\n\n4 Method\nIn this section, we first propose an effective state representa-\ntion as queue length with intensive knowledge. Next, we dis-\ncuss why queue length is more effective than some typical\nused state representation. Then, we propose a transportation\nmethod MaxQueue based on intensive knowledge inspired\nby MaxPressure. Finally, we develop an RL-based TSC tem-\nplate: QL-XLight and generate QL-DQN, QL-FRAP, and\nQL-COLight.\n\n4.1 Queue length as the state\nFor TSC, each vehicle in the traffic network has two states:\nrunning and queueing. Queueing vehicles can directly re-\nsult in congestion while running vehicles potentially result\nin congestion. Almost all the queueing vehicles stop near\nthe intersection and have the demand for a green signal.\nMP[14] maximizes the throughput of the traffic by balanc-\ning the queue length in the network. The queueing vehicles\nplay an essential role in the traffic condition representation.\nFrom empirical knowledge, the queue length is considered\nadequate.\n\nIn the traffic network, the phase signal can only directly\nchange the state of the queuing vehicles. Any consequent\nchanges such as the number of vehicles, vehicle position,\nspeed score are full of uncertainty. Therefore, we choose to\nuse queue length as the traffic state representation.\n\nDiscussion According to the existing studies, various state\nrepresentations are used in TSC, while some state represen-\ntation is more effective than others. We will summarize the\ntypically used state representations and give a systematical\nanalysis to answer which state is a effective traffic state rep-\nresentation.\n\nThe RL agents learn from the environment through trial-\nand-error and learn the state-action value through explo-\nration. Suppose the state representation does not include crit-\nical contents of traffic movement. In that case, the agent will\nbe confused about the state and can\u2019t learn an appropriate\npolicy.\n\nIf one phase is activated, the queue length of the phase\nchanges to zero, while the queue length for other phases may\ngrow gradually, depending on the arrival from upstream.\nThere is a deterministic change when each phase is acti-\nvated, and is considered effective.\n\nThen, we analyze the following traffic state representation\nand explain why they are as effective as queue length.\n\n\u2022 Number of vehicles: it is described as the total vehicle\nnumber of the incoming lanes. If one phase is activated,\nthe vehicles near the intersection pass through gradually,\nbut vehicles also arrive gradually from upstream. The to-\ntal number of the corresponding lanes probably:(1) be-\ncomes larger if the number of entering is larger than exit-\ning;(2) do not change if the number of entering is equal to\nexit; (3) become smaller if a number of entering smaller\nthan exiting. In addition, if there is no vehicle near the in-\ntersection, the traffic state can\u2019t change even if the phase\nchanges. Therefore, the change of state is vague and can\u2019t\nbe explicitly captured, which makes the agent \u201dconfus-\ning.\u201d\n\n\u2022 Vehicle position. The position of vehicles is usually in-\ntegrated as an image representation, which is defined as\na matrix, with \u201d1\u201d indicating the presence of vehicles on\na location, and \u201d0\u201d the absence of vehicles on that loca-\ntion. Each lane is usually divided into small segments,\nand some use the total vehicle number to replace \u201d1\u201d and\n\u201d0\u201d. This is similar to a number of vehicles that do not\nhave explicit changes after one phase be activated.\n\n\u2022 Speed score. The speed score is calculated by the aver-\nage speed divided by the speed limit. If one phase is acti-\nvated, the speed score change degree relies on the accel-\neration. In addition, if there are only queueing vehicles,\nthe speed score grows proportional to the acceleration; if\nthere are lots of running vehicles and few queueing vehi-\ncles, the speed score may change, not obvious. It is also\nconfusing for the RL agents.\n\n\u2022 Traffic movement pressure calculated by a number of ve-\nhicles. It is calculated by a number of vehicles and has\nsimilar properties to it.\n\n4.2 MaxQueue control\nBased on MaxPressure[14] and the property of queue length,\nwe propose a TSC method called MaxQueue. Like MaxPres-\nsure, MaxQueue control selects the phase with maximum\nqueue length in a greedy manner. At intersection i, the phase\n\n\n\nqueue length is calculated (by equation(1)), then activate the\nphase with maximum pressure every tduration, denoted by\n\nd\u0302 = argmax (q(d))|d \u2208 Di) (3)\nThe MaxQueue method is formally summarized in Algo-\nrithm 1.\n\nAlgorithm 1: MaxQueue Control\nParameter: Current phase time t, minimum phase duration\ntduration\n\nfor (time step) do\nt = t+ 1;\nif t = tduration then\n\nFor each intersection, get q(d) by equation (1);\nActivate the phase according to equation (3);\nt = 0\n\nend if\nend for\n\nComparison of MaxQueue and MaxPressure MaxPres-\nsure control selects the phase with maximum pressure,\nwhich is the difference of queue length between upstream\nand downstream, indicating the balance of the queue length.\nOnly consider the control logic, MaxQueue(MQ) and Max-\nPressure(MP) are highly similar, and both use a greedy man-\nner to select the phase. For the case of single intersection\ncontrol, MP and MQ are the same. There are no queueing\nvehicles on the outgoing lanes of the single intersection be-\ncause it is assumed that the outgoing is infinite. Thus, the\ncalculated pressure is exactly the queue length.\n\nMP considers the neighbor influence, stabilizes the queue\nlength, and maximizes the throughput by selecting the phase\nwith maximum pressure. The key idea of MP is that ensure\nthe vehicles can\u2019t be stopped by the queue vehicle of the up-\nstream. Therefore, if a phase has a large pressure, the queue\nlength can only be larger. The MP method is really effective\nwhen the traffic road length is small because the neighbor\nvehicles can fast influence the current intersection.\n\nHowever, when the traffic road length is longer, the influ-\nence may come after several tduration, and the pressure is\nnot effective. For example, set tduration = 15s and vehi-\ncles\u2019 maximum velocity is 10m/s; if the road is 100m, then\nit takes 10s to the neighbor, and the neighbor condition in-\nfluences the policy; if the road is 300m, then it takes at least\n30s to the neighbor, and the policy can\u2019t be influenced by the\nneighbor condition.\n\nIn summary, if the traffic road is relatively long, the MQ\nwill perform better; if the traffic road is relatively short, the\nMP will perform better.\n\n4.3 QL-XLight\nWe develop an RL-based TSC methods template with queue\nlength as the traffic state and reward, QL-XLight. Based on\nQL-XLight, DQN, FRAP, and CoLight are introduced as\nthe based model, and we get QL-DQN, QL-FRAP, and QL-\nCoLight.\n\u2022 State The current phase and queue length are used as the\n\nstate representation(agent observations).\n\n\u2022 Action At time t, each agent choose a phase d\u0302 according\nto the state, and the traffic signal will be changed to d\u0302.\n\n\u2022 Reward Negative intersection queue length is used as the\nreward. The reward for the agent controlling intersection\ni is denoted by\n\nri = \u2212Qi = \u2212|\n\u2211\n\nq(l)|, l \u2208 Lini (4)\n\nin which q(l) is the queue length at lane l. By maximizing\nthe reward, the agent is trying to maximize the through-\nput in the system.\n\nDeep Q-learning The DQN agents are updated by the\nBellman Equation:\n\nQ(st, at) = R(st, at) + \u03b3maxQ(st+1, at+1) (5)\n\nin which st and st+1 are the state, at and at+1 are the action.\n\nBase model The following base models are introduced to\nget QL-DQN, QL-FRAP, QL-CoLight:\n\u2022 DQN based model. A simple DQN[8] with only two\n\nfully connected layers. The neural network structure is\nstraightforward and basic. Besides, we also adopt the de-\ncentralized approach from MPLight to train the model.\nWe refer to a simple DQN based approach as QL-DQN.\n\n\u2022 FRAP-based model. FRAP[22] is adopted as one of the\nbase models. FRAP can learn the phase competition\nin TSC with a specially designed architecture. It has a\nfast training process compared with other TSC methods.\nFRAP has been used as the base model by MPLight. We\nrefer to FRAP based approach as QL-FRAP.\n\n\u2022 CoLight based model. CoLight[17] is graph attention\nnetwork[15] based method, and learns intersection com-\nmunication and cooperation for TSC. CoLight is capable\nof large-scale TSC. We will adopt CoLight as one of the\nbase models. We refer to CoLight based model in this\narticle as QL-CoLight.\n\nTheoretically, we could build QL-LIT and QL-HiLight.\nHowever, because the code of HiLight is not available,\nwe implement QL-FRAP, QL-CoLight, and QL-DQN first,\nwithout the loss of validity of our conclusion.\n\nParameter Sharing Parameters of the network are shared\namong all the agents. It is essential to improve model\nperformance[1]. Besides, the replay memory is also shared\nso that all the intersections can benefit from the experiences\nof others. Note that the CoLight based model does not need\nparameter sharing. Some baseline models are also trained\nunder parameter sharing for fair model comparison.\n\nDiscussion We are not the first that introduce queue\nlength into both state and reward, but we are the first to\npropose queue length as an effective state representation.\nIntelliLight[18] uses complex state and reward representa-\ntion apart from queue length. GCN[9] uses queue length and\naverage velocity as state, total wait time as a reward. Tan et\nal.[11] uses queue length as state, queue length and a num-\nber of running vehicles as a reward. Although these studies\nhave used queue length as state representation, they do not\nemphasize queue length property.\n\n\n\nThe results of FRAP[22] and CoLight[17] demonstrates\nthe poor performance of IntelliLight and GCN. In addition,\nthe reward in [11] indicates the smaller queue length and\nrunning number, the better results, which is unreasonable\nbecause for the number of running vehicles, the larger, the\nbetter. Therefore, the reward is also essential for RL-based\nTSC, and only queue length is more reliable than that used\nin IntelliLight, GCN, and [11].\n\n5 Experiment\n5.1 Settings\nSimulator We conduct the experiments on an open-source\nsimulator called CityFlow2[21], which supports large-scale\ntraffic signal control and has faster speed than SUMO.\nThe simulator provides the environment observations to the\nagent and receives the command from the agent. In the ex-\nperiments, each green signal is followed by three-second\nyellow time and two-second all red time to prepare the tran-\nsition.\n\nTable 3: Average arrival rate of the two datasets\n\nDataset Arrival rate(vehicles/s)\nDJiNan1 1.75\nDJiNan2 1.21\nDJiNan3 1.53\nDHangZhou1 0.83\nDHangZhou2 1.94\n\nDatasets We use five real-world datasets3 in the exper-\niment, three from Jinan and two from Hangzhou. These\ndatasets have been wildly used by various methods such as\nMPLight, CoLight, and HiLight.\n\nEach traffic dataset consists of two parts: (1) traffic road-\nnet dataset; (2) traffic flow dataset. The traffic road-net\ndataset describes the traffic network, including lanes, roads,\nand intersections. The traffic flow dataset contains vehicles\ntravel information, which is described as (t, u), where t is\nthe time that each vehicle starts entering the traffic network,\nu is the pre-planned route from its original location to desti-\nnation.\n\n\u2022 Jinan datasets: The road network has 12 intersections\n(3 \u00d7 4). Each intersection is four-way, with two 400-\nmeter road segments (East-West) and two 800-meter\nroad segments (South-North). There are three traffic flow\ndatasets, and they have different average arrival rates (Ta-\nble 3).\n\n\u2022 Hangzhou datasets. The road network has 16 intersec-\ntions (4 \u00d7 4). Each intersection is four-way, with two\n800-meter road segments (East-West) and two 600-meter\nroad segments (South-North). There are two traffic flow\ndatasets, and they also have different average arrival rates\n(Table 3).\n2https://cityflow-project.github.io\n3https://traffic-signal-control.github.io\n\nEvaluation metric Based on existing studies in traffic sig-\nnal control[17], we choose average travel time as the eval-\nuation metric, which is the mostly used metric to evaluate\ncontrol performance in the TSC. The travel time of each ve-\nhicle is the time speed between entering and leaving the traf-\nfic network. We use all the vehicles\u2019 average travel time to\nevaluate the model performance.\n\nCompared methods We compare our methods with the\nfollowing baseline methods, including both transportation\nand RL methods. For a fair comparison, the phase num-\nber is set as four, and the action interval (phase duration) is\nset as 15 seconds. All the RL methods are learned with the\nsame hyper-parameters. Each episode is a 60-minutes sim-\nulation, and we adopt one result as the average of the last\nten episodes of testing. Each reported result is the average\nof three independent results.\n\nTransportation Methods:\n\u2022 Fixed-Time[5]: a policy uses fixed cycle length with pre-\n\ndefined phase split among all the phases.\n\u2022 Max-Pressure[14]: the max-pressure control selects the\n\nphase with maximum pressure.\nRL Methods:\n\n\u2022 PressLight[16]: incorporates pressure in the state and\nreward design for the RL model and has shown supe-\nrior performance in multi-intersection control problems.\nPressLight is trained with parameter sharing for fairly\ncomparison.\n\n\u2022 FRAP[22]: uses a novel network structure to cap-\nture phase competition relation between different traffic\nmovements. FRAP is trained with parameter sharing like\nMPLight for fair comparison.\n\n\u2022 MPLight[1]: a FRAP[22] based decentralized model, in-\ncorporates pressure in the state and reward design and has\nshown superior performance in city-level TSC. It is one\nstate-of-the-art RL-based TSC method.\n\n\u2022 CoLight[17]: another state-of-the-art method uses a\ngraph attention network to realize intersection coopera-\ntion and has shown superior performance in large-scale\nTSC.\n\nOur Proposed Methods:\n\u2022 MaxQueue: the MaxQueue control selects the phase\n\nwith maximum queue length.\n\u2022 QL-DQN: adopts a two-layer network as the base model,\n\nuses queue length and current phase as state, intersection\nqueue length as a reward.\n\n\u2022 QL-FRAP: a FRAP-based model, uses queue length and\ncurrent phase as state, intersection queue length as a re-\nward.\n\n\u2022 QL-CoLight: a CoLight based model, uses queue length\nand current phase as state, intersection queue length as a\nreward.\n\n5.2 Overall Performance\nTable 2 reports our experimental results under JiNan and\nHangZhou real-world datasets with respect to the average\ntravel time. We have the following findings:\n\n\n\nTable 2: Overall performance. For average travel time, the smaller the better.\n\nMethod\nJiNan HangZhou\n\n1 2 3 1 2\nFixedTime 428.11(+56.29%) 368.77(+50.29%) 383.01(+55.82%) 495.57(+71.75%) 406.65(+16.53%)\nMaxPressure 273.96 245.38 245.81 288.54 348.98\nPressLight 314.63(+14.85%) 264.62(+7.84%) 258.12(+5.01%) 385.71(+33.68%) 458.12(+31.27%)\nFRAP 296.46(+8.21%) 266.93(+8.78%) 269.64(+9.69%) 309.60(+7.30%) 356.47(+2.15%)\nMPLight 297.46(+8.58%) 270.05(+10.05%) 276.15(+12.34%) 314.60(+9.03%) 357.61(+2.47%)\nCoLight 272.06(\u22120.69%) 252.44(+2.88%) 249.56(+1.53%) 297.02(+2.94%) 347.27(\u22120.49%)\nMaxQueue 268.21(\u22122.10%) 238.91(\u22122.64%) 237.8(\u22123.26%) 283.12(\u22121.88%) 324.38(\u22127.05%)\nQL-DQN 260.74(\u22124.83%) 245.32(0.02%) 239.33(\u22122.64%) 284.74(\u22121.32%) 333.44(\u22124.45%)\nQL-FRAP 255.53(\u22126.73%) 238.74(\u22122.71%) 236.04(\u22123.97%) 282.28(\u22122.17%) 315.03(\u22129.73%)\nQL-CoLight 254.94(\u22126.94%) 239.05(\u22122.58%) 236.25(\u22123.89%) 282.17(\u22122.21%) 322.75(\u22127.52%)\n\n(1) Our proposed MaxQueue consistently outperforms all\nother previous methods. MaxQueue has a significant im-\nprovement as a conventional transportation method com-\npared to MaxPressure. In addition, MaxQueue has superior\nperformance than MPLight and CoLight. The conventional\ntransportation methods are still powerful.\n\n(2) Our proposed QL-DQN, QL-FRAP, and QL-CoLight\noutperform all other previous methods. With only changing\nthe state and reward compared to MPLight and CoLight, the\nimprovement of QL-FRAP and QL-CoLight is significant,\nproving the importance of state representation for RL-based\nTSC.\n\n(3) QL-FRAP and QL-CoLight are state-of-the-art among\ntraditional and RL-based TSC methods. CoLight and MP-\nLight are the previous state-of-the-art methods, and QL-\nFRAP and QL-CoLight have a better performance. Besides,\nQL-XLight only uses queue length information of a particu-\nlar intersection, which has the advantage of deployment than\nCoLight and MPLight.\n\n(4) Parameter sharing is essential for RL-based models.\nMPLight[1] has shown better performance than FRAP and\naddresses the importance of parameter sharing. However,\nwhen FRAP is trained with parameter sharing same to MP-\nLight, it has slightly better performance than MPLight.\n\n5.3 State representation is also essential\n\nBoth the neural network structure and the state representa-\ntion play an important role in the performance improvement.\nHowever, most studies pay attention to the network design.\nWe will demonstrate that the state representation is also es-\nsential.\n\nQL-DQN uses a simple neural network structure, but\neffective state representation. FRAP and CoLight use ad-\nvanced neural network structure, but the state representation\nis not effective. In addition, FRAP, CoLight, and QL-DQN\nuse the same reward. Compare the performance of QL-DQN\nwith FRAP and CoLight, QL-DQN is consistently better\nover all the datasets.\n\nTherefore, we can conclude that state representation is\nalso essential as neural network structure for TSC. The state\nrepresentation should be paid more attention in TSC.\n\nFigure 2: Model performance under different phase duration.\n\n5.4 Performance under different phase duration\nExperiments are also conducted under different phase du-\nration for further model comparison. Figure 2 reports the\nmodel performance under different phase duration. QL-\nDQN, QL-FRAP, and QL-CoLight consistently perform bet-\nter than CoLight and MPLight over all the datasets and\nphase duration. MaxQueue performs better than MPLight\nand CoLight in most cases, except at JiNan1 and JiNan3\nwhen tduration = 10s. MaxQueue also perform better than\nQL-DQN in most cases. The performance of MaxQueue ad-\ndresses that the transportation method is still powerful and\nessential.\n\n5.5 Reward comparison\nPressLight and MPLight have demonstrated that RL ap-\nproaches perform better under pressure than queue length.\nWe will re-test the impact of reward settings with queue\nlength as the state representation. The FRAP and CoLight\nare used as the base model; experiments are conducted un-\nder the following configurations:\n\n\u2022 Config1: queue length and current phase are used as the\nstate, intersection queue length as the reward. This is ex-\nactly QL-XLight.\n\n\u2022 Config2: queue length and current phase are used as the\nstate, intersection pressure as the reward.\n\nExperiments are conducted over all the datasets, and Fig-\nure 3 reports the model performance with a different reward.\n\n\n\nFigure 3: Model performance under different reward w.r.t\naverage travel time, the smaller the better.\n\nQL-FRAP performs better under queue length than pressure.\nThe performance difference is not promising. QL-CoLight\nhas significantly better performance under queue length than\npressure. The CoLight based model has poor performance\nunder pressure, maybe the property of GAT that already con-\nsiders the neighbor influence and optimizes the global queue\nlength in the network.\n\nConsidering the calculation of state and reward, the queue\nlength is easier to get because pressure requires complex\ncalculation and neighbor information. Queue length can di-\nrectly get from the traffic environment. In summary, using\nthe queue length as state and reward is a better choice.\n\n6 Conclusion\nIn this paper, we propose an effective state representation\nas queue length. Based on queue length, we developed a\ntransportation method: MaxQueue and an RL template: QL-\nXLight. Our proposed methods have demonstrated supe-\nrior performance than the previous state-of-the-art method,\nand QL-CoLight achieves state-of-the-art performance. The\nimportance of transportation methods is also addressed by\nMaxQueue. The comparison of QL-DQN with FRAP and\nCoLight demonstrates that state representation is as essen-\ntial as a neural network structure. In a word, we should pay\nmore attention to the state design apart from the neural net-\nwork design.\n\nHowever, only queue length as the state representation is\nnot enough for the complex traffic conditions, and more in-\nformation about the traffic conditions should be added to\nthe state representation. In future research, we will try to\nadd more information about the traffic conditions to the RL\nagent observations. In addition, a more complex reward and\nnovel network structure are also taken into consideration to\nimprove the performance of TSC.\n\n7 Acknowledgments\nReferences\n\n[1] Chen, C.; Wei, H.; Xu, N.; Zheng, G.; Yang, M.;\nXiong, Y.; Xu, K.; and Li, Z. 2020. Toward a thousand\nlights: Decentralized deep reinforcement learning for\nlarge-scale traffic signal control. In Proceedings of the\n\nAAAI Conference on Artificial Intelligence, volume 34,\n3414\u20133421.\n\n[2] Cools, S.-B.; Gershenson, C.; and D\u2019Hooghe, B. 2013.\nSelf-organizing traffic lights: A realistic simulation. In\nAdvances in applied self-organizing systems, 45\u201355.\nSpringer.\n\n[3] Gershenson, C. 2004. Self-organizing traffic lights.\narXiv preprint nlin/0411066.\n\n[4] Hunt, P.; Robertson, D.; Bretherton, R.; and Royle,\nM. C. 1982. The SCOOT on-line traffic signal op-\ntimisation technique. Traffic Engineering & Control,\n23(4).\n\n[5] Koonce, P.; and Rodegerdts, L. 2008. Traffic signal\ntiming manual. Technical report, United States. Fed-\neral Highway Administration.\n\n[6] Le, T.; Kova\u0301cs, P.; Walton, N.; Vu, H. L.; Andrew,\nL. L.; and Hoogendoorn, S. S. 2015. Decentralized\nsignal control for urban road networks. Transportation\nResearch Part C: Emerging Technologies, 58: 431\u2013\n450.\n\n[7] Lowrie, P. 1992. SCATS: A traffic responsive method\nof controlling urban traffic control. Roads and Traffic\nAuthority.\n\n[8] Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.;\nVeness, J.; Bellemare, M. G.; Graves, A.; Ried-\nmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al.\n2015. Human-level control through deep reinforce-\nment learning. nature, 518(7540): 529\u2013533.\n\n[9] Nishi, T.; Otaki, K.; Hayakawa, K.; and Yoshimura,\nT. 2018. Traffic signal control based on reinforce-\nment learning with graph convolutional neural nets.\nIn 2018 21st International conference on intelligent\ntransportation systems (ITSC), 877\u2013883. IEEE.\n\n[10] Sun, X.; and Yin, Y. 2018. A simulation study on max\npressure control of signalized intersections. Trans-\nportation research record, 2672(18): 117\u2013127.\n\n[11] Tan, T.; Bao, F.; Deng, Y.; Jin, A.; Dai, Q.; and Wang,\nJ. 2019. Cooperative deep reinforcement learning for\nlarge-scale traffic grid signal control. IEEE transac-\ntions on cybernetics, 50(6): 2687\u20132700.\n\n[12] To\u0308ro\u0308k, J.; and Kerte\u0301sz, J. 1996. The green wave\nmodel of two-dimensional traffic: Transitions in the\nflow properties and in the geometry of the traffic jam.\nPhysica A: Statistical Mechanics and its Applications,\n231(4): 515\u2013533.\n\n[13] Van der Pol, E.; and Oliehoek, F. A. 2016. Coordi-\nnated deep reinforcement learners for traffic light con-\ntrol. Proceedings of Learning, Inference and Control\nof Multi-Agent Systems (at NIPS 2016).\n\n[14] Varaiya, P. 2013. Max pressure control of a network of\nsignalized intersections. Transportation Research Part\nC: Emerging Technologies, 36: 177\u2013195.\n\n[15] Velic\u030ckovic\u0301, P.; Cucurull, G.; Casanova, A.; Romero,\nA.; Lio, P.; and Bengio, Y. 2017. Graph attention net-\nworks. arXiv preprint arXiv:1710.10903.\n\n\n\n[16] Wei, H.; Chen, C.; Zheng, G.; Wu, K.; Gayah, V.; Xu,\nK.; and Li, Z. 2019. Presslight: Learning max pres-\nsure control to coordinate traffic signals in arterial net-\nwork. In Proceedings of the 25th ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery & Data\nMining, 1290\u20131298.\n\n[17] Wei, H.; Xu, N.; Zhang, H.; Zheng, G.; Zang, X.; Chen,\nC.; Zhang, W.; Zhu, Y.; Xu, K.; and Li, Z. 2019. Co-\nlight: Learning network-level cooperation for traffic\nsignal control. In Proceedings of the 28th ACM In-\nternational Conference on Information and Knowledge\nManagement, 1913\u20131922.\n\n[18] Wei, H.; Zheng, G.; Yao, H.; and Li, Z. 2018. In-\ntellilight: A reinforcement learning approach for intel-\nligent traffic light control. In Proceedings of the 24th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, 2496\u20132505.\n\n[19] Xu, B.; Wang, Y.; Wang, Z.; Jia, H.; and Lu, Z. 2021.\nHierarchically and Cooperatively Learning Traffic Sig-\nnal Control. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, 669\u2013677.\n\n[20] Zang, X.; Yao, H.; Zheng, G.; Xu, N.; Xu, K.; and Li,\nZ. 2020. Metalight: Value-based meta-reinforcement\nlearning for traffic signal control. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 34,\n1153\u20131160.\n\n[21] Zhang, H.; Feng, S.; Liu, C.; Ding, Y.; Zhu, Y.; Zhou,\nZ.; Zhang, W.; Yu, Y.; Jin, H.; and Li, Z. 2019.\nCityflow: A multi-agent reinforcement learning envi-\nronment for large scale city traffic scenario. In The\nWorld Wide Web Conference, 3620\u20133624.\n\n[22] Zheng, G.; Xiong, Y.; Zang, X.; Feng, J.; Wei, H.;\nZhang, H.; Li, Y.; Xu, K.; and Li, Z. 2019. Learning\nphase competition for traffic signal control. In Pro-\nceedings of the 28th ACM International Conference on\nInformation and Knowledge Management, 1963\u20131972.\n\n[23] Zheng, G.; Zang, X.; Xu, N.; Wei, H.; Yu, Z.; Gayah,\nV.; Xu, K.; and Li, Z. 2019. Diagnosing reinforce-\nment learning for traffic signal control. arXiv preprint\narXiv:1905.04716.\n\n\n\t1 Introduction\n\t2 Related work\n\t2.1 Conventional transportation methods\n\t2.2 RL based methods\n\t2.3 State design\n\n\t3 Preliminary\n\t4 Method\n\t4.1 Queue length as the state\n\t4.2 MaxQueue control\n\t4.3 QL-XLight\n\n\t5 Experiment\n\t5.1 Settings\n\t5.2  Overall Performance \n\t5.3 State representation is also essential\n\t5.4 Performance under different phase duration\n\t5.5 Reward comparison\n\n\t6 Conclusion\n\t7 Acknowledgments\n\n"}
{"Title": "Confidence-Aware Multi-Teacher Knowledge Distillation", "Authors": "Hailin Zhang, Defang Chen, Can Wang", "Abstract": "  Knowledge distillation is initially introduced to utilize additional supervision from a single teacher model for the student model training. To boost the student performance, some recent variants attempt to exploit diverse knowledge sources from multiple teachers. However, existing studies mainly integrate knowledge from diverse sources by averaging over multiple teacher predictions or combining them using other various label-free strategies, which may mislead student in the presence of low-quality teacher predictions. To tackle this problem, we propose Confidence-Aware Multi-teacher Knowledge Distillation (CA-MKD), which adaptively assigns sample-wise reliability for each teacher prediction with the help of ground-truth labels, with those teacher predictions close to one-hot labels assigned large weights. Besides, CA-MKD incorporates intermediate layers to further improve student performance. Extensive experiments show that our CA-MKD consistently outperforms all compared state-of-the-art methods across various teacher-student architectures.      ", "Subject": "Machine Learning (cs.LG)", "ID": "arXiv:2201.00007", "Text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCONFIDENCE-AWARE MULTI-TEACHER KNOWLEDGE DISTILLATION\n\nHailin Zhang Defang Chen Can Wang?\n\nZhejiang University, China.\n{zzzhl, defchern, wcan}@zju.edu.cn\n\nABSTRACT\n\nKnowledge distillation is initially introduced to utilize addi-\ntional supervision from a single teacher model for the student\nmodel training. To boost the student performance, some re-\ncent variants attempt to exploit diverse knowledge sources\nfrom multiple teachers. However, existing studies mainly in-\ntegrate knowledge from diverse sources by averaging over\nmultiple teacher predictions or combining them using other\nvarious label-free strategies, which may mislead student in\nthe presence of low-quality teacher predictions. To tackle\nthis problem, we propose Confidence-Aware Multi-teacher\nKnowledge Distillation (CA-MKD), which adaptively assigns\nsample-wise reliability for each teacher prediction with the\nhelp of ground-truth labels, with those teacher predictions\nclose to one-hot labels assigned large weights. Besides, CA-\nMKD incorporates intermediate layers to further improve\nstudent performance. Extensive experiments show that our\nCA-MKD consistently outperforms all compared state-of-the-\nart methods across various teacher-student architectures.\n\nIndex Terms\u2014 knowledge distillation, multiple teachers,\nconfidence-aware weighting\n\n1. INTRODUCTION\n\nNowadays, deep neural networks have achieved unprece-\ndented success in various applications [1, 2, 3]. However,\nthese complex models requiring huge memory footprint and\ncomputational resources are difficult to be applied on embed-\nded devices. Knowledge distillation (KD) is thus proposed as\na model compression technique to resolve this issue, which\nimproves the accuracy of a lightweight student model by dis-\ntilling the knowledge from a pre-trained cumbersome teacher\nmodel [4]. The transferred knowledge was originally formal-\nized as softmax outputs (soft targets) of the teacher model\n[4] and latter extended to the intermediate teacher layers for\nachieving more promising performance [5, 6, 7].\n\nAs the wisdom of the masses exceeds that of the wisest in-\ndividual, some multi-teacher knowledge distillation (MKD)\n\n?Corresponding author\nThis work is supported by National Key R&D Program of China (Grant\n\nNo: 2019YFB1600700) and National Natural Science Foundation of China\n(Grant No: U1866602).\n\nFig. 1. Comparison of the previous average direction (green\nline) and our proposed confidence-aware direction (red line).\n\nmethods are proposed and have been proven to be benefi-\ncial [8, 9, 10, 11, 12]. Basically, they combine predictions\nfrom multiple teachers with the fixed weight assignment [8,\n9, 10] or other various label-free schemes, such as calculat-\ning weights based on a optimization problem or entropy crite-\nrion [11, 12], etc. However, fixed weights fail to differentiate\nhigh-quality teachers from low-quality ones [8, 9, 10], and\nthe other schemes may mislead the student in the presence of\nlow-quality teacher predictions [11, 12]. Figure 1 provides an\nintuitive illustration on this issue, where the student trained\nwith the average weighting strategy might deviate from the\ncorrect direction once most teacher predictions are biased.\n\nFortunately, we actually have ground-truth labels in hand\nto quantify our confidence about teacher predictions and then\nfilter out low-quality predictions for better student training.\nTo this end, we propose Confidence-Aware Multi-teacher\nKnowledge Distillation (CA-MKD) to learn sample-wise\nweights by taking the prediction confidence of teachers into\nconsideration for adaptive knowledge integration. The con-\nfidence is obtained based on the cross entropy loss between\nprediction distributions and ground-truth labels. Compared\nwith previous label-free weighting strategies, our technique\nenables the student to learn from a relatively correct direction.\n\nNote that our confidence-aware mechanism not only is\nable to adaptively weight different teacher predictions based\non their sample-wise confidence, but also can be extended to\nthe student-teacher feature pairs in intermediate layers. With\nthe help of our generated flexible and effective weights, we\ncould avoid those poor teacher predictions dominating the\nknowledge transfer process and considerably improve the stu-\ndent performance on eight teacher-student architecture com-\nbinations (as shown in Table 1 and 3).\n\nar\nX\n\niv\n:2\n\n20\n1.\n\n00\n00\n\n7v\n1 \n\n [\ncs\n\n.L\nG\n\n] \n 3\n\n0 \nD\n\nec\n 2\n\n02\n1\n\n\n\n2. RELATED WORK\n\nKnowledge Distillation. Vanilla KD aims to transfer knowl-\nedge from a complex network (teacher) to a simple network\n(student) with the KL divergence minimization between their\nsoftened outputs [13, 4]. Mimicking the teacher representa-\ntions from intermediate layers was latter proposed to explore\nmore knowledge forms [5, 6, 14, 15, 7]. Compared to these\nmethods that require pre-training a teacher, some works si-\nmultaneously train multiple students and encourage them to\nlearn from each other instead [16, 17]. Our technique dif-\nfers from these online KD methods since we attempt to distill\nknowledge from multiple pre-trained teachers.\nMulti-teacher Knowledge Distillation. Rather than employ-\ning a single teacher, MKD boosts the effectiveness of distil-\nlation by integrating predictions from multiple teachers. A\nbunch of methods are proposed, such as simply assigning av-\nerage or other fixed weights for different teachers [8, 9, 10],\nand calculating the weights based on entropy [12], latent fac-\ntor [18] or multi-objective optimization in the gradient space\n[11]. However, these label-free strategies may mislead the\nstudent training in the presence of low-quality predictions.\nFor instance, entropy-based strategy will prefer models with\nblind faith since it favors predictions with low variance [12];\noptimization-based strategy favors majority opinion and will\nbe easily misled by noisy data [11]. In contrast, our CA-MKD\nquantifies the teacher predictions based on ground-truth labels\nand further improves the student performance.\n\n3. METHODOLOGY\n\nWe denote D = {xi,yi}Ni as a labeled training set, N is\nthe number of samples, K is the number of teachers. F \u2208\nRh\u00d7w\u00d7c is the output of the last network block. We denote\nz = [z1, ..., zC ] as the logits output, where C is the category\nnumber. The final model prediction is obtained by a softmax\nfunction \u03c3 (zc) = exp(z\n\nc/\u03c4)\u2211\nj\nexp(zj/\u03c4)\n\nwith temperature \u03c4 . In the\nfollowing sections, we will introduce our CA-MKD in detail.\n\n3.1. The Loss of Teacher Predictions\n\nTo effectively aggregate the prediction distributions of multi-\nple teachers, we assign different weights which reflects their\nsample-wise confidence by calculating the cross entropy loss\nbetween teacher predictions and ground-truth labels\n\nLkCEKD = \u2212\nC\u2211\nc=1\n\nyc log\n(\n\u03c3\n(\nzcTk\n))\n, (1)\n\nwkKD =\n1\n\nK \u2212 1\n\n\uf8eb\n\uf8ed1\u2212 exp (LkCEKD)\u2211\n\nj exp\n(\nLjCEKD\n\n)\n\uf8f6\n\uf8f8 , (2)\n\nwhere Tk denotes the kth teacher. The less LkCEKD corre-\nsponds to the larger wkKD. The overall teacher predictions are\n\nFig. 2. An overview of our CA-MKD. The weight calculation\nof teacher predictions and intermediate teacher features are\ndepicted as the red lines and green lines, respectively.\n\nthen aggregated with calculated weights\n\nLKD = \u2212\nK\u2211\nk=1\n\nwkKD\n\nC\u2211\nc=1\n\nzcTk log (\u03c3 (z\nc\nS)) . (3)\n\nAccording to the above formulas, the teacher whose pre-\ndiction is closer to ground-truth labels will be assigned larger\nweight wkKD, since it has enough confidence to make accu-\nrate judgement for correct guidance. In contrast, if we simply\nacquire the weights by calculating the entropy of teacher pre-\ndictions [12], the weight will become large when the output\ndistribution is sharp regardless of whether the highest prob-\nability category is correct. In this case, those biased targets\nmay misguide the student training and further hurt its distilla-\ntion performance.\n\n3.2. The Loss of Intermediate Teacher Features\n\nIn addition to KD Loss, inspired by FitNets [5], we believe\nthat the intermediate layers are also beneficial for learning\nstructural knowledge, and thus extend our method to interme-\ndiate layers for mining more information. The calculation of\nintermediate feature matching is presented as follows\n\nzS\u2192Tk =WTkhS , (4)\n\nLkCEinter = \u2212\nC\u2211\nc=1\n\nyc log\n(\n\u03c3\n(\nzcS\u2192Tk\n\n))\n, (5)\n\nwkinter =\n1\n\nK \u2212 1\n\n\uf8eb\n\uf8ed1\u2212 exp (LkCEinter)\u2211\n\nj exp\n(\nLjCEinter\n\n)\n\uf8f6\n\uf8f8 . (6)\n\nwhere WTk is the final classifier of the kth teacher. hS \u2208 R\nc\n\nis the last student feature vector, i.e, hS = AvgPooling(FS).\nLkCEinter is obtained by passing hS through each teacher clas-\nsifier. The calculation of wkinter is similar to that of w\n\nk\nKD.\n\n\n\nTable 1. Top-1 test accuracy of MKD methods by distilling the knowledge on multiple teachers with the same architectures.\n\nTeacher\nWRN40-2 ResNet56 VGG13 VGG13 ResNet32x4 ResNet32x4 ResNet32x4\n\n76.62\u00b10.26 73.28\u00b10.30 75.17\u00b10.18 75.17\u00b10.18 79.31\u00b10.14 79.31\u00b10.14 79.31\u00b10.14\nEnsemble 79.62 76.00 77.07 77.07 81.16 81.16 81.16\n\nStudent\nShuffleNetV1 MobileNetV2 VGG8 MobileNetV2 ResNet8x4 ShuffleNetV2 VGG8\n71.70\u00b10.43 65.64\u00b10.19 70.74\u00b10.40 65.64\u00b10.19 72.79\u00b10.14 72.94\u00b10.24 70.74\u00b10.40\n\nAVER [8] 76.30\u00b10.25 70.21\u00b10.10 74.07\u00b10.23 68.91\u00b10.35 74.99\u00b10.24 75.87\u00b10.19 73.26\u00b10.39\nFitNet-MKD [5] 76.59\u00b10.17 70.69\u00b10.56 73.97\u00b10.22 68.48\u00b10.07 74.86\u00b10.21 76.09\u00b10.13 73.27\u00b10.19\n\nEBKD [12] 76.61\u00b10.14 70.91\u00b10.22 74.10\u00b10.27 68.24\u00b10.82 75.59\u00b10.15 76.41\u00b10.12 73.60\u00b10.22\nAEKD [11] 76.34\u00b10.24 70.47\u00b10.15 73.78\u00b10.03 68.39\u00b10.50 74.75\u00b10.28 75.95\u00b10.20 73.11\u00b10.27\nCA-MKD 77.94\u00b10.31 71.38\u00b10.02 74.30\u00b10.16 69.41\u00b10.20 75.90\u00b10.13 77.41\u00b10.14 75.26\u00b10.32\n\nTable 2. Top-1 test accuracy of CA-MKD compared to\nsingle-teacher knowledge distillation methods.\n\nTeacher\nWRN40-2 ResNet32x4 ResNet56\n\n76.62\u00b10.26 79.31\u00b10.14 73.28\u00b10.30\n\nStudent\nShuffleNetV1 VGG8 MobileNetV2\n71.70\u00b10.19 70.74\u00b10.40 65.64\u00b10.43\n\nKD [4] 75.77\u00b10.14 72.90\u00b10.34 69.96\u00b10.14\nFitNet [5] 76.22\u00b10.21 72.55\u00b10.66 69.02\u00b10.28\n\nAT [6] 76.44\u00b10.38 72.16\u00b10.12 69.79\u00b10.26\nVID [14] 76.32\u00b10.08 73.09\u00b10.29 69.45\u00b10.17\nCRD [15] 76.58\u00b10.23 73.57\u00b10.25 71.15\u00b10.44\nCA-MKD 77.94\u00b10.31 75.26\u00b10.13 71.38\u00b10.02\n\nWe utilize wkinter instead of w\nk\nKD for the knowledge ag-\n\ngregation in intermediate layers, which achieves better results\nas shown in our ablation study. Perhaps this is due to the ex-\nistence of the last classifier will affect the whole knowledge\ntransfer process and should be taken into account.\n\nLinter =\nK\u2211\nk=1\n\nwkinter||FTk \u2212 r (FS) ||\n2\n2, (7)\n\nwhere r(\u00b7) is a function for aligning the student and teacher\nfeature dimensions. The `2 loss function is used as distance\nmeasure of intermediate features. Finally, the overall training\nloss between feature pairs will be aggregated by wkinter.\n\nIn our work, only the output features of the last block are\nadopted to avoid incurring too much computational cost.\n\n3.3. The Overall Loss Function\n\nIn addition to the aforementioned two losses, a regular cross\nentropy with the ground-truth labels is calculated\n\nLCE = \u2212\nC\u2211\nc=1\n\nyc log (\u03c3(zcS)) . (8)\n\nThe overall loss function of our CA-MKD is summarize as\n\nL = LCE + \u03b1LKD + \u03b2Linter, (9)\n\nwhere \u03b1 and \u03b2 are hyper-parameters to balance the effect of\nknowledge distillation and standard cross entropy losses.\n\n4. EXPERIMENT\n\nIn this section, we conduct extensive experiments on CIFAR-\n100 dataset [19] to verify the effectiveness of our proposed\nCA-MKD. We adopt eight different teacher-student combi-\nnations based on popular neural network architectures. All\ncompared multi-teacher knowledge distillation (MKD) meth-\nods use three teachers except for special declarations.\n\nCompared Methods. Besides the na\u0131\u0308ve AVER [8], we\nreimplement a single-teacher based method FitNet [5] on\nmultiple teachers and denote it as FitNet-MKD. FitNet-MKD\nwill leverage extra information coming from averaged inter-\nmediate teacher features. We also reimplement an entropy-\nbased MKD method [12], which has achieved remarkable\nresults in acoustic experiments, on our image classification\ntask and we denote it as EBKD. As for AEKD, we adopt its\nlogits-based version with the author provided code [11].\n\nHyper-parameters. All neural networks are optimized\nby stochastic gradient descent with momentum 0.9, weight\ndecay 0.0001. The batch size is set to 64. As the previous\nworks do [15, 7], the initial learning rate is set to 0.1, ex-\ncept MobileNetV2, ShuffleNetV1 and ShuffleNetV2 are set\nto 0.05. The learning rate is multiplied by 0.1 at 150, 180 and\n210 of the total 240 training epochs. For the sake of fairness,\nthe temperature \u03c4 is set to 4 and the \u03b1 is set to 1 in all methods.\nFurthermore, we set the \u03b2 of our CA-MKD to 50 throughout\nthe experiments. All results are reported in means and stan-\ndard deviations over 3 runs with different random seeds.\n\n4.1. Results on the Same Teacher Architectures\n\nTable 1 shows the top-1 accuracy comparison on CIFAR-100.\nWe also include the results of teacher ensemble with the ma-\njority voting strategy. We can find that CA-MKD surpasses\nall competitors cross various architectures. Specifically, com-\npared to the second best method (EBKD), CA-MKD out-\n\n\n\nTable 3. Top-1 test accuracy of MKD approaches by distilling the knowledge on multiple teachers with different architectures.\nVGG8 AVER FitNet-MKD EBKD AEKD CA-MKD ResNet8x4 ResNet20x4 ResNet32x4\n\n70.74\u00b10.40 74.55\u00b10.24 74.47\u00b10.21 74.07\u00b10.17 74.69\u00b10.29 75.96\u00b10.05 72.79 78.39 79.31\n\nFig. 3. The visualization results of learned weights by CA-\nMKD on each training sample.\n\nperforms it with 0.81% average improvement1, and achieves\n1.66% absolute accuracy improvement in the best case.\n\nTo verify the benefits of diverse information brought by\nmultiple teachers, we compare CA-MKD with some excellent\nsingle-teacher based methods. The results in Table 2 show the\nstudent indeed has the potential to learn knowledge from mul-\ntiple teachers, and its accuracy is further improved compared\nwith the single-teacher methods to a certain extent.\n\n4.2. Results on the Different Teacher Architectures\n\nTable 3 shows the results of training a student (VGG8)\nwith three different teacher architectures, i.e., ResNet8x4,\nResNet20x4 and ResNet32x4. We find the student accu-\nracy becomes even higher than that of training with three\nResNet32x4 teachers, which may be attributed to that the\nknowledge diversity is enlarged in different architectures.\n\nSince the performance of ResNet20x4/ResNet32x4 is bet-\nter than that of ResNet8x4, we could reasonably believe that\nfor most training samples, the student will put larger weights\non predictions from the former two rather than the latter one,\nwhich is verified in Figure 3. Moreover, our CA-MKD can\ncapture those samples on which the predictions are more con-\nfident by ResNet8x4, and assign them dynamic weights to\nhelp the student model achieve better performance.\n\n4.3. Impact of the Teacher Number\n\nAs shown in Figure 4, the student model trained with CA-\nMKD generally achieves satisfactory results. For example,\non the \u201cResNet56 & MobileNetV2\u201d setting, the accuracy of\n\n1Average Improvement= 1\nn\n\n\u2211n\ni\n\n(\nAcci\n\nCA\u2212MKD \u2212Acc\ni\nEBKD\n\n)\n, where\n\nthe accuracies of CA-MKD, EBKD in the i-th teacher-student combination\nare denoted as Acci\n\nCA\u2212MKD, Acc\ni\nEBKD\n\n, respectively.\n\nFig. 4. The effect of different teacher numbers.\n\nTable 4. Ablation study with VGG13 & MobileNetV2.\navg weight w/o Linter w/o wkinter CA-MKD\n67.74\u00b10.87 68.11\u00b10.02 68.82\u00b10.63 69.41\u00b10.20\n\nCA-MKD increases continually as the number of teachers in-\ncreases and it surpasses the competitors with three teachers\neven those competitors are trained with more teachers.\n\n4.4. Ablation Study\n\nWe summarize the observations from Table 4 as follows:\n(1) avg weight. Simply averaging multiple teachers will\n\ncause 1.67% accuracy drop, which confirms the necessity of\ntreating different teachers based on their specific quality.\n\n(2) w/o Linter. The accuracy will appear considerably\nreduction as we remove the Equation (7), demonstrating the\nintermediate layer contains useful information for distillation.\n\n(3) w/o wkinter. we directly use the w\nk\nKD obtained from\n\nthe last layer to integrate intermediate features. The lower\nresult indicates the benefits of designing a separate way of\ncalculating weights for the intermediate layer.\n\n5. CONCLUSION\n\nIn this paper, we introduce confidence-aware mechanism on\nboth predictions and intermediate features for multi-teacher\nknowledge distillation. The confidence of teachers is calcu-\nlated based on the closeness between their predictions or fea-\ntures and the ground-truth labels for the reliability identifica-\ntion on each training sample. With the guidance of labels,\nour technique effectively integrates diverse knowledge from\nmultiple teachers for the student training. Extensive empiri-\ncal results show that our method outperforms all competitors\nin various teacher-student architectures.\n\n\n\n6. REFERENCES\n\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun, \u201cDeep residual learning for image recognition,\u201d in\nProceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770\u2013778.\n\n[2] David Silver, Julian Schrittwieser, Karen Simonyan,\nIoannis Antonoglou, Aja Huang, Arthur Guez, Thomas\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton,\net al., \u201cMastering the game of go without human knowl-\nedge,\u201d Nature, vol. 550, no. 7676, pp. 354\u2013359, 2017.\n\n[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, \u201cBERT: pre-training of deep bidi-\nrectional transformers for language understanding,\u201d in\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\n2019, pp. 4171\u20134186.\n\n[4] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, \u201cDistill-\ning the knowledge in a neural network,\u201d arXiv preprint\narXiv:1503.02531, 2015.\n\n[5] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua Ben-\ngio, \u201cFitnets: Hints for thin deep nets,\u201d in International\nConference on Learning Representations, 2015.\n\n[6] Sergey Zagoruyko and Nikos Komodakis, \u201cPaying more\nattention to attention: improving the performance of\nconvolutional neural networks via attention transfer,\u201d in\nInternational Conference on Learning Representations,\n2017.\n\n[7] Defang Chen, Jian-Ping Mei, Yuan Zhang, Can Wang,\nZhe Wang, Yan Feng, and Chun Chen, \u201cCross-layer\ndistillation with semantic calibration,\u201d in Proceedings\nof the AAAI Conference on Artificial Intelligence, 2021,\nvol. 35, pp. 7028\u20137036.\n\n[8] Shan You, Chang Xu, Chao Xu, and Dacheng Tao,\n\u201cLearning from multiple teacher networks,\u201d in Proceed-\nings of the 23rd ACM SIGKDD International Confer-\nence on Knowledge Discovery and Data Mining, 2017,\npp. 1285\u20131294.\n\n[9] Takashi Fukuda, Masayuki Suzuki, Gakuto Kurata,\nSamuel Thomas, Jia Cui, and Bhuvana Ramabhadran,\n\u201cEfficient knowledge distillation from an ensemble of\nteachers.,\u201d in Interspeech, 2017, pp. 3697\u20133701.\n\n[10] Meng-Chieh Wu, Ching-Te Chiu, and Kun-Hsuan Wu,\n\u201cMulti-teacher knowledge distillation for compressed\nvideo action recognition on deep neural networks,\u201d in\nICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp. 2202\u20132206.\n\n[11] Shangchen Du, Shan You, Xiaojie Li, Jianlong Wu, Fei\nWang, Chen Qian, and Changshui Zhang, \u201cAgree to\ndisagree: Adaptive ensemble knowledge distillation in\ngradient space,\u201d Advances in Neural Information Pro-\ncessing Systems, vol. 33, 2020.\n\n[12] Kisoo Kwon, Hwidong Na, Hoshik Lee, and Nam Soo\nKim, \u201cAdaptive knowledge distillation based on en-\ntropy,\u201d in ICASSP 2020-2020 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2020, pp. 7409\u20137413.\n\n[13] Jimmy Ba and Rich Caruana, \u201cDo deep nets really need\nto be deep?,\u201d in Advances in Neural Information Pro-\ncessing Systems, 2014, pp. 2654\u20132662.\n\n[14] Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D\nLawrence, and Zhenwen Dai, \u201cVariational information\ndistillation for knowledge transfer,\u201d in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 2019, pp. 9163\u20139171.\n\n[15] Yonglong Tian, Dilip Krishnan, and Phillip Isola, \u201cCon-\ntrastive representation distillation,\u201d in International\nConference on Learning Representations, 2020.\n\n[16] Xu Lan, Xiatian Zhu, and Shaogang Gong, \u201cKnowl-\nedge distillation by on-the-fly native ensemble,\u201d arXiv\npreprint arXiv:1806.04606, 2018.\n\n[17] Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and\nChun Chen, \u201cOnline knowledge distillation with diverse\npeers.,\u201d in Proceedings of the AAAI Conference on Arti-\nficial Intelligence, 2020, pp. 3430\u20133437.\n\n[18] Yuang Liu, Wei Zhang, and Jun Wang, \u201cAdaptive multi-\nteacher multi-level knowledge distillation,\u201d Neurocom-\nputing, vol. 415, pp. 106\u2013113, 2020.\n\n[19] Alex Krizhevsky and Geoffrey Hinton, \u201cLearning mul-\ntiple layers of features from tiny images,\u201d Technical Re-\nport, 2009.\n\n\n\t1  Introduction\n\t2  RELATED WORK\n\t3  METHODOLOGY\n\t3.1  The Loss of Teacher Predictions\n\t3.2  The Loss of Intermediate Teacher Features\n\t3.3  The Overall Loss Function\n\n\t4  EXPERIMENT\n\t4.1  Results on the Same Teacher Architectures\n\t4.2  Results on the Different Teacher Architectures\n\t4.3  Impact of the Teacher Number\n\t4.4  Ablation Study\n\n\t5  CONCLUSION\n\t6  References\n\n"}
{"Title": "A Lightweight and Accurate Spatial-Temporal Transformer for Traffic Forecasting", "Authors": "Guanyao Li, Shuhan Zhong, Letian Xiang, S.-H. Gary Chan, Ruiyuan Li, Chih-Chieh Hung, Wen-Chih Peng", "Abstract": "  We study the forecasting problem for traffic with dynamic, possibly periodical, and joint spatial-temporal dependency between regions. Given the aggregated inflow and outflow traffic of regions in a city from time slots 0 to t-1, we predict the traffic at time t at any region. Prior arts in the area often consider the spatial and temporal dependencies in a decoupled manner or are rather computationally intensive in training with a large number of hyper-parameters to tune. We propose ST-TIS, a novel, lightweight, and accurate Spatial-Temporal Transformer with information fusion and region sampling for traffic forecasting. ST-TIS extends the canonical Transformer with information fusion and region sampling. The information fusion module captures the complex spatial-temporal dependency between regions. The region sampling module is to improve the efficiency and prediction accuracy, cutting the computation complexity for dependency learning from $O(n^2)$ to $O(n\\sqrt{n})$, where n is the number of regions. With far fewer parameters than state-of-the-art models, the offline training of our model is significantly faster in terms of tuning and computation (with a reduction of up to $90\\%$ on training time and network parameters). Notwithstanding such training efficiency, extensive experiments show that ST-TIS is substantially more accurate in online prediction than state-of-the-art approaches (with an average improvement of up to $9.5\\%$ on RMSE, and $12.4\\%$ on MAPE).      ", "Subject": "Machine Learning (cs.LG)", "ID": "arXiv:2201.00008", "Text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\nA Lightweight and Accurate Spatial-Temporal\nTransformer for Traffic Forecasting\n\nGuanyao Li, Shuhan Zhong, Letian Xiang, S.-H. Gary Chan\nRuiyuan Li, Chih-Chieh Hung, Wen-Chih Peng\n\nAbstract\u2014We study the forecasting problem for traffic with dynamic, possibly periodical, and joint spatial-temporal dependency\nbetween regions. Given the aggregated inflow and outflow traffic of regions in a city from time slots 0 to t\u2212 1, we predict the traffic at\ntime t at any region. Prior arts in the area often consider the spatial and temporal dependencies in a decoupled manner, or are rather\ncomputationally intensive in training with a large number of hyper-parameters to tune.\nWe propose ST-TIS, a novel, lightweight and accurate Spatial-Temporal Transformer with information fusion and region sampling for\ntraffic forecasting. ST-TIS extends the canonical Transformer with information fusion and region sampling. The information fusion\nmodule captures the complex spatial-temporal dependency between regions. The region sampling module is to improve the efficiency\nand prediction accuracy, cutting the computation complexity for dependency learning from O(n2) to O(n\n\n\u221a\nn), where n is the number of\n\nregions. With far fewer parameters than state-of-the-art models, ST-TIS\u2019s offline training is significantly faster in terms of tuning and\ncomputation (with a reduction of up to 90% on training time and network parameters). Notwithstanding such training efficiency,\nextensive experiments show that ST-TIS is substantially more accurate in online prediction than state-of-the-art approaches (with an\naverage improvement of 9.5% on RMSE, and 12.4% on MAPE compared to STDN and DSAN).\n\nIndex Terms\u2014spatial-temporal forecasting; spatial-temporal data mining; efficient Transformer; joint spatial-temporal dependency;\nregion sampling.\n\nF\n\n1 INTRODUCTION\n\nTraffic forecasting is to predict the inflow (i.e., the number\nof arriving objects per unit time) and outflow (i.e., the\nnumber of departing objects per unit time) of any region\nin a city at the next time slot. The objects can be people,\nvehicles, goods/items, etc. Traffic forecasting has important\napplications in transportation, retails, public safety, city\nplanning, etc [1], [2]. For example, with traffic forecasting, a\ntaxi company may dispatch taxis in a timely manner to meet\nthe supply and demand in different regions of a city. Yet\nanother example is bike sharing, where the company may\nwant to balance bike supply and demand at dock stations\n(regions) based on such forecasting.\n\nAlthough there has been much effort on deep learning to\nimprove the prediction accuracy of the state-of-the-art fore-\ncasting models, progressive improvements on benchmarks\nhave been correlated with an increase in the number of\nparameters and the amount of training resources required\nto train the model, making it costly to train and deploy\nlarge deep learning models [3]. Therefore, a lightweight\n\n\u2022 Guanyao Li, Shuhan Zhong, Letian Xiang, andd S.-H. Gary Chan are\nwith the Department of Computer Science and Engineering, The Hong\nKong University of Science and Technology.\nE-mail: {gliaw, szhongaj, lxiangab, gchan}@cse.ust.hk\n\n\u2022 Ruiyuan Li is with College of Computer Science, Chongqing University.\nE-mail: liruiyuan@cqu.edu.cn\n\n\u2022 Chih-Chieh Hung is with Department of Computer Science and Engineer-\ning, National Chung Hsing University.\nE-mail: smalloshin@email.nchu.edu.tw\n\n\u2022 Wen-Chih Peng is with Department of Computer Science, National Yang\nMing Chiao Tung University.\nE-mail: wcpeng@g2.nctu.edu.tw\n\nand training-efficient model is essential for fast delivery and\ndeployment.\n\nIn this work, we study the following spatial-temporal\ntraffic forecasting problem: Given the historical (aggregated)\ninflow and outflow data of different regions from time slots\n0 to t \u2212 1 (with slot size of, say, 30 minutes), what is the\ndesign of a training-efficient model to accurately predict the\ninflow and outflow of any region at time t? (Note that even\nthough we consider predicting for the next time slot, our\nwork can be straightforwardly extended to any future time\nslot by successive application of the algorithm.) We seek\na \u201csmall\u201d training model with substantially fewer parame-\nters, which naturally leads to efficiency in tuning, memory,\nand computation time. Despite its training efficiency, our\nlightweight model lightweight, it should also achieve higher\naccuracy than the state-of-the-art approaches in its online\npredictions.\n\nIntuitively, region traffic is spatially and temporally cor-\nrelated. As an example, the traffic of a region could be\ncorrelated with that of another with some temporal lag\ndue to the travel time between them. Moreover, such de-\npendency may be dynamic over different time slots, and it\nmay have temporal periodic patterns. This is the case for the\ntraffic of office regions, which exhibit high correlation with\nthe residence regions in workday morning but much less\nthan at night or weekend. To accurately predict the region\ntraffic, it is hence significantly crucial to account for the\ndynamic, possibly periodical, and joint spatial-temporal (ST)\ndependency between regions, no matter how far the regions\nis apart.\n\nMuch effort has been devoted to capturing the de-\npendency between regions for traffic forecasting. While\n\nar\nX\n\niv\n:2\n\n20\n1.\n\n00\n00\n\n8v\n2 \n\n [\ncs\n\n.L\nG\n\n] \n 4\n\n J\nan\n\n 2\n02\n\n2\n\n\n\n2\n\nFig. 1. Visualization of attention scores between a target region (6,4)\nand other regions. The color of a cell (xi, yi) indicates the dependency\nof (6, 4) on (xi, yi), where a darker color indicates stronger dependency.\n\ncommendable, most works consider spatial and temporal\ndependency separately with independent processing mod-\nules [4]\u2013[9], which can hardly capture their joint nature\nin our current setting. Some recent works apply canonical\nTransformer [10] to capture region dependency [11]\u2013[14].\nWhile impressive, canonical Transformer limits the train-\ning efficiency because it learns a region\u2019s embedding as\nthe weighted aggregation of all the other regions based\non their computed attention scores. This results in O(n2)\ncomputation complexity per layer, where n is the number of\nregions. Moreover, it has been observed that the attention\nscores from the canonical Transformer have a long tail\ndistribution [15]. We illustrate this in Figure 1 using a taxi\ndataset collected in New York City. We split New York City\ninto 10\u00d720 regions and visualize the attention scores (after a\nSoftMax operation) between a target region (i.e., the region\n(6, 4)) and other regions. Clearly, most regions have very\nsmall attention scores (i.e., the long-tail phenomenon), with\nthe attention scores of more than 60% of regions being less\nthan 0.004. Such a long-tail effect may introduce noise for\nthe region embedding learning and degrade the prediction\nperformance.\n\nWe propose ST-TIS, a novel, small, efficient and accurate\nSpatial-Temporal Transformer with information fusion and\nregion sampling for traffic forecasting. Given the histor-\nical (aggregated) inflow and outflow of regions from 0\nto t \u2212 1, ST-TIS predicts the inflow and outflow of any\nregion at t, without relying on the transistion data between\nregions. With a small set of parameters, ST-TIS is efficient\nto train (offline phase). It extends the canonical Transformer\nwith novel information fusion and region sampling strate-\ngies to learn the dynamic and possibly periodical spatial-\ntemporal dependency between regions in a joint manner,\nhence achieving high prediction accuracy (online phase).\n\nST-TIS makes the following contributions:\n\n\u2022 A data-driven Transformer scheme for dynamic, possibly\nperiodical, and joint spatial-temporal dependency learn-\ning. ST-TIS jointly considers the spatial-temporal de-\npendencies between regions, rather than considering\nthe two dependencies sequentially in a decoupled\nmanner. In particular, ST-TIS considers the dynamic\nspatial-temporal dependency for any individual time\nslot with an information fusion module, and also the\npossibly periodical characteristic of spatial-temporal\n\ndependency from multiple time slots using an atten-\ntion mechanism. Moreover, the dependency learning\nis data-driven, without any assumption on spatial\nlocality. Due to its design, ST-TIS is small (in parame-\nter footprint), fast (in training time), and accurate (in\nprediction).\n\n\u2022 A novel region sampling strategy for computationally effi-\ncient dependency learning. ST-TIS leverages the Trans-\nformer framework [10] to learn region dependency.\nTo address the quadratic computation issue and\nmitigate the long-tail effect, it employs a novel re-\ngion sampling strategy to generate a connected re-\ngion graph and learns the dependency based on\nthe graph. The dependencies between any pair of\nregions (both close and distant dependencies) are\nguaranteed to be considered in ST-TIS via informa-\ntion propagation, and the computational complexity\nis reduced from O(n2) to O(n\n\n\u221a\nn), where n is the\n\nnumber of regions.\n\u2022 Extensive experimental validation: We evaluate ST-TIS\n\non two large-scale real datasets of taxi and bike\nsharing. Our results shows that ST-TIS is substan-\ntially more accurate than the state-of-the-art ap-\nproaches, with a significant improvement in RMSE\nand MAPE (an average improvement of 9.5% on\nRMSE, and 12.4% on MAPE compared to STDN and\nDSAN). Furthermore, it is much more lightweight\nthan most state-of-the-art models, and is ultra fast for\ntraining (with a reduction of 46% \u223c 95% on training\ntime and 23% \u223c 98% on network parameters).\n\nThe remainder of this paper is organized as follows. We\nfirst discuss related works in Section 2. After preliminaries\nin Section 3, we detail ST-TIS in Section 4. We present the\nexperimental settings and results in Section 5, and conclude\nin Section 6.\n\n2 RELATED WORKS\nTraffic forecasting has raised much attention in both\nacademia and industry due to its social and commercial\nvalues. Some early traffic forecasting works propose using\nregression models, such as auto-regressive integrated mov-\ning average (ARIMA) models [16]\u2013[19] and non-parametric\nregion models [20], [21]. All of these works consider tem-\nporal dependency, but they have not considered the spatial\ndependency between regions. Some other works extract fea-\ntures from heterogeneous data sources (e.g., POI, weather,\netc.), and use machine learning models such as Support\nVector Machine [22], Gradient Boosting Regression Tree [23],\nand linear regression model [24]. Despite of the encouraging\nresults, they rely on manually defined features and have not\nconsidered the joint spatial-temporal dependency.\n\nIn recent years, deep learning techniques have been\nemployed to study spatial and temporal correlations for\ntraffic forecasting. Most existing works consider the spa-\ntial and temporal dependency in a decouple manner [4]\u2013\n[9], which can hardly capture their joint effect. For spatial\ndependency, Convolution Neural Network (CNN), Graph\nNeural Network (GNN), and Transformer have been widely\napplied. Regarding temporal dependency, Recurrent Neural\nNetwork (RNN) and its variants such as Long Short Term\n\n\n\n3\n\nMemory (LSTM) and Gated Recurrent Unit (GRU) have\nbeen extensively studied. Compared with these works, ST-\nTIS considers the spatial and temporal dependency in a joint\nmanner.\n\nCNN has been applied in many works to capture depen-\ndencies between close regions [4]\u2013[7], [25]. In these works,\na city is divided into some connected but non-overlapping\ngrids, and the traffic in each grid is then predicted. However,\nthese works cannot be used for fine-grained flow forecasting\nat an individual location [26], such as predicting flow for a\ndocked bike-sharing station or a subway station. Moreover,\nCNN can hardly capture distant traffic dependency due to\nits relatively small receptive field [27], [28].\n\nSome other works use GCN to capture spatial depen-\ndency [8], [9], [29]\u2013[35]. In these works, a city is represented\nas a graph structure, and convolution operations are ap-\nplied to aggregate spatially distributed information in the\ngraph. In each aggregation layer, a region would aggregate\nthe embedding of its neighbouring regions in the graph.\nHowever, these works highly rely on the graph structure for\ndependency learning. Prior works usually construct graphs\nbased on the distance between regions or road network,\nbased on the locality assumption (i.e., close regions have\nhigher dependency). They have to stack more layers to\nlearn dependency if the distance between two regions in the\ngraph is long, and it ends up with an inefficient and over-\nsmoothing model [31]. In recent years, Transformer [10] has\nbeen applied for traffic forecasting [11]\u2013[14]. The canonical\nTransformer can be seen as a special graph neural network\nwith a complete graph, in which any pair of regions are\nconnected. Consequently, the dependencies between both\nclose and distant regions could be considered. Moreover,\nthe self-attention mechanism and the network structure of\nTransformer have been demonstrated to be powerful in\nmany prior works. However, the computational complexity\nof each aggregation round isO(n2) for Transformer where n\nis the number of regions, while that for GNN is O(E) where\nE is the number of edges in the graph (E \u2264 n2). Further-\nmore, as a region may only have a strong dependency on a\nsmall portion of regions, aggregating the embedding of all\nregions would introduce noise and degrade its performance.\nIn ST-TIS, we propose a region connected graph (i.e., there\nexists a path between any pair of regions in the graph), in\nwhich the degree of any node (i.e., region) is O(\n\n\u221a\nn) and\n\nthe distance between any two regions in the graph is no\nmore than 2. We extend the canonical Transformer with the\nproposed region connected graph, so that it can inherit the\nadvantage of efficiency and effectiveness from both GNN\nand Transformer.\n\nRNN and its variants such as LSTM and GRU [36], [37]\nhave been used to capture temporal dependency [6], [34],\n[35], [38]. However, the performance of RNN-based models\ndeteriorates rapidly as the length of the input sequence\nincreases [39]. Some works incorporate the attention mecha-\nnism [40] to improve their capability of modeling long-term\ntemporal correlations [7], [9], [29], [41]. Nevertheless, RNN-\nbased networks are widely known to be difficult to train and\nare computationally intensive [8]. As recurrent networks\ngenerate the current hidden states as a function of the\nprevious hidden state and the input for the position, they are\nin general more difficult to be trained in parallel. To address\n\nthe issue, the self-attention mechanism is proposed as the\nreplacement of RNN to model sequential data [10]. It has\nenjoyed success in capturing temporal correlations for traf-\nfic forecasting [11]\u2013[13], [42]. Compared with RNN-based\nmodels, self-attention models can directly model long-term\ntemporal interactions, but the computational complexity of\nusing self-attention for temporal dependency learning in\nexisting works is O(q2), where q is the number of historical\ntime slots. Compared with them, ST-TIS is conditional on\nthe spatial-temporal dependency at any individual slot to\ngenerate weights for different time slots, so its computa-\ntional complexity is O(q) in our work. In addition, the\ntemporal dependency is jointly considered with the spatial\ndependency in ST-TIS, instead of in a decouple manner.\n\nSome variants of Transformer have been proposed to\naddress the efficiency issues of the canonical Transformer,\nsuch as LogSparse [43], Reformer [44], Informer [15], etc.\nWhile impressive, these approaches cannot be used in the\nscenario of capturing spatial-temporal dependency between\nregions for traffic forecasting we are considering in this\nwork.\n\n3 PRELIMINARIES\n\n3.1 Problem formulation\n\nDefinition 1. (Region) The area (e.g., a city or subway route)\nis partitioned into n non-overlapping regions. We use R =\n{r1, r2, . . . , rn} to denote the partitioned regions, in which ri\ndenotes the i-th region.\n\nThe way to partition an area is flexible for ST-TIS, e.g.,\ngrid map, road network, clustering, or train/bus/bike sta-\ntions, etc.\n\nDefinition 2. (Traffic data) We use I and O to denote the inflow\nand outflow data of all regions over time, respectively. Specifically,\nIt \u2208 R1\u00d7n and Ot \u2208 R1\u00d7n is the inflow and outflow of the\nn regions at time t. Moreover, Iti and O\n\nt\ni is the inflow and\n\noutflow of region ri at time t (i.e., the number of objects arriving\nat/departing from the region ri at time slot t). Furthermore, we use\nIt\n\n\u2032:t\ni = [I\n\nt\u2032\n\ni , I\nt\u2032+1\ni , . . . , I\n\nt\ni ] to denote the inflow of ri from t\n\n\u2032 to\nt. Similarly, Ot\n\n\u2032:t\ni = [O\n\nt\u2032\n\ni ,O\nt\u2032+1\ni , . . . ,O\n\nt\ni ] indicates the outflow\n\nof ri from t\u2032 to t.\n\nThe formal formulation of the traffic forecasting problem\nis as follows:\n\nDefinition 3. (Traffic Forecasting) Given the traffic data of\nregions from 0 to t \u2212 1, namely I0:t\u22121 and O0:t\u22121, the traffic\nforecasting problem is to predict the inflow and outflow of any\nregion at t, namely It and Ot.\n\n3.2 ST-TIS Overview\n\nWe overview the proposed ST-TIS in Figure 2. Given the\ntraffic data of all regions from time slots 0 to t \u2212 1, ST-\nTIS is an end-to-end model to capture the spatial-temporal\ndependency and predict the inflow and outflow for any\nregion at t. There are five modules in ST-TIS. We explain\nthe design goals and the relationship among modules as\nfollows:\n\n\n\n4\n\nFig. 2. ST-TIS overview.\n\n\u2022 Information Fusion Module: A key to accurately pre-\ndicting the traffic for regions is capturing the dy-\nnamic spatial-temporal (ST) dependency between re-\ngions in a joint manner. To this end, ST-TIS employs\na information fusion module to learn the spatial-\ntemporal-flow (STF) embedding by encoding its spa-\ntial, temporal, and flow information for any individ-\nual region at a time slot.\n\n\u2022 Region Sampling Module: To address the issues of\nquadratic computational complexity and long tail\ndistribution of attention scores for the canonical\nTransformer, ST-TIS uses a novel region sampling\nstrategy to generate a region connected graph. The\ndependency learning would be based on the gener-\nated graph.\n\n\u2022 Dependency Learning for Individual Time Slot (DLI):\nGiven the STF embedding at a time slot and the\nregion connected graph, ST-TIS extends the canonical\nTransformer to jointly capture the dynamic spatial-\ntemporal dependency between regions at the time\nslot. As the spatial and temporal information has\nbeen both encoded in the STF embedding, the joint\neffect of spatial-temporal dependencies between re-\ngions could be captured. Moreover, with the region\nconnected graph, only the attention scores between\nneighbouring nodes (i.e., regions) are computed,\nand only the embedding of one\u2019s neighbouring re-\ngions are aggregated. Dependency between non-\nneighbouring nodes is considered via information\npropagation between multiple layers in the network.\nConsequently, it cuts the computational complexity\nof a layer from O(n2) to O(n \u00d7\n\n\u221a\nn) where n is the\n\nnumber of regions, and addresses the issue of the\nlong-tail effect for dependency learning.\n\n\u2022 Dependency Learning over Multiple Time Slots (DLM):\nGiven the region embedding from DLI at multi-\nple time slots, DLM captures the periodic patterns\n\nof spatial-temporal dependency using the attention\nmechanism. The influence of spatial-temporal de-\npendency at historical time slots on the predicted\ntime slot is hence considered in ST-TIS. The learning\nprocess is data-driven, without any prior assumption\nof traffic periods.\n\n\u2022 Prediction Network (PN): Given the results from DLM,\nST-TIS uses a fully connected neural network to\npredict the inflow and outflow of regions (It and\nOt) simultaneously.\n\n4 ST-TIS DETAILS\nWe present the details of ST-TIS in this section. We first elab-\norate the information fusion module in Section 4.1, followed\nby the region sampling module in Section 4.2. After that,\nwe introduce the dependency learning for individual time\nslot (DLI) in Section 4.3, and the dependency learning over\nmultiple time slots (DLM) in Section 4.4. Finally, we present\nthe prediction network (PN) in Section 4.5.\n\n4.1 Information Fusion Module\nTo capture the dynamic joint spatial-temporal dependency\nfor traffic forecasting, it is essential to fuse the spatial-\ntemporal information for each region at any individual time\nslot. To this end, ST-TIS employs the information fusion\nmodule to learn one\u2019s spatial-temporal-flow (STF) embed-\nding by fusing its position, time slot, and flow information.\n\nGiven n regions, we first use a one-hot vector Si \u2208 R1\u00d7n\nto represent a region ri, in which only the i-th element in\nSi is 1 and otherwise 0. After that, we encode the position\ninformation for a region ri as follows,\n\nS\u0302i = Si \u00b7WS + bS . (1)\n\nwhere S\u0302i \u2208 R1\u00d7d is the spatial embedding of ri, WS \u2208\nRn\u00d7d and bS \u2208 R1\u00d7d are learnable parameters, and d is a\nhyperparameter.\n\nIn terms of temporal information, we first split a day\ninto o time slots and represent the i-th time slot using a\none-hot vector Ti \u2208 R1\u00d7o. After that, we learn the temporal\nembedding by\n\nT\u0302i = Ti \u00b7WT + bT , (2)\n\nwhere T\u0302i \u2208 R1\u00d7d is the temporal embedding of the i-th time\nslot in a day, WT \u2208 Ro\u00d7d and bT \u2208 R1\u00d7d are learnable\nparameters.\n\nRecall that the traffic of one region at ti may depend on\nthat of another region at previous time slots due to the travel\ntime between two regions. Thus, we use one\u2019s surrounding\nobservations instead of solely using a snapshot to learn the\ndependency [43]. We define the surrounding observations\nof a region at a time slot tj as follows:\n\nDefinition 4. (Surrounding observations) For a region\nri at a time slot tj , its surrounding observations are\ndefined as the flow in the w previous time slots:\n{Itj\u2212wi , \u00b7 \u00b7 \u00b7 , I\n\ntj\u22122\ni , I\n\ntj\u22121\ni ,O\n\ntj\u2212w\ni , \u00b7 \u00b7 \u00b7 ,O\n\ntj\u22122\ni ,O\n\ntj\u22121\ni }.\n\nNote that by employing the surrounding observations,\nthe temporal lag of the dependency between regions could\nbe considered. Given the surrounding observations of ri at\ntj , we first apply the 1-D convolution of kernel size 1 \u00d7\n\n\n\n5\n\nFig. 3. Information propagation in a graph.\n\nFig. 4. The process of connected graph generation.\n\np (p < w) with stride 1 and f output channels on its inflow\nand outflow surrounding observations to extract different\npatterns. The flow embedding F tji \u2208 R\n\n1\u00d7d of ri at tj is\ncomputed as\n\nF tji = (Conv\nI(Itj\u2212w:tj\u22121i )||Conv\n\nO(Otj\u2212w:tj\u22121i ))\u00b7W\nF +bF ,\n\n(3)\nwhere || is the concatenation operation, ConvI(\u00b7) and\nConvO(\u00b7) are the convolutional operation for inflow and\noutflow data respectively, WF \u2208 Rl\u00d7d, bF \u2208 R1\u00d7d are\nlearnable parameters, and l = 2\u00d7 f \u00d7 (w \u2212 k + 1).\n\nFinally, we fuse the position, time slot and flow informa-\ntion to learn the STF embedding of a region ri at time tj . We\ndefine that tj is the M(tj)-th time slot in a day, where M(\u00b7)\nis an matching function. The fusion process is defined as\n\nLtji = (S\u0302i + T\u0302M(tj) + F\ntj\ni ) \u00b7W\n\nL + bLi , (4)\n\nwhere Lji \u2208 R\n1\u00d7d is the STF embedding, WL \u2208 Rd\u00d7d and\n\nbLi \u2208 R\n1\u00d7d are learnable parameters.\n\n4.2 Region Sampling\n\nTo capture a region\u2019s dependency on others (both nearby\nand distant), a canonical Transformer computes the atten-\ntion scores between the target region and all other regions,\nand aggregats the embedding of all other regions based\non the computed attention scores. However, this results in\n\nthe issues of quadratic computation and long-tail effect for\ndependency learning.\n\nFortunately, prior works have showed that information\ncould be propogated between nodes in a graph via a multi-\nlayer network structure [45]. Hence, with a proper graph\nstructure and network structure, a region can aggregate\nthe embedding of another even without directly evaluating\ntheir attention score. We present a toy example in Figure\n3, in which regions are represented as nodes in the graph\nand connected with egdes. In the first aggregation layer,\nr1 would capture the information from r2, while r2 would\ncapture the information from r3. Since the information of r3\nhas been aggregated in r2, r1 could also capture it in the\nsecond aggregation layer without computing the attention\nscore between r1 and r3. From this example, we conclude\nthat a node\u2019s information can reach another with a \u03b2-layer\naggregation operation if their distance is not more than \u03b2 in\nthe graph.\n\nTo address the limitations of the canonical Transformer,\nwe propose to generate a connected graph (i.e., there exists\nat least a path between any two nodes in the graph), in\nwhich the degree of any node (i.e., region) is not more\nthan c \u00d7\n\n\u221a\nn and the distance between any pair of nodes\n\nare not more than 2. In this way, for a target region, we\nonly have to aggregate the embedding of O(\n\n\u221a\nn) regions\n\nin an aggregation layer, and the influence from other re-\ngions can be captured in a two-layer aggregation process\nby information propagation. With such design, we do not\nhave to compute the attention scores between any pairs of\nregions and aggregate the embedding of all n regions for a\ntarget region, so that the computation complexity is reduced\nto O(n\n\n\u221a\nn) for each layer, and the long-tail issue is also\n\naddressed.\nIn this work, we present a heuristic approach for region\n\nconnected graph generation. We first calculate the traffic\nsimilarity between any pair of regions. The calculation of\nthe traffic similarity is flexible and it could be any similarity\nmetric. As an example, we use DTW in this work to measure\nthe similarity in terms of the average traffic over time slots\nof a day. We use M \u2208 Rn\u00d7n to represent the similarity\nmatrix, in which Mi,j is the similarity between ri and rj .\nBased onM, the process of the connected graph generation\nis illustrated in Figure 4.\n\nWe first select top b\n\u221a\nnc regions without replacement,\n\nwhich have the largest sum of similarity with other regions,\nrepresented as {r1,1, r1,2, \u00b7 \u00b7 \u00b7 , r1,b\u221anc}. For any region r1,i,\nwe then select b\n\n\u221a\nnc \u2212 1 regions without replacement,\n\nwhich have the largest similarity between them and r1,i,\nrepresented as {r2,i,1, r2,i,2, \u00b7 \u00b7 \u00b7 , r2,i,b\u221anc\u22121}, and we con-\nnect r1,i to r2,i,j (j = 1, 2, \u00b7 \u00b7 \u00b7 , b\n\n\u221a\nnc \u2212 1). After that, we\n\nconnect a region r2,i,j to regions r2,i,k and r2,u,j , where k \u2208\n{{1, 2, ..., b\n\n\u221a\nnc \u2212 1}\\{j}} and u \u2208 {{1, 2, ..., b\n\n\u221a\nnc}\\{i}}.\n\nFinally, if\n\u221a\nn /\u2208 Z, the remaining regions woule be con-\n\nnected to r1,i where i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , b\n\u221a\nnc}, represented\n\nas {r3,1, r3,2, \u00b7 \u00b7 \u00b7 , r3,n\u2212b\u221anc2}. If\n\u221a\nn \u2208 Z, we randomly\n\nselect a region from r1,i, and connect it to r1,j , where\nj \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , b\n\n\u221a\nnc}\\{i}, represented as r\u2217.\n\nTheorem 1. The degree of any node in the region connected graph\nis O(\n\n\u221a\nn), and the distance between any two nodes in the graph\n\nis less than 2.\n\n\n\n6\n\nProof. The generation of the region connected graph ensures\nthat a node is connected to at most max(2\u00d7 b\n\n\u221a\nnc \u2212 2, n\u2212\n\nb\n\u221a\nnc2 + b\n\n\u221a\nnc \u2212 1) other nodes, and hence the degree is\n\nO(\n\u221a\nn).\n\nThe distance of (r3,i, r1,j) (or (r\u2217, r1,j)) and (r1,j , r2,j,k)\nis both 1, where i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n \u2212 b\n\n\u221a\nnc2}, j \u2208\n\n{1, 2, \u00b7 \u00b7 \u00b7 , b\n\u221a\nnc}, and k \u2208 {1, 2, ..., b\n\n\u221a\nnc \u2212 1}. Thus, the\n\ndistance between r3,i (or r\u2217) and any other region is no more\nthan 2 in the graph.\n\nFor region r1,j , since the distance of (r1,j , r2,j,k) and\n(r2,j,k, r2,m,k) is both 1 where k \u2208 {1, 2, ..., b\n\n\u221a\nnc \u2212 1} and\n\nm \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , b\n\u221a\nnc}\\{j}, the distance of (r1,j , r2,m,k) is\n\nhence 2. Thus, the distance between r1,j and any other\nregion is also no more than 2.\n\nIn terms of r2,j,k, as the distance of (r2,j,k, r2,m,k) and\n(r2,j,k, r2,j,v) is 1, where m \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , b\n\n\u221a\nnc}\\{j} and v \u2208\n\n{1, 2, \u00b7 \u00b7 \u00b7 , b\n\u221a\nnc \u2212 1}\\{k}, the distance between r2,j,k and\n\nany other regions is hence no more than 2. Therefore, the\ndistance between any two regions in the graph is less than\n2.\n\nBecause the distance between any two regions in the\nproposed graph is less than 2, the dependencies between\nany two regions could be considered if the layer number of\nthe aggregation network is larger than 2.\n\n4.3 Dependency Learning for Individual Time Slot (DLI)\nGiven the STF embedding of regions for time tj and the gen-\nerated region connected graph, ST-TIS captures the spatial-\ntemporal dependencies between regions based on an ex-\ntended Transformer encoder. Following the canonical Trans-\nformer, ST-TIS employs an multi-head attention mechanism,\nso that it could account for different dependencies between\nregions. For the m-th head, the attention score between ri\nand rv at tj is defined as\n\nAm(ri, rv, tj) =\n(Ltji \u00b7WQm) \u00b7 (L\n\ntj\nv \u00b7WKm)T\u221a\n\nd\n, (5)\n\nwhere Ltji \u2208 R\n1\u00d7d and Ltjv \u2208 R1\u00d7d are the STF embedding\n\nof regions ri and rv at tj (Equation 4), WQm \u2208 Rd\u00d7d\nand WKm \u2208 Rd\u00d7d are learnable parameters, and d is a\nhyperparameter for the embedding size.\n\nUnlike the canonical Transformer, we do not evaluate the\nattention scores between one region and all other regions.\nInstead, we only compute one\u2019s attention scores with its\nneighbouring regions in the region connected graph, and\naggregate their embedding in terms of their attention score\nto update the region\u2019s embedding. For the m-th head, the\nembedding of a region ri at time tj is then updated as\n\nL\u0302tji,m =\n\u2211\n\nrv\u2208Neigh(ri)\n\nsoftmax(Am(ri, rv, tj)) \u00b7 Ltjv\n\n=\n\u2211\n\nrv\u2208Neigh(ri)\n\nexp(Am(ri, rv, tj))\u2211\nru\u2208Neigh(ri) exp(Am(ri, ru, tj))\n\n\u00b7 Ltjv ,\n\n(6)\n\nwhere L\u0302tji,m \u2208 R\n1\u00d7d is the output of the m-th head,\n\nNeigh(ri) is the neighbouring regions of ri in the graph,\nAm(ri, rv, tj) is the attention score defined in Equation 5.\nAs the degree of any node is O(\n\n\u221a\nn), the computation\n\nFig. 5. Processing of dependency learning for individual time slot.\n\ncomplexity of attention score evaluation and embedding\naggregation is hence O(n\n\n\u221a\nn) for all regions in a layer.\n\nFinally, we concatenate the results of multi-heads and\nthe embedding of ri is computed as\n\nL\u0302tji = Concat(L\u0302\ntj\ni,1, \u00b7 \u00b7 \u00b7 , L\u0302\n\ntj\ni,M ) \u00b7W\n\nO, (7)\n\nwhere Concat(\u00b7) is the concatenation operation, L\u0302ti \u2208 R\n1\u00d7d\n\nis the embedding of ri, WO \u2208 R(d\u00d7M)\u00d71 are learnable\nparameters and M is the number of heads.\n\nFollowing the structure of Transformer [10], the output\nof the multi-head region attention layers L\u0302tji is then passed\nto a fully connected neural network (Figure 5). We also em-\nploy a residual connection between each of the two layers.\nAs we discussed in Section 4.2, the information propagation\nis achieved with a multi-layer network structure. Thus, we\nstack the layers \u03b1 times (the effect of \u03b1 will be discussed\nin Section 5.7). We denote the final output of the DLI as\nRtj = {Rtj1 ,R\n\ntj\n2 , \u00b7 \u00b7 \u00b7 ,R\n\ntj\nn }, where R\n\ntj\ni is the embedding of\n\nregion ri at tj .\nCompared with the canonical Transformer, we only need\n\nto compute the attention scores and aggregate the embed-\nding between adjacent nodes in the region connected graph.\nThe computation complexity is hence reduced from O(n2)\nto O(n\n\n\u221a\nn) for each layer.\n\n4.4 Dependency Learning over Multiple Time\nSlots (DLM)\nConsidering the spatial-temporal dependency may have pe-\nriorical characteristic, DLM learns the periodic dependency\nby evaluating the correlation between Rtn and the depen-\ndency at other historical time slots Rt\u0302i (where t\u0302 < t). After\nthat, it generates a new embedding for ri by aggregating\nthe embedding at different time slots according to their\ncorrelations.\n\nSpecifically, we consider the short-term and long-term\nperiod for traffic data in this work. The spatial-temporal\ndependency at the following historical time slots are used\nas the model input to predict the inflow and outflow of\n\n\n\n7\n\nregions at t: spatial-temporal dependency in the recent h\ntime slots (i.e., short-term period); the same time interval in\nthe recent l days (i.e.,long-term period).\n\nWe employ a point-wise aggregation with self-attention\nto evaluate their correlations and aggregate the embedding\naccordingly. To capture the multiple periodic dependency,\nwe use an multi-head attention network in DLM.\n\nGiven a set of historical time slot Q, the z-th dependency\non a historical time slot t\u0302 \u2208 Q is calculated as follows:\n\nez(t, t\u0302) =\n(Rti \u00b7W\n\nT\nQz\n\n) \u00b7 (Rt\u0302i \u00b7W\nT\nKz\n\n)T\n\u221a\nd\n\n, (8)\n\nwhere W TQz \u2208 R\nd\u00d7d and W TKz \u2208 R\n\nd\u00d7d are learnable param-\neters.\n\nWe use a softmax function to normalize the dependency\nand aggregate the context of each time slot by weight:\n\n\u03b2z(t, t\u0302) = softmax(ez(t, t\u0302)) =\nexp(ez(t, t\u0302))\u2211\n\ntp\u2208Q exp(ez(t, tp))\n. (9)\n\nThe aggregation of the z-th head is hence\n\nR\u0302ti,z = (\n\u2211\nt\u0302\u2208Q\n\n(\u03b2z(t, t\u0302) \u00b7 Rt\u0302i) \u00b7W\nT\nV , (10)\n\nwhere Q is the set of historical time slots, R\u0302ti,z is the\naggregation result of the z-th head, and W TV \u2208 R\n\nd\u00d7d are\nlearnable parameters. Finally, we concatenate the results of\ndifferent heads with\n\nR\u0302ti = Concat(R\u0302\nt\ni,1, R\u0302\n\nt\ni,2, \u00b7 \u00b7 \u00b7 , R\u0302\n\nt\ni,Z) \u00b7WT , (11)\n\nwhere Concat(\u00b7) is the concatenation operation, and WT \u2208\nR(Z\u00d7d)\u00d7d are learnable parameters. We then pass R\u0302ti to\na fully connected neural network to obtain the spatial-\ntemporal embedding of ri at t. Note that we also employ a\nresidual connection between each of the two layers to avoid\ngradient exploding or vanishing. The output of the DLM is\ndenoted as \u2126ti for region ri at t.\n\nDifferent from prior works, the periodic dependency\nlearning is conditional on the spatial-temporal dependency\nat each individual time slot. Consequently, the spatial and\ntemproal dependencies are jointly considered during the\nperiodic dependency learning. The computation complexity\nis O(|Q|), where |Q| is the number of historical time slots\nused for learning.\n\n4.5 Prediction Network (PN)\nGiven the spatial-temporal embedding T tri of a region, the\nprediction network predicts the inflow and outflow using\nthe fully connected network. The forecasting function is\ndefined as\n\n[I\u0302ti , O\u0302\nt\ni ] = \u03c3(\u2126\n\nt\ni \u00b7W\n\nP + bP ), (12)\n\nwhere I\u0302ti and O\u0302\nt\ni is the forecasting inflow and outflow\n\nrespectively, \u03c3(\u00b7) is the ReLU activation function and WP \u2208\nRd\u00d72, and bP \u2208 R1\u00d72 are learnable parameters.\n\nWe simultaneously forecast the inflow and outflow in\nour work, and define the loss function as follows:\n\nLOSS =\n\n\u221a\u2211n\ni=1(I\n\nt\ni \u2212 I\u0302\n\nt\ni )\n\n2 +\n\u2211n\n\ni=1(O\nt\ni \u2212 O\u0302\n\nt\ni)\n\n2\n\n2n\n, (13)\n\nwhere n is the number of regions.\n\n5 ILLUSTRATIVE EXPERIMENTAL RESULTS\nIn this section, we first introduce the datasets and the data\nprocessing approaches in Section 5.1, and the evaluation\nmetrics and baseline approaches in Section 5.2. Then, we\ncompare the accuracy and training efficiency of ST-TIS with\nthe state-of-the-art methods in Sections 5.3 and 5.4, respec-\ntively. After that, we evaluate the performance of variants\nof ST-TIS and the effect of surrounding observations in\nSections 5.6 and 5.5, respectively, followed by the discussion\non the hyperparameters of layer number in Section 5.7 and\nhead number in Section 5.8.\n\n5.1 Datasets\nWe conduct extensive traffic study and model evaluations\nbased on two real-world traffic flow datasets collected in\nNew York City (NYC), the NYC-Taxi dataset and the NYC-\nBike dataset. Each dataset contains trip records, each of\nwhich consists of origin, destination, departure time, and\narrival time. The NYC-Taxi dataset contains 22, 349, 490\ntaxi trip records of NYC in 2015, from 01/01/2015 to\n03/01/2015. The NYC-Bike dataset was collected from the\nNYC Citi Bike system from 07/01/2016 to 08/29/2016, and\ncontains 2, 605, 648 trip records.\n\nThe city is split into 10 \u00d7 20 regions with a size of\n1km\u00d71km. The time interval is set as 30 minutes for both\ndatasets. The two datasets were pre-processed and released\nonline1 by the prior work [7].\n\nIn both of the taxi dataset and bike dataset, we use\ndata from the previous 40 days as the training data, and\nthe remaining 20 days as the testing data. In the training\ndata, we select 80% of the training data to train our model\nand the remaining 20% for validation. We use the Min-Max\nnormalization to rescale the range of volume value in [0, 1],\nand recover the result for evaluation after forecasting. In\nour experiments, we exclude the results of those regions the\ninflow or outflow of which is less than 10 when evaluating\nthe model. It is a common practice used in industry and\nmany prior works [6], [7], [42].\n\n5.2 Performance Metrics and Baseline Methods\nWe use Root Mean Squared Errors (RMSE) and Mean Av-\nerage Percentage Error (MAPE) as the evaluation metrics,\nwhich are defined as follows:\n\nRMSE =\n\n\u221a\u2211N\ni=1(yi \u2212 y\u0302i)2\n\nN\n, (14)\n\nMAPE =\n1\n\nN\n\nN\u2211\ni=1\n\n|yi \u2212 y\u0302i|\nyi\n\n, (15)\n\nwhere yi and y\u0302i are the ground-truth and forecasting result\nof the i-th sample, and N is the total number of samples.\n\nWe compare our model with the following state-of-the-\nart approaches:\n\n\u2022 Historical average (HA): It uses the average of traffic at\nthe same time slots in historical data for prediction.\n\n\u2022 ARIMA: It is a conventional approach for time series\ndata forecasting.\n\n1. https://github.com/tangxianfeng/STDN/blob/master/data.zip\n\n\n\n8\n\nTABLE 1\nComparison with the state-of-the-art methods.\n\nDataset Method\nInflow Outflow\n\nRMSE MAPE RMSE MAPE\n\nNYC-Taxi\n\nHA 33.83 21.14% 43.82 23.18%\nARIMA 27.25 20.91% 36.53 22.21%\nRidge 24.38 20.07% 28.51 19.94%\n\nXGBoost 21.72 18.70% 26.07 19.35%\nMLP 22.08\u00b10.50 18.31\u00b10.83% 26.67\u00b10.56 18.43\u00b10.62%\n\nConvLSTM 23.67\u00b10.20 20.70\u00b10.20% 28.13\u00b10.25 20.50\u00b10.10%\nST-ResNet 21.63\u00b10.25 21.09\u00b10.51% 26.23\u00b10.33 21.13\u00b10.63%\n\nSTDN 19.05\u00b10.31 16.25\u00b10.26% 24.10\u00b10.25 16.30\u00b10.23%\nASTGCN 22.05\u00b10.37 20.25\u00b10.26% 26.10\u00b10.25 20.30\u00b10.31%\nSTGODE 21.46\u00b10.42 19.22\u00b10.36% 27.24\u00b10.46 19.30\u00b10.34%\nSTSAN 23.07\u00b10.64 22.24\u00b11.91% 27.83\u00b10.30 25.90\u00b11.67%\nDSAN 18.32\u00b10.39 16.07\u00b10.31% 24.27\u00b10.30 17.70\u00b10.35%\nST-TIS 17.73\u00b10.23 14.65\u00b10.32% 21.96\u00b10.13 14.83\u00b10.76%\n\nNYC-Bike\n\nHA 11.93 27.06% 12.49 27.82%\nARIMA 11.25 25.79% 11.53 26.35%\nRidge 10.33 24.58% 10.92 25.29%\n\nXGBoost 8.94 22.54% 9.57 23.52%\nMLP 9.12\u00b10.24 22.40\u00b10.40% 9.83\u00b10.19 23.12\u00b10.24%\n\nConvLSTM 9.22\u00b10.19 23.20\u00b10.47% 10.40\u00b10.17 25.10\u00b10.45%\nST-ResNet 8.85\u00b10.13 22.98\u00b10.53% 9.80\u00b10.12 25.06\u00b10.36%\n\nSTDN 8.15\u00b10.15 20.87\u00b10.39% 8.85\u00b10.11 21.84\u00b10.36%\nASTGCN 9.05\u00b10.31 22.25\u00b10.36% 9.34\u00b10.24 23.13\u00b10.30%\nSTGODE 8.58\u00b10.38 23.33\u00b10.26% 9.23\u00b10.31 23.99\u00b10.23%\nSTSAN 8.20\u00b10.45 20.42\u00b11.33% 9.87\u00b10.23 23.87\u00b10.71%\nDSAN 7.97\u00b10.25 20.23\u00b10.18% 10.07\u00b10.58 23.92\u00b10.39%\nST-TIS 7.57\u00b10.04 18.64\u00b10.23% 7.73\u00b10.10 18.58\u00b10.19%\n\n\u2022 Ridge Regression: A regression approach for time se-\nries data forecasting.\n\n\u2022 XGBoost [46]: A powerful approach for building su-\npervised regression models.\n\n\u2022 Multi-Layer Perceptron (MLP): A three-layer fully-\nconnected neural network.\n\n\u2022 Convolutional LSTM (ConvLSTM) [47]: It is a special\nrecurrent neural network with a convolution struc-\nture for spatial-temporal prediction.\n\n\u2022 ST-ResNet [5]: It uses multiple convolutional net-\nworks with residual structures to capture spatial cor-\nrelations from different temporal periods for traffic\nforecasting. It also considers external data such as\nweather, holiday events, and metadata.\n\n\u2022 STDN [7]: It considers the dynamic spatial correla-\ntion and temporal shifted problem using the com-\nbination of CNN and LSTM. External data such as\nweather and event are considered in the work.\n\n\u2022 ASTGCN [48]: It is an attention-based spatial-\ntemporal graph convolutional network (ASTGCN)\nmodel to solve traffic flow forecasting problem.\n\n\u2022 STGODE [31]: It uses a spatial-temporal graph ordi-\nnary differential equation network to predict traffic\nflow based on two predefine graph, namely a spatial\ngraph in terms of distance, and a semantic graph in\nterms of flow similarity.\n\n\u2022 STSAN [12]: It uses CNN to capture spatial infor-\nmation and the canonical Transformer to consider\nthe temporal dependencies over time. In particular,\ntransition data between regions are used to indicate\nthe correlation between regions.\n\n\u2022 DSAN [13]: It uses the canonical Transformer to\ncapture the spatial-temporal correlations for spatial-\ntemporal prediction, in which transition data be-\n\ntween regions are used for correlation modeling.\n\nWe use the identical datasets and data process approach\nas the work STDN [7], and use the results of the work [7] as\nthe benchmark for discussion. The experiment results of (1)\n\u223c (8) in Table 1 are reported in the work [7]. The evaluation\nof ASTGCN, STGODE, STSAN, and DSAN is based on the\ncode from their authors\u2019 GitHubs.\n\nWe implement our model using PyTorch. Data and code\ncan be found in https://github.com/GuanyaoLI/ST-TIS.\nWe use the following data as the model input since they\nachieve the best performance on the validation datasets:\ndata in the recent past 3 hours (i.e., h = 6 as the slot duration\nis 30 minutes); the same time slot in the recent past 10\ndays (i.e., l = 10). The other default hyperparameter settings\nare as follows. The default length w of the surrounding\nobservations is 6 (i.e. 3 hours), the number of convolution\nkernels F is 4, and the dimension d is set as 8. The number\nof k for DLI in Figure 5 is set as 3, and the number of heads\nis set as 6 for the two modules. Furthermore, the dropout\nrate is set as 0.1, the learning rate is set as 0.001, and the\nbatch size is set to be 32. Adam optimizer is used for model\ntraining. We trained our model on a machine with a NVIDIA\nRTX2080 Ti GPU.\n\n5.3 Prediction Accuracy\nWe compared the accuracy of ST-TIS with the state-of-the-\nart methods using the metrics RMSE and MAPE. The results\nfor the two datasets are presented in Table 1. Each approach\nwas run 10 times, and the mean and standard deviation are\nreported. As shown in the table, ST-TIS significantly outper-\nforms all other approaches on both metrics and datasets.\n\nSpecifically, the performance of the conventional time\nseries forecasting approaches (HA and ARIMA) is poor for\n\n\n\n9\n\nTABLE 2\nComparison of training time.\n\nDataset Method\nAverage time\nper epoch (s) Total time (s)\n\nNYC-Taxi\n\nST-ResNet 7.31 3077.51\nSTDN 445.47 34746.66\n\nASTGCN 25.31 6272.88\nSTGODE 18.53 3423.48\nSTSAN 426.75 33769.32\nDSAN 386.17 29390.75\nST-TIS 10.21 1231.5\n\nNYC-Bike\n\nST-ResNet 7.25 2921.75\nSTDN 480.21 23066.43\n\nASTGCN 25.68 5084.64\nSTGODE 18.76 3752.89\nSTSAN 426.28 31216.28\nDSAN 434.03 26476.20\nST-TIS 10.37 1556.8\n\nboth datasets because these approaches do not consider\nthe spatial dependency. Conventional machine learning\napproaches (Ridge, XGBoost, and MLP), which consider\nspatial dependency as features, have better performance\nthan HA and ARIMA. However, they fail to consider the\njoint spatial-temporal dependencies between regions. Most\ndeep learning-based models have further improvements\nthan conventional works, illustrating the ability of deep\nneural networks to capture the complicated spatial and\ntemporal dependency. ST-TIS is substantially more accurate\nthan state-of-the-art approaches (i.e., ConvLSTM, STResNet,\nSTDN, ASTGCN and STGODE). For example, it has an\naverage improvement of 9.5% on RMSE and 12.4% on\nMAPE compared to STDN and DSAN. The reasons for\nthe improvements are that it can capture the correlations\nbetween both nearby and distant regions, and it considers\nthe spatial and temporal dependency in a joint manner.\nWe find that the improvement is more significant on the\nNYC-Taxi dataset than on the NYC-Bike dataset. The reason\ncould be that people prefer using taxis instead of bikes for\nlong-distance travel, and so the correlations with distant\nregions are more important for the prediction task on the\ntaxi dataset. The significant improvement demonstrates that\nST-TIS has a better ability to capture the correlations for\ndistant regions than the other approaches which have the\nspatial locality assumption. ST-TIS also outperforms other\nTransformer-based approaches (such as STSAN and DSAN).\nThe reason is that with the information fusion and region\nsampling strategies in ST-TIS, the long-tail issue of the\ncanonical Transformer is addressed and the joint spatial-\ntemporal correlations are considered. The significant im-\nprovements demonstrate the effectiveness of our proposed\nmodel.\n\n5.4 Training Efficiency\n\nTraining and deploying large deep learning models is costly.\nFor example, the cost of trying combinations of different\nhyper-parameters for a large model is computationally ex-\npensive and it highly relies on training resources [3]. Thus,\nwe compare the training efficiency of ST-TIS with some\nstate-of-the-art deep learning based approaches (i.e., ST-\nResNet, STDN, ASTGCN, STGODE, STSAN, and DSAN) in\nterms of training time and number of learnable parameters.\n\nTABLE 3\nComparison of the number of parameters.\n\nMethod Number of parameters\nST-ResNet 4,917,041\n\nSTDN 9,446,274\nASTGCN 450,031\nSTGODE 433,073\n\nDSAN 1,621,634\nSTSAN 34,327,298\nST-TIS 139,506\n\n 16\n\n 18\n\n 20\n\n 22\n\n 24\n\n 26\n\nInflow Outflow\n\nR\nM\n\nS\nE\n\n NoIMF\n\n NoDLM\n\n NoRSM\n\n ST\u2212TIS\n\n(a) RMSE.\n\n14%\n\n15%\n\n16%\n\n17%\n\n18%\n\n19%\n\nInflow Outflow\n\nM\nA\n\nP\nE\n\n NoIMF\n\n NoDLM\n\n NoRSM\n\n ST\u2212TIS\n\n(b) MAPE.\n\nFig. 6. Performance of variants on the NYC-Taxi dataset.\n\nThe results of the average training time per epoch and\nthe total training time are presented in Table 2. ST-ResNet\nachieves the least training time among all comparison ap-\nproaches because it solely employs simple CNN and does\nnot rely on RNN for temporal dependency learning. The\naverage time per epoch of ST-TIS is close to ST-ResNet. In\naddition, ST-TIS is trained significantly faster than other\napproaches (with a reduction of 46% \u223c 95%). STDN uses\nLSTM to capture temporal correlation, which is in general\nmore difficult to be trained in parallel. STSAN and DSAN\nalso employ Transformer with self-attention for spatial and\ntemporal correlation learning, but our proposed approach is\nsignificantly efficient than them, illustrating the efficiency of\nthe proposed region graph for model training.\n\nFurthermore, we also compare the number of learnable\nparameters of each model in Table 3. More parameters may\nlead to difficulties in model training, and it requires more\nmemory and training resources. Compared with other state-\nof-the-art approaches, ST-TIS is much more lightweight\nwith fewer parameters for training (with a reduction of\n23% \u223c 98%). The comparison results in Tables 1, 2 and 3\ndemonstrate that our proposed ST-TIS is faster and more\nlightweight than other deep learning based baseline ap-\nproaches, while achieving even better prediction accuracy.\n\n5.5 Design Variations of ST-TIS\nWe compare ST-TIS with its variants to evaluate the effec-\ntiveness of the proposed modules. The following variants\nare discussed:\n\n\u2022 NoIFM: We remove the information fusion module\nfrom ST-TIS. Only the surrounding observation of a\ntime slot is used as the input of the model instead of\nthe fusion result.\n\n\u2022 NoRSM: We remove the region sampling module\nfrom ST-TIS. The canonical Transformer is used to\ncapture the dependencies between regions without\nregion sampling.\n\n\n\n10\n\n 7.5\n\n 8\n\n 8.5\n\n 9\n\n 9.5\n\nInflow Outflow\n\nR\nM\n\nS\nE\n\n NoIMF\n\n NoDLM\n\n NoRSM\n\n ST\u2212TIS\n\n(a) RMSE.\n\n17%\n\n18%\n\n19%\n\n20%\n\n21%\n\n22%\n\n23%\n\n24%\n\n25%\n\nInflow Outflow\n\nM\nA\n\nP\nE\n\n NoIFM\n\n NoDLM\n\n NoRSM\n\n ST\u2212TIS\n\n(b) MAPE.\n\nFig. 7. Performance of variants on the NYC-Bike dataset.\n\n 17\n\n 19\n\n 21\n\n 23\n\n 25\n\n 1  2  3  4  5  6  7  8\n\nR\nM\n\nS\nE\n\nw\n\n outflow\n\n inflow\n\n(a) RMSE.\n\n14%\n\n15%\n\n16%\n\n17%\n\n 1  2  3  4  5  6  7  8\n\nM\nA\n\nP\nE\n\nw\n\n inflow\n\n outflow\n\n(b) MAPE.\n\nFig. 8. Impact of surrounding observations on the NYC-Taxi dataset.\n\n\u2022 NoDLM: We remove the module of dependency\nlearning over multiple time slots, and only use Rti\nas the input of the prediction network for prediction.\n\nThe RMSE and MAPE on the NYC-Taxi dataset are\npresented in Figures 6(a) and 6(b), respectively. After tak-\ning the information fusion module away, the performance\ndegrades significantly. The reason is that the information\nfusion module plays a fundermental role in jointly con-\nsidering the spatial-temporal dependency. Without such\nmodule, our approach would degenerate to consider the\nspatial and temporal dependency in a decouple manner.\nThe experimental results demonstrate the importance of\nconsidering spatial-temporal dependency jointly and the\neffective of the proposed information fusion module. More-\nover, without the region sampling module, our approach\nstill achieve a good prediction performance because the\ncanonical Transformer is good at capturing dependencies\nbetween regions. The performance is further improved with\nthe region sampling since it could address the long-tail issue\nof the canonical Transformer for embedding aggregation.\nFurthermore, the RMSE and MAPE increase when the peri-\nodical characteristic of spatial-temporal dependency is not\nconsidered (i.e., NoDLM), which indicates the necessity of\nconsidering the period of spatial-temporal dependency and\ndemonstrates the effectiveness and rationality of our model\ndesign. Similar and consistent findings can be observed on\nthe NYC-Bike dataset in Figures 7(a) and 7(b).\n\n5.6 Surrounding Observations\nRecall that the dependency between two regions may have\ntemporal lagging due to the travel time between them. To\ncapture such lagging dependency, ST-TIS uses the surround-\ning observations to learn the region correlations for each\ntime slot, which contains the inflow and outflow in the\nprevious w time slots. We thus evaluate the impact of w\non the performance of our model.\n\n 7\n\n 7.5\n\n 8\n\n 8.5\n\n 9\n\n 1  2  3  4  5  6  7  8\n\nR\nM\n\nS\nE\n\nw\n\n outflow\n\n inflow\n\n(a) RMSE.\n\n19%\n\n20%\n\n20%\n\n20%\n\n21%\n\n22%\n\n22%\n\n 1  2  3  4  5  6  7  8\n\nM\nA\n\nP\nE\n\nw\n\n outflow\n\n inflow\n\n(b) MAPE.\n\nFig. 9. Impact of surrounding observations on the NYC-Bike dataset.\n\n 17\n\n 19\n\n 21\n\n 23\n\n 1  2  3  4  5  6\n\nR\nM\n\nS\nE\n\n\u03b1\n\n outflow\n\n inflow\n\n(a) RMSE.\n\n14%\n\n15%\n\n16%\n\n17%\n\n 1  2  3  4  5  6\n\nM\nA\n\nP\nE\n\n\u03b1\n\n outflow\n\n inflow\n\n(b) MAPE.\n\nFig. 10. Impact of layer number on the NYC-Taxi dataset.\n\nFigures 8(a) and 8(b) show the RMSE and MAPE versus\ndifferent lengths on the NYC-Taxi dataset. A larger length\nw indicates that more information is encoded from the sur-\nrounding observations. When w = 1, only the observation\nat a time slot is used for dependency learning, and the\nmodel fails to capture the lagging characteristic of depen-\ndency. As the length increases, the RMSE and MAPE of\nboth inflow and outflow forecasting decrease (w \u2264 5). The\nperformance improvement demonstrates the importance of\nusing the surrounding observations to learn the dependen-\ncies between regions. RMSE and MAPE increase slightly\nbut remain stable when the length is large (w \u2265 5). The\npotential reason is that, when w is larger than the travel time\nbetween regions, increasing w would not introduce more\ninformation for dependency learning. On the other hand, a\nlarger w may introduce some noise and more parameters for\nthe model, leading to difficulties in model training [13]. The\nRMSE and MAPE versus different lengths of surrounding\nobservations on the NYC-Bike dataset are shown in Figures\n9(a) and 9(b), which are consistent with the results for the\nNYC-Taxi dataset. When the length is small (w \u2264 4), RMSE\nand MAPE decrease as the length becomes larger, but they\nslightly increase when the length is large (w \u2265 4).\n\n5.7 Layer Number\n\nIn ST-TIS, DLI is stacked \u03b1 times to ensure the informa-\ntion propagtion between regions and improve the model\nrobustness. We evaluate the effect of \u03b1 on the prediction\nperformance using RMSE and MAPE. The results for the taxi\ndataset are presented in Figure 10. When \u03b1 = 1, only the\ndependencies on one\u2019s neighbouring regions in the graph\nare considering, resulting in the worst prediction accuracy.\nWhen \u03b1 \u2265 2, the dependencies on all other regions could be\ncaptured. We find that with the layer number \u03b1 increases,\nthe RMSE and the MAPE declines for both inflow and out-\nflow, indicating the performance improvement. However,\n\n\n\n11\n\n 7\n\n 7.5\n\n 8\n\n 8.5\n\n 1  2  3  4  5  6\n\nR\nM\n\nS\nE\n\n\u03b1\n\n outflow\n\n inflow\n\n(a) RMSE.\n\n18%\n\n19%\n\n20%\n\n21%\n\n 1  2  3  4  5  6\n\nM\nA\n\nP\nE\n\n\u03b1\n\n inflow\n\n outflow\n\n(b) MAPE.\n\nFig. 11. Impact of layer number on the NYC-Bike dataset.\n\n 17\n\n 19\n\n 21\n\n 23\n\n 25\n\n 1  2  3  4  5  6  7  8\n\nR\nM\n\nS\nE\n\nM\n\n outflow\n\n inflow\n\n(a) RMSE.\n\n14%\n\n15%\n\n16%\n\n17%\n\n18%\n\n19%\n\n 1  2  3  4  5  6  7  8\n\nM\nA\n\nP\nE\n\nM\n\n outflow\n\n inflow\n\n(b) MAPE.\n\nFig. 12. Impact of head number on the NYC-Taxi dataset.\n\nwe find that when the layer number is too large (\u03b1 > 5\nin our experiments), the performance on RMSE and MAPE\ndegrades, because too many layers may result in difficulties\nof model training. Similar results are observed on the bike\ndataset (Figure 11).\n\n5.8 Head Number\n\nWe use the multi-head attention mechanism in ST-TIS for\ndependency learning, so that different heads could capture\ndifferent patterns from the historical data. In our experi-\nments, we evaluate the impact of the head number M on\nthe prediction performance. The results of RMSE and MAPE\nversus the head number M on the taxi and bike datasets are\npresented in Figures 12 and 13, respectively. As shown in\nFigures 12(a) and 13(a), with the head number increases,\nthe RMSE on the two datasets declines, demonstrating\nthe multi-head mechanism could benefit the dependency\nlearning and improve the prediction accuracy. We also ob-\nserve that when the head number become larger (M > 4\non the taxi dataset while M > 6 on the bike dataset),\nthe improvements are not significant. The reason could\nbe that some heads may focus on the same pattern when\nthere are many heads. In terms of MAPE, similar findings\ncould be observed in Figures 12(b) and 13(b). Moreover, the\nMAPE increases slightly when the head number becomes\nlarge (M > 6). It is because increasing the head number\nleads to more learnable parameters, which would result in\ndifficulties for model training.\n\n6 CONCLUSION\nWe propose ST-TIS, a novel, small (in parameters), com-\nputationally efficient and highly accurate model for traffic\nforecasting. ST-TIS employs a spatial-temporal Transformer\nwith information fusion and region sampling to jointly\nconsider the dynamic spatial and temporal dependencies\n\n 7.5\n\n 8\n\n 8.5\n\n 9\n\n 1  2  3  4  5  6  7  8\n\nR\nM\n\nS\nE\n\nM\n\n outflow\n\n inflow\n\n(a) RMSE.\n\n18%\n\n19%\n\n20%\n\n21%\n\n22%\n\n 1  2  3  4  5  6  7  8\n\nM\nA\n\nP\nE\n\nM\n\n outflow\n\n inflow\n\n(b) MAPE.\n\nFig. 13. Impact of head number on the NYC-Bike dataset.\n\nbetween regions at any individual time slots, and also the\npossibly periodic spatial-temporal dependency from multi-\nple time slots. In particular, ST-TIS boosts the efficiency and\naddresses the long-tail issue of the canonical Transformer\nusing a novel region sampling strategy, which reduces the\ncomplexity from O(n2) to O(n\n\n\u221a\nn), where n is the number\n\nof regions. We have conducted extensive experiments to\nevaluate ST-TIS, using a taxi and a bike sharing datasets.\nOur experimental results show that ST-TIS significantly out-\nperforms the state-of-the-art approaches in terms of training\nefficiency (with a reduction of 46% \u223c 95% on training time\nand 23% \u223c 98% on network parameters), and hence is\nefficient in tuning, training and memory. Despite its small\nsize and fast training, it achieves higher accuracy in its\nonline predictions than other state-of-the-art works (with\nimprovement of up to 9.5% on RMSE, and 12.4% on MAPE).\n\nREFERENCES\n[1] Y. Zheng, L. Capra, O. Wolfson, and H. Yang, \u201cUrban computing:\n\nconcepts, methodologies, and applications,\u201d ACM Transactions on\nIntelligent Systems and Technology (TIST), vol. 5, no. 3, pp. 1\u201355,\n2014.\n\n[2] W. Jiang and J. Luo, \u201cGraph neural network for traffic forecasting:\nA survey,\u201d arXiv preprint arXiv:2101.11174, 2021.\n\n[3] G. Menghani, \u201cEfficient deep learning: A survey on making\ndeep learning models smaller, faster, and better,\u201d arXiv preprint\narXiv:2106.08962, 2021.\n\n[4] J. Zhang, Y. Zheng, D. Qi, R. Li, and X. Yi, \u201cDnn-based prediction\nmodel for spatio-temporal data,\u201d in Proceedings of the 24th ACM\nSIGSPATIAL International Conference on Advances in Geographic\nInformation Systems. California, USA: ACM, 2016, pp. 1\u20134.\n\n[5] J. Zhang, Y. Zheng, and D. Qi, \u201cDeep spatio-temporal residual\nnetworks for citywide crowd flows prediction,\u201d in Thirty-First\nAAAI Conference on Artificial Intelligence. California USA: AAAI,\n2017, pp. 1655 \u2013 1661.\n\n[6] H. Yao, F. Wu, J. Ke, X. Tang, Y. Jia, S. Lu, P. Gong, J. Ye,\nand Z. Li, \u201cDeep multi-view spatial-temporal network for taxi\ndemand prediction,\u201d in Thirty-Second AAAI Conference on Artificial\nIntelligence. Louisiana, USA: AAAI, 2018, pp. 2588 \u2013 2595.\n\n[7] H. Yao, X. Tang, H. Wei, G. Zheng, and Z. Li, \u201cRevisiting spatial-\ntemporal similarity: A deep learning framework for traffic predic-\ntion,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence,\nvol. 33. New York, USA: AAAI, 2019, pp. 5668\u20135675.\n\n[8] B. Yu, H. Yin, and Z. Zhu, \u201cSpatio-temporal graph convolutional\nnetworks: a deep learning framework for traffic forecasting,\u201d in\nProceedings of the 27th International Joint Conference on Artificial\nIntelligence. Stockholm, Sweden: IJCAI, 2018, pp. 3634\u20133640.\n\n[9] X. Geng, Y. Li, L. Wang, L. Zhang, Q. Yang, J. Ye, and Y. Liu,\n\u201cSpatiotemporal multi-graph convolution network for ride-hailing\ndemand forecasting,\u201d in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 33. Hawaii, USA: AAAI, 2019, pp. 3656\u2013\n3663.\n\n[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nin Advances in neural information processing systems. Long Beach,\nCA, USA: ACM, 2017, pp. 5998\u20136008.\n\n\n\n12\n\n[11] Y. Zhou, J. Li, H. Chen, Y. Wu, J. Wu, and L. Chen, \u201cA spatiotem-\nporal attention mechanism-based model for multi-step citywide\npassenger demand prediction,\u201d Information Sciences, vol. 513, pp.\n372\u2013385, 2020.\n\n[12] H. Lin, W. Jia, Y. You, and Y. Sun, \u201cInterpretable crowd flow\nprediction with spatial-temporal self-attention,\u201d arXiv preprint\narXiv:2002.09693, vol. [cs.LG], 2020.\n\n[13] H. Lin, R. Bai, W. Jia, X. Yang, and Y. You, \u201cPreserving dynamic\nattention for long-term spatial-temporal prediction,\u201d in Proceedings\nof the 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining. Virtual Conference: ACM, 2020, pp.\n36\u201346.\n\n[14] M. Xu, W. Dai, C. Liu, X. Gao, W. Lin, G.-J. Qi, and H. Xiong,\n\u201cSpatial-temporal transformer networks for traffic flow forecast-\ning,\u201d arXiv preprint arXiv:2001.02908, 2020.\n\n[15] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and\nW. Zhang, \u201cInformer: Beyond efficient transformer for long se-\nquence time-series forecasting,\u201d in Proceedings of AAAI, 2021.\n\n[16] S. Shekhar and B. M. Williams, \u201cAdaptive seasonal time series\nmodels for forecasting short-term traffic flow,\u201d Transportation Re-\nsearch Record, vol. 2024, no. 1, pp. 116\u2013125, 2007.\n\n[17] B. Pan, U. Demiryurek, and C. Shahabi, \u201cUtilizing real-world\ntransportation data for accurate traffic prediction,\u201d in 2012 IEEE\n12th International Conference on Data Mining. Brussels, Belgium:\nIEEE, 2012, pp. 595\u2013604.\n\n[18] L. Moreira-Matias, J. Gama, M. Ferreira, J. Mendes-Moreira, and\nL. Damas, \u201cPredicting taxi\u2013passenger demand using stream-\ning data,\u201d IEEE Transactions on Intelligent Transportation Systems,\nvol. 14, no. 3, pp. 1393\u20131402, 2013.\n\n[19] A. Abadi, T. Rajabioun, and P. A. Ioannou, \u201cTraffic flow prediction\nfor road transportation networks with limited traffic data,\u201d IEEE\ntransactions on intelligent transportation systems, vol. 16, no. 2, pp.\n653\u2013662, 2014.\n\n[20] B. L. Smith, B. M. Williams, and R. K. Oswald, \u201cComparison of\nparametric and nonparametric models for traffic flow forecasting,\u201d\nTransportation Research Part C: Emerging Technologies, vol. 10, no. 4,\npp. 303\u2013321, 2002.\n\n[21] R. Silva, S. M. Kang, and E. M. Airoldi, \u201cPredicting traffic volumes\nand estimating the effects of shocks in massive transportation\nsystems,\u201d Proceedings of the National Academy of Sciences, vol. 112,\nno. 18, pp. 5643\u20135648, 2015.\n\n[22] Y. Zhang and Y. Xie, \u201cForecasting of short-term freeway volume\nwith v-support vector machines,\u201d Transportation Research Record,\nvol. 2024, no. 1, pp. 92\u201399, 2007.\n\n[23] Y. Li, Y. Zheng, H. Zhang, and L. Chen, \u201cTraffic prediction in a\nbike-sharing system,\u201d in Proceedings of the 23rd SIGSPATIAL Inter-\nnational Conference on Advances in Geographic Information Systems.\nSeattle, Washington: ACM, 2015, pp. 1\u201310.\n\n[24] Y. Tong, Y. Chen, Z. Zhou, L. Chen, J. Wang, Q. Yang, J. Ye, and\nW. Lv, \u201cThe simpler the better: a unified approach to predicting\noriginal taxi demands based on large-scale online platforms,\u201d in\nProceedings of the 23rd ACM SIGKDD international conference on\nknowledge discovery and data mining. Halifax, NS, Canada: ACM,\n2017, pp. 1653\u20131662.\n\n[25] T. Li, J. Zhang, K. Bao, Y. Liang, Y. Li, and Y. Zheng, \u201cAutost: Ef-\nficient neural architecture search for spatio-temporal prediction,\u201d\nin Proceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining. Virtual Conference: ACM,\n2020, pp. 794\u2013802.\n\n[26] S. He and K. G. Shin, \u201cTowards fine-grained flow forecasting: A\ngraph attention approach for bike sharing systems,\u201d in Proceedings\nof The Web Conference 2020. Taiwan: ACM, 2020, pp. 88\u201398.\n\n[27] H. Yao, C. Zhang, Y. Wei, M. Jiang, S. Wang, J. Huang, N. V.\nChawla, and Z. Li, \u201cGraph few-shot learning via knowledge\ntransfer,\u201d in Thirty-Forth AAAI Conference on Artificial Intelligence.\nNew York, USA: AAAI, 2020, pp. 6656 \u2013 6663.\n\n[28] G. Li, M. Muller, A. Thabet, and B. Ghanem, \u201cDeepgcns: Can\ngcns go as deep as cnns?\u201d in Proceedings of the IEEE International\nConference on Computer Vision. Seoul, Korea: IEEE, 2019, pp. 9267\u2013\n9276.\n\n[29] Z. Pan, Y. Liang, W. Wang, Y. Yu, Y. Zheng, and J. Zhang, \u201cUrban\ntraffic prediction from spatio-temporal data using deep meta\nlearning,\u201d in Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining. Anchorage,\nAK, USA: ACM, 2019, pp. 1720\u20131730.\n\n[30] X. Wang, Y. Ma, Y. Wang, W. Jin, X. Wang, J. Tang, C. Jia, and\nJ. Yu, \u201cTraffic flow prediction via spatial temporal graph neural\n\nnetwork,\u201d in Proceedings of The Web Conference 2020. Taiwan:\nACM, 2020, pp. 1082\u20131092.\n\n[31] Z. Fang, Q. Long, G. Song, and K. Xie, \u201cSpatial-temporal graph\node networks for traffic flow forecasting,\u201d in Proceedings of the\n27th ACM International Conference on knowledge Discovery and Data\nMining. Singapore: ACM, 2021.\n\n[32] M. Li and Z. Zhu, \u201cSpatial-temporal fusion graph neural net-\nworks for traffic flow forecasting,\u201d in Proceedings of the 27th ACM\nInternational Conference on knowledge Discovery and Data Mining.\nSingapore: ACM, 2021.\n\n[33] C. Song, Y. Lin, S. Guo, and H. Wan, \u201cSpatial-temporal syn-\nchronous graph convolutional networks: A new framework for\nspatial-temporal network data forecasting,\u201d in Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 34, no. 01, 2020, pp.\n914\u2013921.\n\n[34] H. Shi, Q. Yao, Q. Guo, Y. Li, L. Zhang, J. Ye, Y. Li, and Y. Liu,\n\u201cPredicting origin-destination flow via multi-perspective graph\nconvolutional network,\u201d in 2020 IEEE 36th International Conference\non Data Engineering (ICDE). IEEE, 2020, pp. 1818\u20131821.\n\n[35] H. Yuan, G. Li, Z. Bao, and L. Feng, \u201cAn effective joint prediction\nmodel for travel demands and traffic flows,\u201d in 2021 IEEE 37th\nInternational Conference on Data Engineering (ICDE). IEEE, 2021,\npp. 348\u2013359.\n\n[36] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d\nNeural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n\n[37] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \u201cEmpirical evalua-\ntion of gated recurrent neural networks on sequence modeling,\u201d\narXiv preprint arXiv:1412.3555, 2014.\n\n[38] Y. Li, R. Yu, C. Shahabi, and Y. Liu, \u201cDiffusion convolutional\nrecurrent neural network: Data-driven traffic forecasting,\u201d in In-\nternational Conference on Learning Representations. Vancouver, BC,\nCanada: ICLR, 2018, pp. 1 \u2013 16.\n\n[39] K. Cho, B. van Merrie\u0308nboer, D. Bahdanau, and Y. Bengio, \u201cOn\nthe properties of neural machine translation: Encoder\u2013decoder\napproaches,\u201d in Proceedings of SSST-8, Eighth Workshop on Syntax,\nSemantics and Structure in Statistical Translation. Doha, Qatar:\nAssociation for Computational Linguistics, Oct. 2014, pp. 103\u2013111.\n[Online]. Available: https://www.aclweb.org/anthology/W14-\n4012\n\n[40] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation\nby jointly learning to align and translate,\u201d in 3rd International\nConference on Learning Representations. San Diego, CA, USA: ICLR,\n2015, pp. 7 \u2013 9.\n\n[41] Y. Liang, S. Ke, J. Zhang, X. Yi, and Y. Zheng, \u201cGeoman: Multi-\nlevel attention networks for geo-sensory time series prediction.\u201d\nin Proceedings of the 27th International Joint Conference on Artificial\nIntelligence. Stockholm, Sweden: IJCAI, 2018, pp. 3428\u20133434.\n\n[42] Y. Li and J. M. Moura, \u201cForecaster: A graph transformer for fore-\ncasting spatial and time-dependent data,\u201d in European Conference\non Artificial Intelligence (ECAI). Pitesti, Arges, Romania: EurAI,\n2020, p. 274.\n\n[43] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan,\n\u201cEnhancing the locality and breaking the memory bottleneck of\ntransformer on time series forecasting,\u201d Advances in Neural Infor-\nmation Processing Systems, vol. 32, pp. 5243\u20135253, 2019.\n\n[44] N. Kitaev, L. Kaiser, and A. Levskaya, \u201cReformer: The efficient\ntransformer,\u201d in International Conference on Learning Representations,\n2019.\n\n[45] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, \u201cA\ncomprehensive survey on graph neural networks,\u201d IEEE transac-\ntions on neural networks and learning systems, vol. 32, no. 1, pp. 4\u201324,\n2020.\n\n[46] T. Chen and C. Guestrin, \u201cXgboost: A scalable tree boosting\nsystem,\u201d in Proceedings of the 22nd acm sigkdd international conference\non knowledge discovery and data mining. San Francisco: ACM, 2016,\npp. 785\u2013794.\n\n[47] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c.\nWoo, \u201cConvolutional lstm network: A machine learning approach\nfor precipitation nowcasting,\u201d in Advances in neural information\nprocessing systems. Montreal, Quebec, Canada: ACM, 2015, pp.\n802\u2013810.\n\n[48] S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, \u201cAttention based\nspatial-temporal graph convolutional networks for traffic flow\nforecasting,\u201d in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 33, no. 01, 2019, pp. 922\u2013929.\n\n\n"}
{"Title": "Improving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI", "Authors": "Erico Tjoa, Hong Jing Khok, Tushar Chouhan, Guan Cuntai", "Abstract": "  This paper quantifies the quality of heatmap-based eXplainable AI methods w.r.t image classification problem. Here, a heatmap is considered desirable if it improves the probability of predicting the correct classes. Different XAI heatmap-based methods are empirically shown to improve classification confidence to different extents depending on the datasets, e.g. Saliency works best on ImageNet and Deconvolution on ChestX-Ray Pneumonia dataset. The novelty includes a new gap distribution that shows a stark difference between correct and wrong predictions. Finally, the generative augmentative explanation is introduced, a method to generate heatmaps maps capable of improving predictive confidence to a high level.      ", "Subject": "Machine Learning (cs.LG)", "ID": "arXiv:2201.00009", "Text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImproving Deep Neural Network Classification Confidence using\nHeatmap-based eXplainable AI\n\nErico Tjoa 1 2 Hong Jing Khok 1 Tushar Chouhan 1 Guan Cuntai 1\n\nAbstract\nThis paper quantifies the quality of heatmap-based\neXplainable AI methods w.r.t image classification\nproblem. Here, a heatmap is considered desir-\nable if it improves the probability of predicting\nthe correct classes. Different XAI heatmap-based\nmethods are empirically shown to improve classi-\nfication confidence to different extents depending\non the datasets, e.g. Saliency works best on Ima-\ngeNet and Deconvolution on Chest X-Ray Pneu-\nmonia dataset. The novelty includes a new gap\ndistribution that shows a stark difference between\ncorrect and wrong predictions. Finally, the gen-\nerative augmentative explanation is introduced,\na method to generate heatmaps maps capable of\nimproving predictive confidence to a high level.\n\n1. Introduction\nArtificial intelligence (AI) and machine learning (ML) mod-\nels have been developed with various levels of transparency\nand interpretability. Recent issues related to the responsible\nusage of AI have been highlighted by large companies like\nGoogle (Lakshmanan, 2021) and Meta (Pesenti, 2021); this\nmay reflect the increasing demand for transparency and in-\nterpretability, hence the demand for eXplainable Artificial\nIntelligence (XAI). In particular, the blackbox nature of a\ndeep neural network (DNN) is a well-known problem in\nXAI. Many attempts to tackle the problem can be found in\nsurveys like (Adadi & Berrada, 2018; Dos\u030cilovic\u0301 et al., 2018;\nGilpin et al., 2018; Tjoa & Guan, 2020a).\n\nPopular XAI methods include post-hoc methods such as\nLocal Interpretable Model-agnostic Explanations (LIME)\n(Ribeiro et al., 2016) and SHapley Additive exPlanations\n(SHAP) that uses a game-theoretical concept (Lundberg &\n\n*Equal contribution 1Nanyang Technological University, Sin-\ngapore 2Alibaba Inc. Correspondence to: Erico Tjoa <eri-\ncotjoa@gmail.com>.\n\nPaper is under review. This format is borrowed from Proceedings\nof the 39 th International Conference on Machine Learning, Balti-\nmore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the\nauthor(s).\n\nLee, 2017). Many heatmap-generating XAI methods have\nalso been developed for DNN, in particular Class Activation\nMappings (CAM) (Zhou et al., 2016; Selvaraju et al., 2016),\nLayerwise Relevance Propagation (LRP) (Bach et al., 2015)\nand many other well-known methods, as listed in afore-\nmentioned surveys papers. These methods are appealing\nbecause heatmap-like attributions are intuitive and easy to\nunderstand. Although there are other remarkable ways to\ninvestigate interpretability and explainability e.g. methods\nthat directly attempt to visualize the inner working of a DNN\n(Zeiler & Fergus, 2014; Olah et al., 2017; 2020), we do not\ncover them here. This paper focuses on heatmap-based\nmethods.\n\nQuantifying the quality of heatmap-based XAI meth-\nods. Several existing efforts have also been dedicated to\nquantitatively measure the quality of heatmaps and other ex-\nplanations. For example, heatmaps have been measured by\ntheir potentials to improve object localization performance\n(Zhou et al., 2016; Selvaraju et al., 2016). The pointing\ngame (Fong & Vedaldi, 2017; Rebuffi et al., 2020) is an-\nother example where localization concept is used to quan-\ntify XAI\u2019s performance. The \u201cmost relevant first\u201d (MORF)\nframework has also been introduced to quantify the explain-\nability of heatmaps by ordered removal of pixels based on\ntheir importance (Samek et al., 2017); the MORF paper\nalso emphasizes that there is a difference between compu-\ntational relevance and human relevance i.e. objects which\nalgorithms find salient may not be necessarily salient for a\nhuman observer. Others can be found e.g. in (Tjoa & Guan,\n2020b). This paper quantifies the quality of a heatmap\nbased on how much the heatmap improves classification\nconfidence.\n\nUsing heatmaps to improve the classification confidence\nof DNN. Heatmaps have been said to not \u201c[tell] us anything\nexcept where the network is looking at\u201d (Rudin, 2019). In\nthis work, we would like to refute such claims and show\nthat heatmaps can be computationally useful. To test the\nusefulness of heatmaps in a direct way, we perform the Aug-\nmentative eXplanation (AX) process: combine an image x\nwith its heatmap h to obtain higher probability of predict-\ning the correct class, e.g. if f(x) gives a 60% probability\nof making a correct prediction, we consider using h such\n\nar\nX\n\niv\n:2\n\n20\n1.\n\n00\n00\n\n9v\n2 \n\n [\ncs\n\n.L\nG\n\n] \n 8\n\n J\nan\n\n 2\n02\n\n2\n\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nthat f(x+ h) yields 65%. We empirically show that such\nimprovement is possible for existing XAI methods but it\ndoes not happen in general since heatmaps are usually not\ndesigned to explicitly improve prediction computationally.\nThis improvement is quantified through a metric we call the\nConfidence Optimization (CO) score. Briefly speaking, CO\nscore is a weighted difference between raw output values\nbefore and after heatmaps/attributions modify the images\nx+ h. The metric assigns a positive/negative score if x+ h\nincreases/decreases the probability of making the correct\nprediction.\n\nThis paper is arranged as the following. In the next sec-\ntion, AX and Generative AX (GAX) are demonstrated\nthrough a two-dimensional toy example. Explicit form\nof heatmaps/attribution values can be obtained in the toy\nexample, useful for lower level analysis and direct ob-\nservation. The following section describes dataset pre-\nprocessing, computation of CO scores for AX process\non existing XAI methods, formal definition GAX process\nand the results. We then present our results, starting with\nthe novel finding: distribution gap as correctness indica-\ntors, CO scores distribution for common XAI methods,\nfollowed by high scores attained by GAX heatmaps and\nfinally qualitative aspects of the methods. All codes are\navailable in https://github.com/ericotjo001/\nexplainable_ai/tree/master/gax.\n\n2. Formulation in Low Dimensional Space\n\nFigure 1. Solid red (blue) lines are x1(x2) components of sample\ndata x. Dotted red (blue) lines are h1(h2) components of heatmaps\nh with k\u03b7 = 1.2. Heatmap values or attribute importances are\nassigned large values when either (1) the true components a1, a2\ndiffer significantly (2) the W transforms the data heterogenously\ni.e. not \u03b8 \u2248 (2k + 1)\u03c0\n\n4\n. See interpretations in the main text for\n\nmore details.\n\nThe application presented in this paper is based on the fol-\nlowing concept. We illustrate the idea using binary clas-\nsification of data sample x \u2208 R2, a 2D toy example. Let\ny = W\u22121x where y \u2208 R2 and W \u2208 R2\u00d72 is invertible. Let\nthe true label/category of sample x be c = argmaxiyi so\nthat it is compatible with one-hot encoding usually used in\na DNN classification task. Conventions:\n\n1. Output space Y . Let the output variable be y =\na1\n(\n1\n0\n\n)\n+ a2\n\n(\n0\n1\n\n)\n, clearly an element of a vector space.\n\nThe shape of this vector is the same as the output\nshape of the last fully connected layer for the stan-\ndard binary classification. Class prediction can be per-\nformed in the winner-takes-all manner, for example, if\na1 = 1, a2 = 0, then the label is c = argmaxiyi = 1.\nIf a1 = 0.1, a2 = 0.5, then c = 2. Basis of Y is\nBY =\n\n{\ny(1) =\n\n(\n1\n0\n\n)\n, y(2) =\n\n(\n0\n1\n\n)}\n.\n\n2. Sample space X is a vector space with the correspond-\ning basis BX = {Wy : y \u2208 BY } = {x(1) =\nWy(1), x(2) = Wy(2)} so x = a1x(1) + a2x(2) \u2208 X .\n\n3. Pixelwise sample space is the same sample space, but\nwe specifically distinguish it as the sample space with\nthe canonical basis. We will need this later, because\npixelwise space has \u201chuman relevance\u201d, since human ob-\nservers perceive the components (pixels) directly, rather\nthan automatically knowing the underlying structure (i.e.\nwe cannot see a1, a2 directly). We denote a sample in\nthis basis with x = x1\n\n(\n1\n0\n\n)\n+ x2(\n\n0\n1 ).\n\n4. A heatmap or attribute vector h in this paper has the\nsame shape as x and can be operated directly with x\nvia component-wise addition. Thus, they also belong to\nsample space or the pixelwise sample space. Writing\na heatmap in the sample space h = Ax(1) + Bx(2) is\nuseful for obtaining a closed form expression later.\n\nThe perfect classifier, f . Define f(x,\u0398) = \u03c3(\u0398x) as\na trainable classifier with parameters \u0398 \u2208 R2\u00d72. Let\n\u0398 = W\u22121 and the activation \u03c3 be any strictly monotonic\nfunction, like the sigmoid function. Then, the classifier\nf(x) = \u03c3(W\u22121x) \u2208 R2 is perfect, in the sense that,\nif a1 > a2, then c = argmaxifi(x) = 1; likewise if\na1 < a2, then c = 2 and, for a1 = a2 either decision\nis equally probable. This is easily seen as the following:\nf(x) = \u03c3(W\u22121(a1x\n\n(1)+a2x\n(2))) = \u03c3(a1\n\n(\n1\n0\n\n)\n+a2\n\n(\n0\n1\n\n)\n) =(\n\n\u03c3(a1)\n\u03c3(a2)\n\n)\n.\n\nConfidence optimization score (CO score), sco. In this\nsection, we show a simple explicit form of CO score for\nbetter illustration; in the experimental method section, for-\nmal definition will be given. The score increases if x + h\nleads to an improvement in the probability of correctly pre-\ndicting label c, hence the score\u2019s definition depends on the\ngroundtruth label. Throughout this section, for illustration,\nwe use x = a1x(1) + a2x(2) with groundtruth label c = 1,\ni.e. a1 > a2. Define the CO score as\n\nsco(x, h) =\n(\n\n1\n\u22121\n)\n\u00b7\n[\nf(x+ h)\u2212 f(x)\n\n]\n(1)\n\nFor the perfect classifier, see that f1(x + h) > f1(x) and\nf2(x + h) < f2(x) contribute to a larger sco. In other\n\nhttps://github.com/ericotjo001/explainable_ai/tree/master/gax\nhttps://github.com/ericotjo001/explainable_ai/tree/master/gax\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nwords, increasing the probability of predicting the correct\nlabel c = 1 increases the score. For c = 2, replace\n\n(\n1\n\u22121\n)\n\nwith\n(\n\u22121\n1\n\n)\n.\n\nAugmentative explanation. AX is defined here as any\nmodification on x by h that is intended to yield positive\nthe CO score, i.e to increase the probability of making a\ncorrect classification. This paper mainly considers the sim-\nplest implementation, namely x+ h. Let us consider a few\npossibilities. Suppose \u03c3 = LeakyReLU and h = x. We\nget sco =\n\n(\n1\n\u22121\n)\n\u00b7\n(\n\u03c3(2a1)\u2212\u03c3(a1)\n\u03c3(2a2)\u2212\u03c3(a2)\n\n)\n= a1 \u2212 a2 > 0. In other\n\nwords, choosing the image as the heatmap itself improves\nthe score. However, as a heatmap or attribute vector, h is\nuseless, since it does not provide us with any information\nabout the relative importance of the components of x in\ncanonical basis, which is the part of data directly visible to\nthe observer. Even so, h = x has computational relevance\nto the model, since a1, a2 are modified in the correct direc-\ntion. Our aim is to find computationally relevant h that does\nnot score zero in \u201chuman relevance\u201d, figuratively speak-\ning. We therefore rule out obviously uninformative heatmap\nin the upcoming sections. Further, consider similar situa-\ntion but set \u03c3 to sigmoid function. Simply setting h = x\nwill no longer increase the score significantly all the time.\nSince sigmoid is asymptotic, when a1, a2 are sufficiently\nfar away from zero, the increase will be so negligible, the\nheatmap will be uninformative even though the magnitude\nof |a1 \u2212 a2| may be large. Hence, we use the raw DNN\noutput in our main experiment, without sigmoid, softmax\netc.\n\nGenerative Augmentative EXplanation (GAX) is an AX\nprocess where the heatmap h = w \u2217 x is generated by\ntuning the trainable parameter w so that sCO is optimized;\n\u2217 denotes component/pixel-wise multiplication. Here we\nwill define \u2206 = s1 as the term that we maximize by hand,\nfor clarity and illustration. By comparison, in the main\nexperiment, we directly perform gradient descent on \u2212s1\n(plus regularization terms) to generate GAX heatmaps, i.e.\nwe minimize a total loss. To start with GAX, recall our\nchoice of heatmap written in sample space basis,\n\nh = w \u2217 x = Ax(1) +Bx(2) (2)\n\nThis form is desirable as it can be manipulated more easily\nthan the pixelwise sample space form h = (w1x1w2x2 ), as the\nfollowing. From RHS of eq. (2), get AWy(1) +BWy(2) =\nW\n(\nA\nB\n\n)\n. We thus have ( AB ) = W\n\n\u22121(w \u2217 x). To increase\nCO score, the aim is to find parameter w that maximizes\nA\u2212B, i.e. find w\u2217 = argmaxw(A\u2212B). Expanding the\nterms in w \u2217x of eq. (2), we obtain (w1w2 ) \u2217\n\n(\na1W11+a2W12\na2W21+a2W22\n\n)\n.\n\nTaking the difference between the components gives us\n\n\u2206 \u2261A\u2212B\n=w1(W\n\n\u22121\n11 \u2212W\n\n\u22121\n21 )(a1W11 + a2W12)\n\n\u2212 w2(W\u2212122 \u2212W\n\u22121\n12 )(a1W21 + a2W22)\n\n(3)\n\nMaximizing \u2206 to a large \u2206 > 0 will clearly optimize\nsco(x, h) = \u03c3(a1)\u2212 \u03c3(a2) + \u03c3(A)\u2212 \u03c3(B), assuming \u03c3 is\nstrictly monotonously increasing.\n\nHeatmap obtained through optimization using gradient\nascent. Recall that gradient ascent is done by \u2206 \u2192\n\u2206 + dw \u00b7 \u2207w\u2206 with the choice dw = \u03b7\u2207w\u2206, hence\n\u2206 + \u03b7||\u2207w\u2206||2 \u2265 \u2206. Hence, the heatmap after k steps\nof optimization is given by\n\nh =(w + kdw) \u2217 x\n\n=\n[\nw + k\u03b7\n\n(\n(W\n\n\u22121\n11 \u2212W\n\n\u22121\n21 )(a1W11+a2W12)\n\n\u2212(W\u2212122 \u2212W\n\u22121\n12 )(a1W21+a2W22)\n\n)]\n\u2217 x\n\n(4)\n\nTo visualize the heatmap, here we use the example where\nW is the rotation matrix W =\n\n(\ncos\u03b8 \u2212sin\u03b8\nsin\u03b8 cos\u03b8\n\n)\n. Examples\n\nof heatmaps plotted along with the input x are shown in\nfig. 1, to be discussed in the next subsection. If \u03b8 = 0, x\nare identical to y, so binary classification is straightforward\nand requires no explanation. Otherwise, consider \u03b8 being\na small deviation from 0. Such slightly rotated system is a\ngood toy-example for the demonstration of component-wise\n\u201cimportance attribution\u201d. This is because if x belongs to\ncategory c = 1 with high a1 component, then it still has a\nmore significant first component x1 after the small rotation.\nThus, a heatmap that correspondingly gives a higher score to\nthe first component is \u201ccorrect\u201d in the sense that it matches\nthe intuition of attribute importance: high h1 emphasizes\nthe fact that high x1 literally causes high y1. Furthermore,\nif the system rotates by \u03c0/4, we see that the classification\nbecomes harder. This is because the components x1 and\nx2 start to look more similar because cos\n\n\u03c0\n4\n\n= sin\u03c0\n4\n\n, and\nconsequently, the attribution values will be less prominent\nas well.\n\n2.1. Interpretability\n\nDISCLAIMER: for the benefit of readers who are used to\nregard heatmaps as the explanation or a method to perform\nlocalization, we must emphasize that this paper does not\nappeal to that ideal. To reiterate, in this paper, heatmaps are\nthe maps of pixel intensity that computationally optimize\nthe classification confidence.\n\nHomogenous and Heterogenous transformations. For the\nlack of better words, we refer to transformations like \u03b8 \u2248\n\u03c0/4 or more generally (2k+ 1)\u03c0\n\n4\nfor k = ...,\u22121, 0, 1, ... as\n\nhomogenous transformations, since the components become\nmore indistinguishable (recall: cos\u03c0\n\n4\n= sin\u03c0\n\n4\n). Otherwise,\n\nthe transformation is called heterogenous. These defini-\ntions are given here with the intention of drawing parallels\nbetween (1) the toy data that have been homogenously trans-\nformed (hence hard to distinguish) and (2) samples in real\ndatasets that look similar to each other, but are categorized\ndifferently due to a small, not obvious difference.\n\nInterpretation of attribute values for distinct non-negative\n\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\ncomponents. In the pixelwise sample space, we will be more\ninterested in non-negative data sample x1, x2 \u2265 0 since we\nonly pass [0, 1]-normalized images for GAX. Fig. 1 left\nshows a data sample with distinct components, indicated\nby high a1 = 0.95 component and low a2. Non-negative\ndata samples are found around \u03b8 \u2208 [0, \u03c0/2]. High x1 value\nis given high h1 attribution score while low x2 is given a\nsuppressed value of h2 near \u03b8 = 0, matching our intuition as\ndesired. As rotation proceeds to \u03c0/4, there is a convergence\nbetween x1 and x2, making the components more indistin-\nguishable. At \u03b8 = \u03c0/4 exactly, we still see high h1 that\npicks up high signal due to high a1, also as desired. Between\n\u03c0/4 and \u03c0/2, rotation starts to flip the components; in fact,\nat \u03c0/2, x = [0, 1] is categorized as c = 1 and x = [1, 0] as\nc = 2. The attribution value h2 becomes more prominent,\nhighlighting x2, also as desired for our prediction of class\nc = 1. In fig. 1 middle, decreased/increased a1, a2 are\nassigned less prominent h1, h2 respectively than fig. 1 left,\nsince the model becomes less confident in its prediction,\nalso consistent with our intuition.\n\nThe other extreme. Fig. 1 right shows a1 and a2 that do\nnot differ significantly. At homogeneous transformation\n\u03b8 \u2248 \u00b1\u03c0\n\n4\n, heatmaps are almost equal to the input x. As\n\nexpected, it will be difficult to pick up signals that are very\nsimilar, although very close inspection might reveal small\ndifferences that could probably yield some information (not\nin the scope of this paper). Other interpretations can be\nfound in appendix More interpretations in low dimensional\nexample.\n\n3. Experimental Method and Datasets\nIn the previous section, we described how heatmap h can be\nused to improve classification probability. More precisely,\nx+h yields higher confidence in making a correct prediction\ncompared to x alone when used as the input to the model f .\nWe apply the same method to real dataset ImageNet (Deng\net al., 2009) and Chest X-Ray Images (Pneumonia) from\nKaggle (Mooney, 2018). The Pneumonia dataset needs\nreshuffling, since Kaggle\u2019s validation dataset consists of\nonly of 16 images for healthy and pneumonia cases com-\nbined. We combined the training and validation datasets and\nthen randomly draw 266/1083 healthy and 790/3093 pneu-\nmonia images for validation/training. There are 234/390\nhealthy/pneumonia images in the test dataset. Images are\nall resized to 256 \u00d7 256. The X-Ray images are black\nand white, so we stack them to 3 channels. Images from\nImageNet are normalized according to suggestion in the py-\ntorch website, with mean = [0.485, 0.456, 0.406], std =\n[0.229, 0.224, 0.225].\n\nFor both datasets, we use pre-trained models Resnet34 (He\net al., 2016) and AlexNet (Krizhevsky, 2014) available in\nPytorch. The models are used on ImageNet without fine-\n\nTable 1. Fine-tuning results for pre-trained models on Chest X-\nRay Pneumonia test dataset. The architectures marked with sub\nare deliberately trained to achieve lower validation accuracy for\ncomparison.\n\nResnet34 1 Resnet34 sub Alexnet sub\n\naccuracy 0.800 0.636 0.745\nprecision 0.757 0.632 0.726\n\nrecall 1.000 1.000 0.951\nval. acc. 0.99 0.8 0.8\n\ntuning. Resnet34 is fine-tuned for Pneumonia binary classi-\nfications, where the first 8 modules of the pre-trained model\n(according to pytorch\u2019s arrangement) are used, plus a new\nfully-connected (FC) layer with two output channels at the\nend. Similarly, for Alexnet, the first 6 modules are used\nwith a two-channel FC at the end. For Resnet34, we will\nuse Resnet34 1 and Resnet34 sub respectively trained to\nachieve 99% and 80% validation accuracies for comparison.\nThe same targets were specified for Alexnet, but only 80%\nvalidation accuracy was achieved, thus only Alexnet sub\nwill be used. Adam optimizer is used with learning rate\n0.001, \u03b2 = (0.5, 0.999). The usual weight regularization\nis not used during optimization i.e. in pytorch\u2019s Adam op-\ntimizer, weight decay is set to zero because we allow zero\nattribution values in large patches of the images. No number\nof epochs are specified. Instead, training is stopped after the\nmax number of iterations (240000) or the specified valida-\ntion accuracy is achieved after 2400 iterations have passed.\nAt each iteration, samples are drawn uniform-randomly with\nbatch size 32.\n\nCO scores on existing XAI methods via AX process. De-\nnote the deep neural network as DNN, define the CO score\nas the weighted difference between the predictive scores\naltered by AX process and the original predictive scores,\n\nsco(x, h) = \u03ba \u00b7\n[\nDNN(x+ h)\u2212DNN(x)\n\n]\n(5)\n\nwhere \u03ba \u2208 RC is defined as the score constants, C the\nnumber of classes, \u03baj = 1 if the groundtruth belongs to\nlabel/category j and \u03bai = \u22121/(C \u2212 1) for all i 6= j.\nThis equation is the general form of eq. (1). In our im-\nplementation, each DNN\u2019s output is raw, i.e. last layer\nis FC with C channels without softmax layer etc. A\nheatmap h that yields sco = 0 is uninformative (see ap-\npendix). We compute CO scores for heatmaps gener-\nated by six different existing heatmap-based XAI meth-\nods (all available in Pytorch Captum), namely, Saliency\n(Simonyan et al., 2014), Input*Gradient (Shrikumar et al.,\n2016), Layer GradCAM (Selvaraju et al., 2016), Decon-\nvolution (Zeiler & Fergus, 2014), Guided Backpropaga-\ntion (Springenberg et al., 2015) and DeepLift (Shrikumar\net al., 2017). Each heatmap is generated w.r.t predicted\ntarget, not groundtruth e.g. if y pred=DNN(x) predicts\n\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nFigure 2. Distribution of CO scores obtained through AX process on existing XAI methods. Classification probability is improved if the\nscore is positive. All distributions show gaps between CO scores of data whose classes are correctly and wrongly predicted (e.g. red\narrows); correct prediction tends to yield higher CO scores. The result is obtained using Resnet34 1 on Pneumonia dataset. [sum] denotes\nAX process with x+ h.\n\nclass n, then h=DeepLIFT(net).attribute(input, target=n)\nin Pytorch Captum notation. Then normalization is applied\nh\u2192 h/max(|h|) before we perform the AX process. Note:\nFor ImageNet, C = 1000, chest X-Ray, C = 2. We also\nconsider f(x \u2217 h), where \u2217 denotes component-wise multi-\nplication. The idea is generally to interact h with x so that,\nfor any interaction g, higher probability of correct predic-\ntion is achieved by f(g(x, h)); see appendix for their results.\nGradCAM of \u2018conv1\u2019 layer is used in this paper. Other meth-\nods and different arbitrary settings are to be tested in future\nworks.\n\nAchieving high sco with GAX. Here, GAX is the x+h AX\nprocess where heatmaps h = tanh(w \u2217 x) are generated by\ntraining parameters w to maximize sco. Maximizing sco in-\ndefinitely is impractical, and thus we have chosen sco = 48\nfor ImageNet dataset, a score higher than most sco attained\nby existing XAI methods we tested in this experiment. Tanh\nactivation is used both to ensure non-linearity and to en-\nsure that the heatmap is normalized to [\u22121, 1] range, so\nthat we can make a fair comparison with existing heatmap-\nbased XAI methods. For ImageNet, 10000 data samples are\nrandomly drawn from the validation dataset for evaluating\nGAX. For pneumonia, all data samples are used. Optimiza-\ntion is done with Adam optimizer with learning rate 0.1,\n\u03b2 = (0.9, 0.999). This typically takes less than 50 steps of\noptimization, a few seconds per data sample using a small\nGPU like NVIDIA GeForce GTX 1050.\n\nSimilarity loss and GAX bias. In our implementation,\nwe minimize \u2212sco. However, this is prone to producing\nheatmaps that are visually imperceptible from the image.\nSince w is initialized as an array of 1s with exactly the same\nshape (c, h, w) = (3, 256, 256) as x, the initial heatmap is\n\nsimply h = w \u2217 x = x. Possibly, small changes in w over\nthe entire pixel space is enough to cause large changes in\nthe prediction, reminiscent of adversarial attack (Szegedy\net al., 2014; Akhtar & Mian, 2018). We solve this prob-\nlem by adding the similarity loss, penalizing h = x. The\noptimization is now done by minimizing the modified loss,\nwhich is negative CO score plus similarity loss\n\nloss = \u2212sco + ls\n\u2329 (h\u2212 x+ \ufffd)2\n\nx+ \ufffd\n\n\u232a\u22121\n(6)\n\nwhere ls = 100 is the similarity loss factor. \u3008X\u3009 computes\nthe average over all pixels. Division / and square 2 are\nperformed component/pixel-wise. Pixel-wise division by x\nnormalizes the pixel magnitude, so that small pixel values\ncan contribute more significantly to the average value. The\nsmall term \ufffd = 10\u22124 is to prevent division by zero and pos-\nsibly helps optimization by ensuring that zero terms do not\nmake the gradients vanish. Furthermore, for X-Ray images,\nwith many zeros (black region), the similarity factor seems\ninsufficient, resulting in heatmaps that mirror the input im-\nages. GAX bias isa dded for the optimization to work, so\nthat h = w \u2217 x+ b, where b is 0.01 array of the same shape\n(c, h, w) as well. Note: the similarity loss is positive, since x\nused here is [0, 1] normalized (by comparison, the standard\nResnet34 normalization can result in negative pixels).\n\n4. Results and Discussions\nRecall that we use pre-trained models for ImageNet. For\npneumonia dataset, the predictive results of fine-tuning mod-\nels are shown in table 1. AX and GAX processes will be\napplied on top of these models.\n\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nFigure 3. Similar to fig. 2, but results are obtained from (A) Resnet34 architecture on Pneumonia dataset, but with less fine-tuning\n(Resnet34 sub). (B) Resnet34 on ImageNet.\n\n4.1. Gaps in CO Scores Distribution\n\nHere, we present the main novel finding: the gap in CO\ndistribution. AX process neither specifies any formulas nor\noptimizes any losses to distinguish correct predictions from\nthe wrong ones, but fig. 2 shows distinct gaps between\nthem (shown by the red arrows). Possible reason: heatmaps\nused in AX process are generated for the class predicted\nby the DNN. If the prediction is correct, there is a match\nbetween cpred in e.g. h=DeepLIFT(net).attribute(input,\ntarget=cpred) (recall: we use Pytorch Captum notation) and\nthe groundtruth label c that that affects CO score through \u03ba.\nThe different distributions found in fig. 2 and 3 indicate that\nsome existing XAI methods possess more information to\ndistinguish between correct and wrong predictions than the\nothers. With this, we might be able to debunk some claims\nthat heatmaps are not useful (Rudin, 2019): regardless of the\nsubjective assessment of heatmap shapes, heatmaps might\nbe relatively informative after some post-processing. In the\nabsence of such information, we expect to see uniformly\nrandom distribution of scores. Since we have observed dis-\ntinct distributional gaps on top of general difference in the\nstatistics, we have shown that some heatmap-based XAI\nmethods combined with CO score might be a new indicator\nto help support classification decision made by the particular\nDNN architecture.\n\nFurthermore, the extent of CO score distribution gap is\nclearly dependent on the dataset and DNN architecture. As\nit is, the discriminative capability of different XAI methods\nis thus comparable only within the same system of archi-\ntecture and dataset. ImageNet dataset shows a smaller gap\ncompared to pneumonia dataset and the largest gap in Im-\nageNet is produced by the Saliency method, as seen in fig.\n3(B). By comparison, the largest gap in pneumonia dataset is\nproduced by Deconvolution. Comparing fig. 2 and fig. 3(A),\nthe gaps appear to be wider when DNN is better trained.\nFurther investigation is necessary to explain the above obser-\nvations, but, to leverage this property, users are encouraged\nto test AX process on different XAI methods to find the\nparticular method that shows the largest gap. Once the XAI\nmethod is determined, it can be used as a supporting tool\nand indicator for the correctness of prediction.\n\nFigure 4. CO score optimization through GAX x+ tanh(w \u2217 x)\non ImageNet data using pre-trained Resnet34 (blue) and Alexnet\n(red), where each curve corresponds to a single image. The target\nsco is set to 48, exceeding most CO scores of other methods.\n\n4.2. Improvement in Predictive Probability with GAX\n\nThe higher the CO scores are, the better is the improvement\nin predictive probability. Is it possible to achieve even higher\nimprovement, i.e. higher CO score? Fig. 2 and 3 show the\nboxplots of CO scores for AX process applied on all six XAI\nmethods we tested in this experiment; histograms applied on\nselect XAI methods are also shown. Different XAI methods\nachieve different CO scores. For pneumonia dataset, very\nhigh CO scores (over 80) are attained by Deconvolution\nmethods. For ImageNet, highest CO scores attained are\naround 10. To attain even higher scores, Generative AX\n(GAX) will be used.\n\nUsing GAX on ImageNet, sco \u2265 48 can be attained as\nshown in fig. 4, where the time evolution of CO score for\neach image is represented by a curve. For Resnet34, most\nof the images attains sco \u2265 48 within 50 iterations. Alexnet\nGAX optimization generally takes more iterations to achieve\nthe target. High sco implies high confidence in making the\ncorrect prediction. We have thus obtained heatmaps and\nattribution values with computational relevance i.e. they can\nbe used to improve the model\u2019s performance. Note: (1) all\nimages tested do attain the target sco (not shown), although\n\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nFigure 5. (A) GAX dynamic heatmaps displayed with a slider for users to observe the evolution of heatmaps through time steps. (B-E)\nGAX heatmaps generated on Resnet34 for (B) healthy chest X-Ray and (C) chest X-Ray of a patient with bacterial pneumonia; and for\nImageNet images (D) a sheep dog image and (E) a planetarium image. (F) An instance of bacterial pneumonia chest X-Ray showing\nan irregular posture with heavy noise (top right). The empty space might have been used as a false distinct feature for pneumonia\nclassification. Three heatmaps for each image correspond to the attribution values assigned to R, G and B color channels respectively.\n\u201cAbs max\u201d specifies the maximum absolute value attained by the heatmap throughout all three channels (max is 1, due to Tanh activation).\nPositive/negative heatmap or attribution values (red/blue) indicate pixels to be increased/reduced in intensity to attain higher prediction\nconfidence. At higher CO scores, negative values emerge.\n\nsome of the images took a few hundreds iterations (2) we\nexclude images where predictions are made incorrectly by\nthe pre-trained or fine-tuned model. By comparison, in\ngeneral, using heatmaps derived from existing methods for\nAX process does not yield positive CO scores i.e. does\nnot improve predictive probability for the correct class (see\nespecially fig. 3(B)). Furthermore, for ImageNet, typically,\nsco \u2264 10. Other boxplots are shown in appendix fig. 7.\n\n4.3. Qualitative Assessment of GAX Heatmaps\n\nHeatmaps in GAX are obtained through a process optimiza-\ntion through a finite number of time steps. We provide\nmatplotlib-based graphic user interface (GUI) for users to\nobserve the evolution of heatmap pixels through GAX; see\nfig. 5(A). This provides users some information about the\nway the DNN architecture perceives input images. But, how\nexactly can user interpret this? Recall that the main premise\nof this paper is the computational relevance: GAX is de-\nsigned to generate heatmaps that improve the confidence in\npredicting the correct label numerically. Hence, the visual\n\ncues generated by the GAX heatmaps show which pixels\ncan be increased or decreased in intensity to give higher\nprobability of making the correct prediction.\n\nDNN optimizes through extreme intensities. Heatmaps in\nfig. 5(B-E) show that predictive confidence is improved\ngenerally through optimizing regions of extreme intensity.\nFor example, to improve CO scores through GAX, the pix-\nels corresponding to white hair of the dog in fig. 5(D)\nare assigned positive values (red regions in the heatmaps).\nDark region of healthy chest X-Ray in (B) are subjected to\nstronger optimization (intense red or blue) to achieve better\npredictive confidence. The DNN architecture seems to be\nmore sensitive to changes in extreme values in the image.\nIn a positive note, this property might be exploited during\ntraining: this is probably why normalization to [\u22121, 1] range\nin the standard practice of deep learning optimization works\ncompared to [0, 1]. On the other hand, this might be a prob-\nlem to address in the future as well: heatmaps that boost\nalgorithmic confidence are not intuitive to human viewers.\nWe can ask the question: is it possible to train DNN such\n\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nthat its internal structure is inherently explainable (e.g. if\nlocalization is accepted as an explanation, does there exist\nan architecture whose predictive confidence is tied directly\nto localization?). For comparison, existing XAI methods\ntypically specify extra settings to obtain these explanations.\nUnfortunately, the settings can be arbitrary, e.g. GradCAM\npaper (Selvaraju et al., 2016) sets an arbitrary 15% threshold\nof max intensity for binarization. To obtain explanation with\nbetter integrity, the settings might need to be specified in\ncontext beforehand. In this paper, we do NOT address such\narbitrary settings taylored to attain subjectively acceptable\nexplanation or to maximize high IoU for bounding boxes.\n\nDiscriminatory but unrefined patterns. Pneumonia dataset\nconsists of chest X-Ray of healthy patients and patients with\nseveral types of pneumonia with different recognizable pat-\nterns. Bacterial pneumonia has a focal lobar consolidation,\nwhile viral pneumonia has diffuse interstitial patterns; nor-\nmal chest X-Ray has neither. This turns out to affect the\nshape of GAX heatmaps. Fig. 5(B) shows a typical normal\nChest X-Ray pattern. By comparison, fig. 5(C) shows a\nheatmap generated on bacterial pneumonia. In the latter, we\nsee the drastic change in the heatmap features, especially\nhigh-intensity stripes around the lobar consolidation. There\nis a possibility that novel class-discriminatory patterns lie\nhidden within heatmaps generated by GAX. The heatmaps\nappear unrefined, but this might be related to the internal\nstructures of the DNN architecture itself, as described in the\nfollowing section.\n\nLimitations and Future works. We have offered some\nplausible explanations on heatmaps generated through our\nmethod GAX that are consistent with success of standard\ndeep learning training pipeline. Now, we discuss possible\nways to address the issues we briefly presented in the pre-\nvious sections and how they can be addressed in the future.\n(1) Optimized regions prefer extreme intensities (very bright\nor very dark regions). The heatmaps in fig. 5(B-E) indicate\nthat we are able to optimize predictive probability through\nrelative intensity manipulation of pixel patterns that are not\nhumanly intuitive. To truly capture variations in patterns\nand not rely heavily on large difference in intensity, a layer\nor module specifically designed to output very smooth repre-\nsentation might be helpful. Training might take longer, but\nwe hypothesize that skewed optimization through extreme\nintensity can be prevented. (2) Some optimized features\nare rife with artifact-looking patterns. An immediate hy-\npothesis that we can offer is the following. The internal\nstructure of the DNN (the set of weights) is noisy, thus, even\nif features are properly captured, they are amplified through\nnoisy channels, yielding artifacts. This is indicative of the\ninstability of high dimensional neuron activations in a DNN,\na sign of fragility against adversarial attack we previously\nmentioned. How should we address this? We need DNN\nthat are robust against adversarial attack; fortunately, many\n\nresearchers have indeed worked on this problem recently.\n(3) The regularity of data distribution is probably an impor-\ntant deciding factor in model training. In cases where the\nX-Ray images are not taken in a regular posture, the empty\nspace can become a false \u201cdistinct feature\u201d, as shown in fig.\n5(F). While this may indicate a worrying trend in the flawed\ntraining of DNN or data preparation (hence a misguided ap-\nplication) we believe GAX can be used to detect such issue\nbefore deployments. Related future studies may be aimed at\nquantifying the effect of skewed distribution on the appear-\nance of such \u201cfalse prediction\u201d cases. (4) Finally, depending\non the explanation context, ground-truth explanations might\nbe the most desirable features in XAI: we specify exactly\nwhat we want as the correct explanation. The ideal heatmaps\nmay for example resemble object-localization-mask or high-\nlight only relevant parts. Also see appendix for more, e.g.\nimplementation-specific limitations etc.\n\n5. Conclusion\nWe have investigated a method to use heatmap-based XAI\nmethods to improve DNN\u2019s classification performance. The\nmethod itself is called the AX process, and the improve-\nment is measured using a metric called the CO score. Some\nheatmaps can be directly used to improve model\u2019s predic-\ntion better than the others as seen by the boxplots of score\ndistribution. The distribution of scores shows a novel gap\ndistribution, an interesting feature that develops without any\nspecific optimization. GAX is also introduced to explicitly\nattain high improvement in predictive performance or help\ndetect issues. This work also debunks claims that heatmaps\nare not useful through the improvement of predictive confi-\ndence. We also give explanations on DNN behaviour con-\nsistent with the standard practice of deep learning training.\nFrom the results, we support the notion that computationally\nrelevant features are not necessarily relevant to human.\n\nSummary of novelties and contributions: (1) CO scores pro-\nvide empirical evidence for informative content of heatmaps\n(2) the distribution gap in CO scores may be a new indi-\ncator in predictive modelling (3) distinct (albeit unrefined)\nclass-dependent patterns that emerge on GAX-generated\nheatmaps could be used as discriminative signals. Overall,\nwe also provide insights into the DNN\u2019s behaviour.\n\nSoftware and Data\nAll codes are available; see main paper and also see ap-\npendix.\n\nAcknowledgements\nThis research was supported by Alibaba Group Holding Lim-\nited, DAMO Academy, Health-AI division under Alibaba-\n\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nNTU Talent Program. The program is the collaboration\nbetween Alibaba and Nanyang Technological University,\nSingapore.\n\nReferences\nAdadi, A. and Berrada, M. Peeking inside the black-box: A\n\nsurvey on explainable artificial intelligence (xai). IEEE\nAccess, 6:52138\u201352160, 2018. doi: 10.1109/ACCESS.\n2018.2870052.\n\nAkhtar, N. and Mian, A. Threat of adversarial attacks on\ndeep learning in computer vision: A survey. IEEE Ac-\ncess, 6:14410\u201314430, 2018. doi: 10.1109/ACCESS.2018.\n2807385.\n\nBach, S., Binder, A., Montavon, G., Klauschen, F., Mu\u0308ller,\nK.-R., and Samek, W. On pixel-wise explanations for\nnon-linear classifier decisions by layer-wise relevance\npropagation. PLOS ONE, 10(7):1\u201346, 07 2015. doi:\n10.1371/journal.pone.0130140. URL https://doi.\norg/10.1371/journal.pone.0130140.\n\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and\nFei-Fei, L. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on com-\nputer vision and pattern recognition, pp. 248\u2013255.\nIeee, 2009. URL https://www.kaggle.com/c/\nimagenet-object-localization-challenge.\n\nDos\u030cilovic\u0301, F. K., Brc\u030cic\u0301, M., and Hlupic\u0301, N. Explainable arti-\nficial intelligence: A survey. In 2018 41st International\nConvention on Information and Communication Tech-\nnology, Electronics and Microelectronics (MIPRO), pp.\n0210\u20130215, 2018. doi: 10.23919/MIPRO.2018.8400040.\n\nFong, R. C. and Vedaldi, A. Interpretable explanations of\nblack boxes by meaningful perturbation. In 2017 IEEE\nInternational Conference on Computer Vision (ICCV), pp.\n3449\u20133457, 2017. doi: 10.1109/ICCV.2017.371.\n\nGilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M.,\nand Kagal, L. Explaining explanations: An overview of\ninterpretability of machine learning. In 2018 IEEE 5th\nInternational Conference on Data Science and Advanced\nAnalytics (DSAA), pp. 80\u201389, 2018. doi: 10.1109/DSAA.\n2018.00018.\n\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pp.\n770\u2013778, 2016.\n\nKrizhevsky, A. One weird trick for parallelizing convolu-\ntional neural networks. ArXiv, abs/1404.5997, 2014.\n\nLakshmanan, L. Why you need to explain\nmachine learning models, Jun 2021. URL\nhttps://cloud.google.com/blog/\nproducts/ai-machine-learning/\nwhy-you-need-to-explain-machine-learning-models.\n\nLundberg, S. M. and Lee, S.-I. A unified approach\nto interpreting model predictions. In Guyon, I.,\nLuxburg, U. V., Bengio, S., Wallach, H., Fer-\ngus, R., Vishwanathan, S., and Garnett, R. (eds.),\nAdvances in Neural Information Processing Sys-\ntems 30, pp. 4765\u20134774. Curran Associates, Inc.,\n2017. URL http://papers.nips.cc/paper/\n7062-a-unified-approach-to-interpreting-model-predictions.\npdf.\n\nMooney, P. Chest x-ray images (pneumo-\nnia), Mar 2018. URL https://www.\nkaggle.com/paultimothymooney/\nchest-xray-pneumonia.\n\nOlah, C., Mordvintsev, A., and Schubert, L. Feature\nvisualization. Distill, 2(11), November 2017. doi:\n10.23915/distill.00007. URL https://doi.org/10.\n23915%2Fdistill.00007.\n\nOlah, C., Satyanarayan, A., Johnson, I., Carter, S., Schubert,\nL., Ye, K., and Mordvintsev, A. The building blocks of\ninterpretability, Jan 2020. URL https://distill.\npub/2018/building-blocks.\n\nPesenti, J. Facebook\u2019s five pillars of responsible ai, Jun\n2021. URL https://ai.facebook.com/blog/\nfacebooks-five-pillars-of-responsible-ai/.\n\nRebuffi, S. A., Fong, R., Ji, X., and Vedaldi, A. There and\nback again: Revisiting backpropagation saliency methods.\nIn 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 8836\u20138845, 2020. doi:\n10.1109/CVPR42600.2020.00886.\n\nRibeiro, M. T., Singh, S., and Guestrin, C. \u201cwhy\nshould i trust you?\u201d: Explaining the predictions of\nany classifier. In Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Dis-\ncovery and Data Mining, KDD \u201916, pp. 1135\u20131144,\nNew York, NY, USA, 2016. Association for Comput-\ning Machinery. ISBN 9781450342322. doi: 10.1145/\n2939672.2939778. URL https://doi.org/10.\n1145/2939672.2939778.\n\nRudin, C. Stop explaining black box machine learning\nmodels for high stakes decisions and use interpretable\nmodels instead. Nature Machine Intelligence, 1(5):206\u2013\n215, May 2019. ISSN 2522-5839. doi: 10.1038/\ns42256-019-0048-x. URL https://doi.org/10.\n1038/s42256-019-0048-x.\n\nhttps://doi.org/10.1371/journal.pone.0130140\nhttps://doi.org/10.1371/journal.pone.0130140\nhttps://www.kaggle.com/c/imagenet-object-localization-challenge\nhttps://www.kaggle.com/c/imagenet-object-localization-challenge\nhttps://cloud.google.com/blog/products/ai-machine-learning/why-you-need-to-explain-machine-learning-models\nhttps://cloud.google.com/blog/products/ai-machine-learning/why-you-need-to-explain-machine-learning-models\nhttps://cloud.google.com/blog/products/ai-machine-learning/why-you-need-to-explain-machine-learning-models\nhttp://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\nhttp://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\nhttp://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\nhttps://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\nhttps://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\nhttps://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\nhttps://doi.org/10.23915%2Fdistill.00007\nhttps://doi.org/10.23915%2Fdistill.00007\nhttps://distill.pub/2018/building-blocks\nhttps://distill.pub/2018/building-blocks\nhttps://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/\nhttps://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/\nhttps://doi.org/10.1145/2939672.2939778\nhttps://doi.org/10.1145/2939672.2939778\nhttps://doi.org/10.1038/s42256-019-0048-x\nhttps://doi.org/10.1038/s42256-019-0048-x\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nSamek, W., Binder, A., Montavon, G., Lapuschkin, S., and\nMu\u0308ller, K. Evaluating the visualization of what a deep\nneural network has learned. IEEE Transactions on Neu-\nral Networks and Learning Systems, 28(11):2660\u20132673,\n2017.\n\nSelvaraju, R. R., Das, A., Vedantam, R., Cogswell,\nM., Parikh, D., and Batra, D. Grad-cam: Why\ndid you say that? visual explanations from deep\nnetworks via gradient-based localization. CoRR,\nabs/1610.02391, 2016. URL http://arxiv.org/\nabs/1610.02391.\n\nShrikumar, A., Greenside, P., Shcherbina, A., and Kun-\ndaje, A. Not just a black box: Learning important fea-\ntures through propagating activation differences. ArXiv,\nabs/1605.01713, 2016.\n\nShrikumar, A., Greenside, P., and Kundaje, A. Learn-\ning important features through propagating activa-\ntion differences. volume 70 of Proceedings of Ma-\nchine Learning Research, pp. 3145\u20133153, International\nConvention Centre, Sydney, Australia, 06\u201311 Aug\n2017. PMLR. URL http://proceedings.mlr.\npress/v70/shrikumar17a.html.\n\nSimonyan, K., Vedaldi, A., and Zisserman, A. Deep inside\nconvolutional networks: Visualising image classification\nmodels and saliency maps. In Workshop at International\nConference on Learning Representations, 2014.\n\nSpringenberg, J. T., Dosovitskiy, A., Brox, T., and Ried-\nmiller, M. A. Striving for simplicity: The all convolu-\ntional net. CoRR, abs/1412.6806, 2015.\n\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing properties\nof neural networks. CoRR, abs/1312.6199, 2014.\n\nTjoa, E. and Guan, C. A survey on explainable artificial in-\ntelligence (xai): Toward medical xai. IEEE Transactions\non Neural Networks and Learning Systems, pp. 1\u201321,\n2020a. doi: 10.1109/TNNLS.2020.3027314.\n\nTjoa, E. and Guan, C. Quantifying explainability of saliency\nmethods in deep neural networks. ArXiv, abs/2009.02899,\n2020b.\n\nZeiler, M. D. and Fergus, R. Visualizing and understanding\nconvolutional networks. In Fleet, D., Pajdla, T., Schiele,\nB., and Tuytelaars, T. (eds.), Computer Vision \u2013 ECCV\n2014, pp. 818\u2013833, Cham, 2014. Springer International\nPublishing. ISBN 978-3-319-10590-1.\n\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba,\nA. Learning deep features for discriminative localization.\nIn 2016 IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pp. 2921\u20132929, June 2016. doi:\n10.1109/CVPR.2016.319.\n\nhttp://arxiv.org/abs/1610.02391\nhttp://arxiv.org/abs/1610.02391\nhttp://proceedings.mlr.press/v70/shrikumar17a.html\nhttp://proceedings.mlr.press/v70/shrikumar17a.html\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nA. Appendix\nAll codes are available in the supplementary materials. All instructions to reproduce the results can be found in README.md,\ngiven as command line input, such as:\n\n1. python main pneu.py \u2013mode xai collect \u2013model resnet34 \u2013PROJECT ID pneu256n 1 \u2013method Saliency \u2013split train\n\u2013realtime print 1 \u2013n debug 0\n\n2. python main pneu.py \u2013mode gax \u2013PROJECT ID pneu256n 1 \u2013model resnet34 \u2013label NORMAL \u2013split test \u2013\nfirst n correct 100 \u2013target co 48 \u2013gax learning rate 0.1\n\nThe whole experiment can be run on small GPU like NVIDIA GeForce GTX 1050 with 4 GB dedicated memory.\n\nThe codes are run on Python 3.8.5. The only specialized library used is Pytorch (specifically torch==1.8.1+cu102,\ntorchvision==0.9.1+cu102) and Pytorch Captum (captum==0.3.1). Other libraries are common python libraries.\n\nRegarding Captum. We replace Pytorch Captum \u201cinplace relu\u201d so that some attribution methods will work properly (see\nadjust for captum problem in model.py where applicable).\n\nWe also manually edit non-full backward hooks in the source codes to prevent the gradient propagation is-\nsues. For example, from Windows, see Lib \\site-packages \\captum \\attr \\ core \\guided backprop deconvnet.py,\nfunction def register hooks(self, module: Module). There is a need to change from hook = module. regis-\nter backward hook(self. backward hook) to hook = module. register full backward hook(self. backward hook).\n\nA.1. More interpretations in low dimensional example\n\nInterpretation of attribute values for non-negative less distinct components. Now, we consider data sample with lower\na1 = 0.7 (i.e. less distinct) but components are still non-negative. Fig. 1 middle shows that components are still non-\nnegative around \u03b8 \u2208 [\u03c0/8, 3\u03c0/8]. Similar attribution of h1 and suppression of h2 are observed similarly although with\nlower magnitude around \u03b8 \u2248 0. At \u03b8 \u2248 \u03c0/4, similar difficulty in distinguishing homogenous transformation is present,\nnaturally. Further rotation to 3\u03c0/8 will give higher h2 as well. Fig. 6 right shows similar behavior even for a1 \u2248 a2,\nthough non-negative values are observed for rotations around [\u2212\u03c0/4, \u03c0/4]. The sample is barely categorized as c = 1 since\na1 > a2. However, the resulting attribution values still highlights the positive contribution x1, primarily through higher h1\nattribution value, even though the magnitudes are lower compared to previous examples.\n\nInterpretation of attribute values for negative components. Beyond the rotation range that yields non-negative components,\nwe do see negative components xi < 0 assigned highly negative hi values. For example, fig. 6 left at \u03b8 \u2248 \u03c0 shows a rotation\nof the components to the negatives. In this formulation, negative attribution values are assigned to negative components\nnaturally, because w \u2217 x starts with wi = 1 and xi < 0, as wi is optimized, our example shows an instance where, indeed,\nwe need higher wi, very negative h1. Recall the main interpretation. In the end, this high negative attribution is aimed at\nimproving CO score. The large negative h1 component increases the likelihood of predicting c = 1; conversely, the relatively\nlow h2 magnitude increases the same likelihood. Therefore, we do not immediately conclude that negative attribution values\ncontribute \u201cnegatively\u201d to prediction, which is a term sometimes ambiguously used in XAI community. In practice, case by\ncase review may be necessary.\n\nA.2. Zero CO scores and other scores\n\nZero CO score might occur when when h yields uniform change to the output, i.e. DNN(x+ h) = DNN(x) + c for some\nconstant c. This is obtained by simply plugging into the CO score formula. Special case may occur when h is constant\nover all pixels, especially when N(g(x+ h)) = N(g(x)) for some intermediate normalization layer N and intermediate\npre-normalized composition of layers g = gk \u25e6 gk\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 g1.\n\nPositive CO score indicates that the component [sco]i, where i corresponds to the groundtruth label, is increased by h at\na greater magnitude than the average of all other components, which in turn means that the component DNN(x + h)i\nis similarly increased at greater magnitude compared to the average of other components. Hence, the prediction favours\ncomponent i relatively more, i.e. the probability of predicting the correct class is increased. Negative CO score is simply the\nreverse: probability of predicting the correct class has been reduced relatively.\n\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nFigure 6. Solid red (blue) lines are x1(x2) components of sample data x. Dotted red (blue) lines are h1(h2) components of heatmaps\nh with k\u03b7 = 1.2. Heatmap values or attribute importances are assigned large values when either (1) the true components a1, a2 differ\nsignificantly (2) the W transforms the data heterogenously i.e. not \u03b8 \u2248 (2k + 1)\u03c0\n\n4\n.\n\nA.3. More Boxplots of CO Scores.\n\nFig. 7 shows more variations of CO scores in our experiments, similar to the ones shown in the main text. Some scores\nclearly demonstrate distinct gaps in CO scores between the correct and wrong predictions.\n\nFrom fig. 8, AX process is applied to the heatmaps generated in different layers of ResNet34. We expect higher improvement\nof CO scores for AX process using heatmaps from deeper layers that are known to detect more features. We do observe a\ndifference in x \u2217 h AX process, but not in x+ h for Layer Grad CAM.\n\nA.4. More Considerations, Limitations and Future Works\n\nDifferent GAX and empirical choices in implementation. Parts of the implementations, such as the initialization of w to 1.0,\nare nearly arbitrary, though it is the first choice made from the 2D example that happens to work. Different implementations\ncome with various trade-offs. Most notably, the choice of learning rate 0.1 is manually chosen for its reliable and fast\nconvergence, although convergence is attainable for smaller learning rate like 0.001 after longer iterations. However, we\nneed to include more practical considerations. For example, saving heatmaps iteration by iteration will generally consume\naround 5-12 MB of memory for current choices. Longer optimization iterations may quickly cause a blow-up, and there is\nno known fixed number of iterations needed to achieve convergence to the target CO score. Saving heatmaps at certain\nCO scores milestones can be considered, though we might miss out on important heatmap changes in between. Parameter\nselection process is thus not straightforward. For practical purposes, learning rates can be tested in order of ten, 10n, and\nother parameters can be tested until a choice is found where each optimization process converges at a rate fast enough for\nnearly instantaneous, quick diagnosis. Other choices of optimizers with different parameters combination can be explored as\nwell, though we have yet to see dramatic changes.\n\nGAX, different DNN architectures and different datasets. Comparisons are tricky, since different architectures might behave\ndifferently at their FC end. For example, for Saliency method on ImageNet, Alexnet\u2019s boxplot of CO scores in appendix\nfig. 7(A) right (AX process x + w \u2217 x) shows a wider range of CO scores than that of Resnet34 in fig. ??. Comparison\nof CO scores on Chest X-Ray dataset shows even larger variability. Furthermore, recall that we illustrated using the 2D\nexample the reason we avoid sigmoid function: suppressed change in the CO score due to its asymptotic part. We have\navoided Softmax for similar reason, and a further study can be conducted to characterize the scores with Softmax or other\nmodifications. From here, the ideal vision is to develop a model that scales with CO score in not only a computationally\nrelevant way, but also in a human relevant way: we want a model that increases predictive probability when the heatmap\nhighlights exactly the correct localization of the objects or highly relevant features related to the objects. This is a tall effort,\nparticularly because explanations are highly context dependent. Transparent and trustworthy applications of DNN may\nbenefit from the combined improvements in humanly understandable context and computationally relevance attributions\nbuilt around that context.\n\n\n\nImproving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI\n\nFigure 7. Boxplots of CO scores for existing XAI methods, including another GAX implementation x \u2217 h = x \u2217 (w \u2217 x)\n.\n\nFigure 8. Boxplots of CO scores for heatmaps from Layer GradCAM for ResNet34 and ImageNet dataset. CO scores of heatmaps\ngenerated from different layers (and resized accordingly) are shown.\n\n.\n\n\n"}
